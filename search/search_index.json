{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Quickstart Getting Started User Documentation For the daphne command-line interface API: see bin/daphne --help Running DAPHNE in a Local Environment Running DAPHNE on the Distributed Runtime DAPHNE Packaging, Distributed Deployment, and Management DaphneLib: DAPHNE's Python API DaphneLib API Reference DaphneDSL Language Reference DaphneDSL Built-in Functions Using SQL in DaphneDSL A Few Early Example Algorithms in DaphneDSL FileMetaData Format (reading and writing data) Profiling DAPHNE using PAPI Developer Documentation How-tos and Guidelines Handling a Pull Request Implementing a Built-in Kernel Binary Data Format DAPHNE Configuration: Getting Information from the User Extending DAPHNE with more scheduling knobs Extending the DAPHNE Distributed Runtime Building DAPHNE with the build.sh script Logging in DAPHNE Profiling in DAPHNE Source Code Documentation The source code is partly documented in doxygen style. Until the generation of a proper documentation has been set up, you can simply have a look at the documentation in the individual source files.","title":"Overview"},{"location":"#overview","text":"Quickstart Getting Started","title":"Overview"},{"location":"#user-documentation","text":"For the daphne command-line interface API: see bin/daphne --help Running DAPHNE in a Local Environment Running DAPHNE on the Distributed Runtime DAPHNE Packaging, Distributed Deployment, and Management DaphneLib: DAPHNE's Python API DaphneLib API Reference DaphneDSL Language Reference DaphneDSL Built-in Functions Using SQL in DaphneDSL A Few Early Example Algorithms in DaphneDSL FileMetaData Format (reading and writing data) Profiling DAPHNE using PAPI","title":"User Documentation"},{"location":"#developer-documentation","text":"","title":"Developer Documentation"},{"location":"#how-tos-and-guidelines","text":"Handling a Pull Request Implementing a Built-in Kernel Binary Data Format DAPHNE Configuration: Getting Information from the User Extending DAPHNE with more scheduling knobs Extending the DAPHNE Distributed Runtime Building DAPHNE with the build.sh script Logging in DAPHNE Profiling in DAPHNE","title":"How-tos and Guidelines"},{"location":"#source-code-documentation","text":"The source code is partly documented in doxygen style. Until the generation of a proper documentation has been set up, you can simply have a look at the documentation in the individual source files.","title":"Source Code Documentation"},{"location":"BinaryFormat/","text":"Binary Data Format DAPHNE defines its own binary representation for the serialization of in-memory data objects (matrices/frames). This representation is intended to be used by default whenever we need to transfer or persistently store these in-memory objects, e.g., for the data transfer in the distributed runtime a custom binary file format the eviction of in-memory data to secondary storage Disclaimer: The current specification is a first draft and will likely be refined as we proceed. At the moment, we focus on the case of a single block per data object. Endianess: For now, we assume little endian . Images: In the images below, all addresses and sizes are specified in bytes ( [B] ). Binary Representation of a Whole Data Object The binary representation of a data object (matrix/frame) starts with a header containing general and data type-specific information. The data object is partitioned into rectangular blocks (in the extreme case, this can mean a single block). All blocks are represented individually (see binary representation of a single block below) and stored along with their position in the data object. +--------+------+ | header | body | +--------+------+ Header The header consists of the following information: DAPHNE binary format version number ( 1 for now) (uint8) data type dt (uint8) number of rows #r (uint64) number of columns #c (uint64) We currently support the following data types : code data type 0 (reserved) 1 DenseMatrix 2 CSRMatrix 3 Frame We currently support the following value types : code C++ value type 0 (reserved) 1 uint8_t 2 uint16_t 3 uint32_t 4 uint64_t 5 int8_t 6 int16_t 7 int32_t 8 int64_t 9 float 10 double Depending on the data type, there are more information in the header: For DenseMatrix and CSRMatrix : value type vt (uint8) addr[B] 0 0 1 1 2 9 10 17 18 18 +---+----+----+-----+-----+ | 1 | dt | #r | #c | vt | +---+----+----+-----+-----+ size[B] 1 1 8 8 1 For Frame : value type vt (uint8), for each column length of the label len (uint16) and label lbl (character string), for each column addr[B] 0 0 1 1 2 9 10 17 18 18+#c-1 18+#c * +---+----+----+-----+-------+ +----------+--------+--------+ +-----------+-----------+ | 1 | dt | #r | #c | vt[0] | ... | vt[#c-1] | len[0] | lbl[0] | ... | len[#c-1] | lbl[#c-1] | +---+----+----+-----+-------+ +----------+--------+--------+ +-----------+-----------+ size[B] 1 1 8 8 1 1 2 len[0] 2 len[#c-1] Body The body consists of a sequence of: a pair of row index rx (uint64) column index cx (uint64) a binary block representation For the special case of a single block, this looks as follows: addr[B] 0 7 8 15 16 * +---+----+----------+ | 0 | 0 | block[0] | +---+----+----------+ 8 8 * size[B] Binary Representation of a Single Block A single data block is a rectangular partition of a data object. In the extreme case, a single block can span the entire data object in both dimensions (one block per data object). General block header number of rows #r (uint32) number of columns #c (uint32) block type bt (uint8) block type-specific information (see below) addr[B] 0 3 4 7 8 8 9 * +----+----+----+--------------------------+ | #r | #c | bt | block type-specific info | +----+----+----+--------------------------+ size[B] 4 4 1 * Block types We define different block types to allow for a space-efficient representation depending on the data. When serializing a data object, the block types are not required to match the in-memory representation (e.g., the blocks of a DenseMatrix could use the sparse binary representation). Moreover, different blocks may be represented as different block types (e.g., some blocks might use the dense binary representation and others the sparse one). We currently support the following block types: code block type 0 empty 1 dense 2 sparse (CSR) 3 ultra-sparse (COO) Most block types store their value type as part of the block type-specific information. Note that the value type used for the binary representation is not required to match the value type of the in-memory object (e.g., DenseMatrix<uint64_t> may be represented as a dense block with value type uint8_t , if the value range permits). Furthermore, each block may be represented using its individual value type. Empty block This block type is used to represent blocks that contain only zeros of the respective value type very space-efficiently. Block type-specific information: none addr[B] 0 3 4 7 8 8 +----+----+---+ | #r | #c | 0 | +----+----+---+ size[B] 4 4 1 Dense block Block type-specific information: value type vt (uint8) values v in row-major (value type vt ) Below, S denotes the size (in bytes) of a single value of type vt . addr[B] 0 3 4 7 8 8 9 9 10 10+#r*#c*S +----+----+---+----+---------+---------+ +---------------+ | #r | #c | 1 | vt | v[0, 0] | v[0, 1] | ... | v[#r-1, #c-1] | +----+----+---+----+---------+---------+ +---------------+ size[B] 4 4 1 1 S S S Sparse block (compressed sparse row, CSR) Block type-specific information: value type vt (uint8) number of non-zeros in the block #nzb (uint64) for each row number of non-zeros in the row #nzr (uint32) for each non-zero in the row column index cx (uint32) value v (value type vt ) Note that both a row and the entire block might contain no non-zeros. Below, S denotes the size (in bytes) of a single value of type vt . 18 + 4*#r + addr[B] 0 3 4 7 8 8 9 9 10 17 18 #nzb*(4+S) +----+----+---+----+------+--------+ +--------+ +-----------+ | #r | #c | 2 | vt | #nzb | row[0] | ... | row[i] | ... | row[#r-1] | +----+----+---+----+------+--------+ +--------+ +-----------+ size[B] 4 4 1 1 8 4+#nzr[i]*(4+S) ______________________________________|_______________________ / \\ +---------+----------+ +----------+ +------------------+ | #nzr[i] | nz[i, 0] | ... | nz[i, j] | ... | nz[i, #nzr[i]-1] | +---------+----------+ +----------+ +------------------+ 4 4+S 4+S 4+S ______________|____________ / \\ +----------+----------------+ | cx[i, j] | v[i, cx[i, j]] | +----------+----------------+ 4 S Ultra-sparse block (coordinate, COO) Ultra-sparse blocks contain almost no non-zeros, so we want to keep the overhead of the meta data low. Thus, we distinguish blocks with a single column (where we don't need to store the column index) and blocks with more than one column. Blocks with a single column Block type-specific information: value type vt (uint8) number of non-zeros in the block #nzb (uint32) for each non-zero row index rx (uint32) value v (value type vt ) Below, S denotes the size (in bytes) of a single value of type vt . addr[B] 0 3 4 7 8 8 9 9 10 13 14 14+#nzb*(4+S) +----+----+---+----+------+-------+ +-------+ +------------+ | #r | #c | 3 | vt | #nzb | nz[0] | ... | nz[i] | ... | nz[#nzb-1] | +----+----+---+----+------+-------+ +-------+ +------------+ size[B] 4 4 1 1 4 4+S 4+S 4+S __________|__________ / \\ +-------+-------------+ | rx[i] | v[rx[i], 0] | +-------+-------------+ 4 S Blocks with more than one column Block type-specific information: value type vt (uint8) number of non-zeros in the block #nzb (uint32) for each non-zero row index rx (uint32) column index cx (uint32) value v (value type vt ) Below, S denotes the size (in bytes) of a single value of type vt . addr[B] 0 3 4 7 8 8 9 9 10 13 14 14+#nzb*(8+S) +----+----+---+----+------+-------+ +-------+ +------------+ | #r | #c | 3 | vt | #nzb | nz[0] | ... | nz[i] | ... | nz[#nzb-1] | +----+----+---+----+------+-------+ +-------+ +------------+ size[B] 4 4 1 1 4 8+S 8+S 8+S ________________|________________ / \\ +-------+-------+-----------------+ | rx[i] | cx[i] | v[rx[i], cx[i]] | +-------+-------+-----------------+ 4 4 S","title":"BinaryFormat"},{"location":"BinaryFormat/#binary-data-format","text":"DAPHNE defines its own binary representation for the serialization of in-memory data objects (matrices/frames). This representation is intended to be used by default whenever we need to transfer or persistently store these in-memory objects, e.g., for the data transfer in the distributed runtime a custom binary file format the eviction of in-memory data to secondary storage Disclaimer: The current specification is a first draft and will likely be refined as we proceed. At the moment, we focus on the case of a single block per data object. Endianess: For now, we assume little endian . Images: In the images below, all addresses and sizes are specified in bytes ( [B] ).","title":"Binary Data Format"},{"location":"BinaryFormat/#binary-representation-of-a-whole-data-object","text":"The binary representation of a data object (matrix/frame) starts with a header containing general and data type-specific information. The data object is partitioned into rectangular blocks (in the extreme case, this can mean a single block). All blocks are represented individually (see binary representation of a single block below) and stored along with their position in the data object. +--------+------+ | header | body | +--------+------+","title":"Binary Representation of a Whole Data Object"},{"location":"BinaryFormat/#header","text":"The header consists of the following information: DAPHNE binary format version number ( 1 for now) (uint8) data type dt (uint8) number of rows #r (uint64) number of columns #c (uint64) We currently support the following data types : code data type 0 (reserved) 1 DenseMatrix 2 CSRMatrix 3 Frame We currently support the following value types : code C++ value type 0 (reserved) 1 uint8_t 2 uint16_t 3 uint32_t 4 uint64_t 5 int8_t 6 int16_t 7 int32_t 8 int64_t 9 float 10 double Depending on the data type, there are more information in the header: For DenseMatrix and CSRMatrix : value type vt (uint8) addr[B] 0 0 1 1 2 9 10 17 18 18 +---+----+----+-----+-----+ | 1 | dt | #r | #c | vt | +---+----+----+-----+-----+ size[B] 1 1 8 8 1 For Frame : value type vt (uint8), for each column length of the label len (uint16) and label lbl (character string), for each column addr[B] 0 0 1 1 2 9 10 17 18 18+#c-1 18+#c * +---+----+----+-----+-------+ +----------+--------+--------+ +-----------+-----------+ | 1 | dt | #r | #c | vt[0] | ... | vt[#c-1] | len[0] | lbl[0] | ... | len[#c-1] | lbl[#c-1] | +---+----+----+-----+-------+ +----------+--------+--------+ +-----------+-----------+ size[B] 1 1 8 8 1 1 2 len[0] 2 len[#c-1]","title":"Header"},{"location":"BinaryFormat/#body","text":"The body consists of a sequence of: a pair of row index rx (uint64) column index cx (uint64) a binary block representation For the special case of a single block, this looks as follows: addr[B] 0 7 8 15 16 * +---+----+----------+ | 0 | 0 | block[0] | +---+----+----------+ 8 8 * size[B]","title":"Body"},{"location":"BinaryFormat/#binary-representation-of-a-single-block","text":"A single data block is a rectangular partition of a data object. In the extreme case, a single block can span the entire data object in both dimensions (one block per data object). General block header number of rows #r (uint32) number of columns #c (uint32) block type bt (uint8) block type-specific information (see below) addr[B] 0 3 4 7 8 8 9 * +----+----+----+--------------------------+ | #r | #c | bt | block type-specific info | +----+----+----+--------------------------+ size[B] 4 4 1 *","title":"Binary Representation of a Single Block"},{"location":"BinaryFormat/#block-types","text":"We define different block types to allow for a space-efficient representation depending on the data. When serializing a data object, the block types are not required to match the in-memory representation (e.g., the blocks of a DenseMatrix could use the sparse binary representation). Moreover, different blocks may be represented as different block types (e.g., some blocks might use the dense binary representation and others the sparse one). We currently support the following block types: code block type 0 empty 1 dense 2 sparse (CSR) 3 ultra-sparse (COO) Most block types store their value type as part of the block type-specific information. Note that the value type used for the binary representation is not required to match the value type of the in-memory object (e.g., DenseMatrix<uint64_t> may be represented as a dense block with value type uint8_t , if the value range permits). Furthermore, each block may be represented using its individual value type.","title":"Block types"},{"location":"BinaryFormat/#empty-block","text":"This block type is used to represent blocks that contain only zeros of the respective value type very space-efficiently. Block type-specific information: none addr[B] 0 3 4 7 8 8 +----+----+---+ | #r | #c | 0 | +----+----+---+ size[B] 4 4 1","title":"Empty block"},{"location":"BinaryFormat/#dense-block","text":"Block type-specific information: value type vt (uint8) values v in row-major (value type vt ) Below, S denotes the size (in bytes) of a single value of type vt . addr[B] 0 3 4 7 8 8 9 9 10 10+#r*#c*S +----+----+---+----+---------+---------+ +---------------+ | #r | #c | 1 | vt | v[0, 0] | v[0, 1] | ... | v[#r-1, #c-1] | +----+----+---+----+---------+---------+ +---------------+ size[B] 4 4 1 1 S S S","title":"Dense block"},{"location":"BinaryFormat/#sparse-block-compressed-sparse-row-csr","text":"Block type-specific information: value type vt (uint8) number of non-zeros in the block #nzb (uint64) for each row number of non-zeros in the row #nzr (uint32) for each non-zero in the row column index cx (uint32) value v (value type vt ) Note that both a row and the entire block might contain no non-zeros. Below, S denotes the size (in bytes) of a single value of type vt . 18 + 4*#r + addr[B] 0 3 4 7 8 8 9 9 10 17 18 #nzb*(4+S) +----+----+---+----+------+--------+ +--------+ +-----------+ | #r | #c | 2 | vt | #nzb | row[0] | ... | row[i] | ... | row[#r-1] | +----+----+---+----+------+--------+ +--------+ +-----------+ size[B] 4 4 1 1 8 4+#nzr[i]*(4+S) ______________________________________|_______________________ / \\ +---------+----------+ +----------+ +------------------+ | #nzr[i] | nz[i, 0] | ... | nz[i, j] | ... | nz[i, #nzr[i]-1] | +---------+----------+ +----------+ +------------------+ 4 4+S 4+S 4+S ______________|____________ / \\ +----------+----------------+ | cx[i, j] | v[i, cx[i, j]] | +----------+----------------+ 4 S","title":"Sparse block (compressed sparse row, CSR)"},{"location":"BinaryFormat/#ultra-sparse-block-coordinate-coo","text":"Ultra-sparse blocks contain almost no non-zeros, so we want to keep the overhead of the meta data low. Thus, we distinguish blocks with a single column (where we don't need to store the column index) and blocks with more than one column.","title":"Ultra-sparse block (coordinate, COO)"},{"location":"BinaryFormat/#blocks-with-a-single-column","text":"Block type-specific information: value type vt (uint8) number of non-zeros in the block #nzb (uint32) for each non-zero row index rx (uint32) value v (value type vt ) Below, S denotes the size (in bytes) of a single value of type vt . addr[B] 0 3 4 7 8 8 9 9 10 13 14 14+#nzb*(4+S) +----+----+---+----+------+-------+ +-------+ +------------+ | #r | #c | 3 | vt | #nzb | nz[0] | ... | nz[i] | ... | nz[#nzb-1] | +----+----+---+----+------+-------+ +-------+ +------------+ size[B] 4 4 1 1 4 4+S 4+S 4+S __________|__________ / \\ +-------+-------------+ | rx[i] | v[rx[i], 0] | +-------+-------------+ 4 S","title":"Blocks with a single column"},{"location":"BinaryFormat/#blocks-with-more-than-one-column","text":"Block type-specific information: value type vt (uint8) number of non-zeros in the block #nzb (uint32) for each non-zero row index rx (uint32) column index cx (uint32) value v (value type vt ) Below, S denotes the size (in bytes) of a single value of type vt . addr[B] 0 3 4 7 8 8 9 9 10 13 14 14+#nzb*(8+S) +----+----+---+----+------+-------+ +-------+ +------------+ | #r | #c | 3 | vt | #nzb | nz[0] | ... | nz[i] | ... | nz[#nzb-1] | +----+----+---+----+------+-------+ +-------+ +------------+ size[B] 4 4 1 1 4 8+S 8+S 8+S ________________|________________ / \\ +-------+-------+-----------------+ | rx[i] | cx[i] | v[rx[i], cx[i]] | +-------+-------+-----------------+ 4 4 S","title":"Blocks with more than one column"},{"location":"Config/","text":"Configuration - Getting Information from the User The behavior of the DAPHNE system can be influenced by the user by means of a cascading configuration mechanism. There is a set of options that can be passed from the user to the system. These options are collected in the DaphneUserConfig ( src/api/cli/DaphneUserConfig.h ). The cascade consists of the following steps: The defaults of all options are hard-coded directly in the DaphneUserConfig . At program start-up, a configuration file is loaded, which overrides the defaults (WIP, #141) . After that, command-line arguments further override the configuration ( src/api/cli/daphne.cpp ). (In the future, DaphneDSL will also offer means to change the configuration at run-time.) The DaphneUserConfig is available to all parts of the code, including: The DAPHNE compiler: The DaphneUserConfig is passed to the DaphneIrExecutor and from there to all passes that need it. The DAPHNE runtime: The DaphneUserConfig is part of the DaphneContext , which is passed to all kernels. Hence, information provided by the user can be used to influence both, the compiler and the runtime. The use of environment variables to pass information into the system is discouraged. How to extend the configuration? If you need to add additional information from the user, you must take roughly the following steps: Create a new member in DaphneUserConfig and hard-code a reasonable default. Add a command-line argument to the system's CLI API in src/api/cli/daphne.cpp . We use LLVM's CommandLine 2.0 library for parsing CLI arguments. Make sure to update the corresponding member the DaphneUserConfig with the parsed argument. For compiler passes : If necessary, pass on the DaphneUserConfig to the compiler pass you are working on in src/compiler/execution/DaphneIrExecutor.cpp . For kernels : All kernels automatically get the DaphneUserConfig via the DaphneContext (their last parameter), so no action is required from your side. Access the new member of the DaphneUserConfig in your code.","title":"Configuration"},{"location":"Config/#configuration-getting-information-from-the-user","text":"The behavior of the DAPHNE system can be influenced by the user by means of a cascading configuration mechanism. There is a set of options that can be passed from the user to the system. These options are collected in the DaphneUserConfig ( src/api/cli/DaphneUserConfig.h ). The cascade consists of the following steps: The defaults of all options are hard-coded directly in the DaphneUserConfig . At program start-up, a configuration file is loaded, which overrides the defaults (WIP, #141) . After that, command-line arguments further override the configuration ( src/api/cli/daphne.cpp ). (In the future, DaphneDSL will also offer means to change the configuration at run-time.) The DaphneUserConfig is available to all parts of the code, including: The DAPHNE compiler: The DaphneUserConfig is passed to the DaphneIrExecutor and from there to all passes that need it. The DAPHNE runtime: The DaphneUserConfig is part of the DaphneContext , which is passed to all kernels. Hence, information provided by the user can be used to influence both, the compiler and the runtime. The use of environment variables to pass information into the system is discouraged.","title":"Configuration - Getting Information from the User"},{"location":"Config/#how-to-extend-the-configuration","text":"If you need to add additional information from the user, you must take roughly the following steps: Create a new member in DaphneUserConfig and hard-code a reasonable default. Add a command-line argument to the system's CLI API in src/api/cli/daphne.cpp . We use LLVM's CommandLine 2.0 library for parsing CLI arguments. Make sure to update the corresponding member the DaphneUserConfig with the parsed argument. For compiler passes : If necessary, pass on the DaphneUserConfig to the compiler pass you are working on in src/compiler/execution/DaphneIrExecutor.cpp . For kernels : All kernels automatically get the DaphneUserConfig via the DaphneContext (their last parameter), so no action is required from your side. Access the new member of the DaphneUserConfig in your code.","title":"How to extend the configuration?"},{"location":"Deploy/","text":"Deploying DAPHNE Packaging, Distributed Deployment, and Management Overview This file explains the deployment of the Daphne system , on HPC with SLURM or manually through SSH, and highlights the excerpts from descriptions of functionalities in deploy/ directory (mostly deploy-distributed-on-slurm.sh ): compilation of the Singularity image, compilation of Daphne (and the Daphne DistributedWorker) within the Singularity image, packaging compiled Daphne, packaging compiled Daphne with user payload as a payload package, uploading the payload package to an HPC platform, starting and managing DAPHNE workers on HPC platforms using SLURM, executing DAPHNE on HPC using SLURM, collection of logs from daphne execution, and cleanup of worker environments and payload deployment. Background Daphne's distributed system consists of a single coordinator and multiple DistributedWorkers (you can read more about Distributed DAPHNE here ). For now, in order to execute Daphne in a distributed fashion, we need to deploy DistributedWorkers manually. Coordinator gets the worker's addresses through an environmental variable. deployDistributed.sh manually connects to machines with SSH and starts up DistributedWorker processes. On the other hand the deploy-distributed-on-slurm.sh packages and starts Daphne on a target HPC platform, and is tailored to the communication required with Slurm and the target HPC platform. Deploying without Slurm support deployDistributed.sh can be used to manually connect to a list of machines and remotely start up workers, get status of running workers or terminate distributed worker processes. This script depends only on an SSH client/server and does not require any use of a resource management tool (e.g. SLURM). With this script you can: build and deploy DistributedWorkers to remote machines start workers check status of running workers kill workers Workers' own IPs and ports to listen to, can be specified inside the script, or with --peers [IP[:PORT]],[IP[:PORT]],... . Default port for all workers is 50000, but this can also be specified inside the script or with -p,--port PORT . If running on same machine (e.g. localhost), different ports must be specified. With --deploy the script builds DistributedWorker executable ( ./build.sh --target DistributedWorker ), compresses build , lib and bin folders and uses scp and ssh to send and decompress at remote machines, inside the directory specified by --pathToBuild (default ~/DaphneDistributedWorker/ ). If running workers on localhost, PATH_TO_BUILD can be set /path/to/daphne and provided DistributedWorker is built, --deploy is not nessecary. Ssh username must be specified inside the script. For now the script assumes all remote machines can be accessed with the same username , id_rsa key and ssh port (default 22). Usage example: # deploy distributed $ ./deployDistributed.sh --help $ ./deployDistributed.sh --deploy --pathToBuild /path/to/dir --peers localhost:5000,localhost:5001 $ ./deployDistributed.sh -r # (Uses default peers and path/to/build/ to start workers) Deploying with Slurm support Building the Daphne system (to be later deployed on distributed nodes) can be done with a Singularity container. The Singularity container can be built on the utilized HPC. deployDistributed.sh sends executables on each node, assuming there are different storages for each node. This might cause unnecessary overwrites if the workers use same mounted user storage (e.g. HPC environments with distributed storages). Instead deploy-distributed-on-slurm.sh should be used for such cases. The latter also automatically generates the environmental variable PEERS from Slurm. How to use deploy-distributed-on-slurm.sh for DAPHNE Packaging, Distributed Deployment, and Management using Slurm This explains how to set up the Distributed Workers on a HPC platform, and it also briefly comments on what to do afterwards (how to run, manage, stop, and clean it). Commands, with their parameters and arguments, are hence described below for deployment with deploy-distributed-on-slurm.sh . Usage: deploy-distributed-on-slurm.sh <options> <command> Start the DAPHNE distributed deployment on remote machines using Slurm. These are the options ( short and long formats available ) : -h, --help Print this help message and exit. -i SSH_IDENTITY_FILE Specify OpenSSH identity file ( default: private key in ~/.ssh/id_rsa.pub ) . -u, --user SSH_USERNAME Specify OpenSSH username ( default: $USER ) . -l, --login SSH_LOGIN_NODE_HOSTNAME Specify OpenSSH login name hostname ( default: localhost ) . -d, --pathToBuild A path to deploy or where the build is already deployed ( default ~/DaphneDistributedWorker can be specified in the script ) . -n, --numcores Specify number of workers ( cores ) to use to deploy DAPHNE workers ( default: 128 ) . -p, --port Specify DAPHNE deployed port range begin ( default: 50000 ) . --args ARGS_CS Specify arguments of a DaphneDSL SCRIPT in a comma-separated format. -S, --ssh-arg = S Specify additional arguments S for ssh client ( default command: $SSH_COMMAND ) . -C, --scp-arg = C Specify additional arguments C for scp client ( default command: $SCP_COMMAND ) . -R, --srun-arg = R Specify additional arguments R for srun client. -G, --singularity-arg = G Specify additional arguments G for singularity client. These are the commands that can be executed: singularity Compile the Singularity SIF image for DAPHNE ( and transfer it to the target platform ) . build Compile DAPHNE codes ( daphne, DistributedWorker ) using the Singularity image for DAPHNE. It should only be invoked from the code base root directory. It could also be invoked on a target platform after a transfer. package Create the package image with *.daphne scripts and a compressed build/ directory. transfer Transfers ( uploads ) a package to the target platform. start Run workers on remote machines through login node ( deploys this script and runs workers ) . workers Run workers on current login node through Slurm. status Get distributed workers ' status. wait Waits until all workers are up. stop Stops all distributed workers. run [ SCRIPT [ ARGs ]] Run one request on the deployed platform by processing one DaphneDSL SCRIPT file ( default: /dev/stdin ) using optional arguments ( ARGs in script format ) . clean Cleans ( deletes ) the package on the target platform. deploy Deploys everything in one sweep: singularity = >build = >package = >transfer = >start = >wait = >run = >clean. The default connection to the target platform ( HPC ) login node is through OpenSSH, configured by default in ~/.ssh ( see: man ssh_config ) . The default ports for worker peers begin at 50000 ( PORTRANGE_BEGIN ) and the list of PEERS is generated as: PEERS = ( WORKER1_IP:PORTRANGE_BEGIN, WORKER1_IP:PORTRANGE_BEGIN+1, ..., WORKER2_IP:PORTRANGE_BEGIN, WORKER2_IP:PORTRANGE_BEGIN+1, ... ) Logs can be found at [ pathToBuild ] /logs. Short Examples The following list presents few examples about how to use the deploy-distributed-on-slurm.sh command. These comprise more hands-on documentation about deployment, including tutorial-like explanation examples about how to package, distributively deploy, manage, and execute workloads using DAPHNE. Builds the Singularity image and uses it to compile the build directory codes, then packages it. ./deploy-distributed-on-slurm.sh singularity && ./deploy-distributed-on-slurm.sh build && ./deploy-distributed-on-slurm.sh package Transfers a package to the target platform through OpenSSH, using login node HPC, user hpc, and identify key hpc.pub. ./deploy-distributed-on-slurm.sh --login HPC --user hpc -i ~/.ssh/hpc.pub transfer Using login node HPC, accesses the target platform and starts workers on remote machines. ./deploy-distributed-on-slurm.sh -l HPC start Runs one request (script called example-time.daphne) on the deployment using 1024 cores, login node HPC, and default OpenSSH configuration. ./deploy-distributed-on-slurm.sh -l HPC -n 1024 run example-time.daphne Executes one request (DaphneDSL script input from standard input) at a running deployed platform, using default singularity/srun configurations. ./deploy-distributed-on-slurm.sh run Deploys once at the target platform through OpenSSH using default login node (localhost), then cleans. ./deploy-distributed-on-slurm.sh deploy -n 10 Starts workers at a running deployed platform using custom srun arguments (2 hours dual-core with 10G memory). ./deploy-distributed-on-slurm.sh workers -R = \"-t 120 --mem-per-cpu=10G --cpu-bind=cores --cpus-per-task=2\" Executes a request with custom srun arguments (30 minutes single-core). ./deploy-distributed-on-slurm.sh run -R = \"--time=30 --cpu-bind=cores --nodes=1 --ntasks-per-node=1 --cpus-per-task=1\" Example request job from a pipe. cat ../scripts/examples/hello-world.daph | ./deploy-distributed-on-slurm.sh run Scenario Usage Example Here is a scenario usage as a longer example demo. Fetch the code from the latest GitHub code repository. function compile () { git clone --recursive git@github.com:daphne-eu/daphne.git 2 > & 1 | tee daphne- $( date +%F-%T ) .log cd daphne/deploy ./deploy-distributed-on-slurm.sh singularity # creates the Singularity container image ./deploy-distributed-on-slurm.sh build # Builds the daphne codes using the container } compile Package the built targets (binaries) to packet file daphne-package.tgz . ./deploy-distributed-on-slurm.sh package Transfer the packet file daphne-package.tgz to HPC (Slurm) with OpenSSH key ~/.ssh/hpc.pub and unpack it. ./deploy-distributed-on-slurm.sh --login HPC --user $USER -i ~/.ssh/hpc.pub transfer E.g., for EuroHPC Vega, use the instance, if your username matches the one at Vega and the key is ~/.ssh/hpc.pub : ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub transfer Start the workers from the local computer by logging into the HPC login node: ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub start Starting a main target on the HPC (Slurm) and connecting it with the started workers, to execute payload from the stream. cat ../scripts/examples/hello-world.daph | ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub run Starting a main target on the HPC (Slurm) and connecting it with the started workers, to execute payload from a file. ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub run example-time.daphne Stopping all workers on the HPC (Slurm). ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub stop Cleaning the uploaded targets from the HPC login node. ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub clean","title":"Deploy"},{"location":"Deploy/#deploying","text":"DAPHNE Packaging, Distributed Deployment, and Management","title":"Deploying"},{"location":"Deploy/#overview","text":"This file explains the deployment of the Daphne system , on HPC with SLURM or manually through SSH, and highlights the excerpts from descriptions of functionalities in deploy/ directory (mostly deploy-distributed-on-slurm.sh ): compilation of the Singularity image, compilation of Daphne (and the Daphne DistributedWorker) within the Singularity image, packaging compiled Daphne, packaging compiled Daphne with user payload as a payload package, uploading the payload package to an HPC platform, starting and managing DAPHNE workers on HPC platforms using SLURM, executing DAPHNE on HPC using SLURM, collection of logs from daphne execution, and cleanup of worker environments and payload deployment.","title":"Overview"},{"location":"Deploy/#background","text":"Daphne's distributed system consists of a single coordinator and multiple DistributedWorkers (you can read more about Distributed DAPHNE here ). For now, in order to execute Daphne in a distributed fashion, we need to deploy DistributedWorkers manually. Coordinator gets the worker's addresses through an environmental variable. deployDistributed.sh manually connects to machines with SSH and starts up DistributedWorker processes. On the other hand the deploy-distributed-on-slurm.sh packages and starts Daphne on a target HPC platform, and is tailored to the communication required with Slurm and the target HPC platform.","title":"Background"},{"location":"Deploy/#deploying-without-slurm-support","text":"deployDistributed.sh can be used to manually connect to a list of machines and remotely start up workers, get status of running workers or terminate distributed worker processes. This script depends only on an SSH client/server and does not require any use of a resource management tool (e.g. SLURM). With this script you can: build and deploy DistributedWorkers to remote machines start workers check status of running workers kill workers Workers' own IPs and ports to listen to, can be specified inside the script, or with --peers [IP[:PORT]],[IP[:PORT]],... . Default port for all workers is 50000, but this can also be specified inside the script or with -p,--port PORT . If running on same machine (e.g. localhost), different ports must be specified. With --deploy the script builds DistributedWorker executable ( ./build.sh --target DistributedWorker ), compresses build , lib and bin folders and uses scp and ssh to send and decompress at remote machines, inside the directory specified by --pathToBuild (default ~/DaphneDistributedWorker/ ). If running workers on localhost, PATH_TO_BUILD can be set /path/to/daphne and provided DistributedWorker is built, --deploy is not nessecary. Ssh username must be specified inside the script. For now the script assumes all remote machines can be accessed with the same username , id_rsa key and ssh port (default 22). Usage example: # deploy distributed $ ./deployDistributed.sh --help $ ./deployDistributed.sh --deploy --pathToBuild /path/to/dir --peers localhost:5000,localhost:5001 $ ./deployDistributed.sh -r # (Uses default peers and path/to/build/ to start workers)","title":"Deploying without Slurm support"},{"location":"Deploy/#deploying-with-slurm-support","text":"Building the Daphne system (to be later deployed on distributed nodes) can be done with a Singularity container. The Singularity container can be built on the utilized HPC. deployDistributed.sh sends executables on each node, assuming there are different storages for each node. This might cause unnecessary overwrites if the workers use same mounted user storage (e.g. HPC environments with distributed storages). Instead deploy-distributed-on-slurm.sh should be used for such cases. The latter also automatically generates the environmental variable PEERS from Slurm.","title":"Deploying with Slurm support"},{"location":"Deploy/#how-to-use-deploy-distributed-on-slurmsh-for-daphne-packaging-distributed-deployment-and-management-using-slurm","text":"This explains how to set up the Distributed Workers on a HPC platform, and it also briefly comments on what to do afterwards (how to run, manage, stop, and clean it). Commands, with their parameters and arguments, are hence described below for deployment with deploy-distributed-on-slurm.sh . Usage: deploy-distributed-on-slurm.sh <options> <command> Start the DAPHNE distributed deployment on remote machines using Slurm. These are the options ( short and long formats available ) : -h, --help Print this help message and exit. -i SSH_IDENTITY_FILE Specify OpenSSH identity file ( default: private key in ~/.ssh/id_rsa.pub ) . -u, --user SSH_USERNAME Specify OpenSSH username ( default: $USER ) . -l, --login SSH_LOGIN_NODE_HOSTNAME Specify OpenSSH login name hostname ( default: localhost ) . -d, --pathToBuild A path to deploy or where the build is already deployed ( default ~/DaphneDistributedWorker can be specified in the script ) . -n, --numcores Specify number of workers ( cores ) to use to deploy DAPHNE workers ( default: 128 ) . -p, --port Specify DAPHNE deployed port range begin ( default: 50000 ) . --args ARGS_CS Specify arguments of a DaphneDSL SCRIPT in a comma-separated format. -S, --ssh-arg = S Specify additional arguments S for ssh client ( default command: $SSH_COMMAND ) . -C, --scp-arg = C Specify additional arguments C for scp client ( default command: $SCP_COMMAND ) . -R, --srun-arg = R Specify additional arguments R for srun client. -G, --singularity-arg = G Specify additional arguments G for singularity client. These are the commands that can be executed: singularity Compile the Singularity SIF image for DAPHNE ( and transfer it to the target platform ) . build Compile DAPHNE codes ( daphne, DistributedWorker ) using the Singularity image for DAPHNE. It should only be invoked from the code base root directory. It could also be invoked on a target platform after a transfer. package Create the package image with *.daphne scripts and a compressed build/ directory. transfer Transfers ( uploads ) a package to the target platform. start Run workers on remote machines through login node ( deploys this script and runs workers ) . workers Run workers on current login node through Slurm. status Get distributed workers ' status. wait Waits until all workers are up. stop Stops all distributed workers. run [ SCRIPT [ ARGs ]] Run one request on the deployed platform by processing one DaphneDSL SCRIPT file ( default: /dev/stdin ) using optional arguments ( ARGs in script format ) . clean Cleans ( deletes ) the package on the target platform. deploy Deploys everything in one sweep: singularity = >build = >package = >transfer = >start = >wait = >run = >clean. The default connection to the target platform ( HPC ) login node is through OpenSSH, configured by default in ~/.ssh ( see: man ssh_config ) . The default ports for worker peers begin at 50000 ( PORTRANGE_BEGIN ) and the list of PEERS is generated as: PEERS = ( WORKER1_IP:PORTRANGE_BEGIN, WORKER1_IP:PORTRANGE_BEGIN+1, ..., WORKER2_IP:PORTRANGE_BEGIN, WORKER2_IP:PORTRANGE_BEGIN+1, ... ) Logs can be found at [ pathToBuild ] /logs.","title":"How to use deploy-distributed-on-slurm.sh for DAPHNE Packaging, Distributed Deployment, and Management using Slurm"},{"location":"Deploy/#short-examples","text":"The following list presents few examples about how to use the deploy-distributed-on-slurm.sh command. These comprise more hands-on documentation about deployment, including tutorial-like explanation examples about how to package, distributively deploy, manage, and execute workloads using DAPHNE. Builds the Singularity image and uses it to compile the build directory codes, then packages it. ./deploy-distributed-on-slurm.sh singularity && ./deploy-distributed-on-slurm.sh build && ./deploy-distributed-on-slurm.sh package Transfers a package to the target platform through OpenSSH, using login node HPC, user hpc, and identify key hpc.pub. ./deploy-distributed-on-slurm.sh --login HPC --user hpc -i ~/.ssh/hpc.pub transfer Using login node HPC, accesses the target platform and starts workers on remote machines. ./deploy-distributed-on-slurm.sh -l HPC start Runs one request (script called example-time.daphne) on the deployment using 1024 cores, login node HPC, and default OpenSSH configuration. ./deploy-distributed-on-slurm.sh -l HPC -n 1024 run example-time.daphne Executes one request (DaphneDSL script input from standard input) at a running deployed platform, using default singularity/srun configurations. ./deploy-distributed-on-slurm.sh run Deploys once at the target platform through OpenSSH using default login node (localhost), then cleans. ./deploy-distributed-on-slurm.sh deploy -n 10 Starts workers at a running deployed platform using custom srun arguments (2 hours dual-core with 10G memory). ./deploy-distributed-on-slurm.sh workers -R = \"-t 120 --mem-per-cpu=10G --cpu-bind=cores --cpus-per-task=2\" Executes a request with custom srun arguments (30 minutes single-core). ./deploy-distributed-on-slurm.sh run -R = \"--time=30 --cpu-bind=cores --nodes=1 --ntasks-per-node=1 --cpus-per-task=1\" Example request job from a pipe. cat ../scripts/examples/hello-world.daph | ./deploy-distributed-on-slurm.sh run","title":"Short Examples"},{"location":"Deploy/#scenario-usage-example","text":"Here is a scenario usage as a longer example demo. Fetch the code from the latest GitHub code repository. function compile () { git clone --recursive git@github.com:daphne-eu/daphne.git 2 > & 1 | tee daphne- $( date +%F-%T ) .log cd daphne/deploy ./deploy-distributed-on-slurm.sh singularity # creates the Singularity container image ./deploy-distributed-on-slurm.sh build # Builds the daphne codes using the container } compile Package the built targets (binaries) to packet file daphne-package.tgz . ./deploy-distributed-on-slurm.sh package Transfer the packet file daphne-package.tgz to HPC (Slurm) with OpenSSH key ~/.ssh/hpc.pub and unpack it. ./deploy-distributed-on-slurm.sh --login HPC --user $USER -i ~/.ssh/hpc.pub transfer E.g., for EuroHPC Vega, use the instance, if your username matches the one at Vega and the key is ~/.ssh/hpc.pub : ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub transfer Start the workers from the local computer by logging into the HPC login node: ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub start Starting a main target on the HPC (Slurm) and connecting it with the started workers, to execute payload from the stream. cat ../scripts/examples/hello-world.daph | ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub run Starting a main target on the HPC (Slurm) and connecting it with the started workers, to execute payload from a file. ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub run example-time.daphne Stopping all workers on the HPC (Slurm). ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub stop Cleaning the uploaded targets from the HPC login node. ./deploy-distributed-on-slurm.sh --login login.vega.izum.si --user $USER -i ~/.ssh/hpc.pub clean","title":"Scenario Usage Example"},{"location":"DistributedRuntime/","text":"Distributed Runtime Running DAPHNE on the Distributed Runtime Background Daphne supports execution in a distributed fashion. Utilizing the Daphne Distributed Runtime does not require any changes to the DaphneDSL script. Similar to the local vectorized engine ( here, section 4 ), the compiler automatically fuses operations and creates pipelines for the distributed runtime, which then uses multiple distributed nodes (workers) that work on their local data, while a main node, the coordinator, is responsible for transferring the data and code to be executed. As mentioned above, changes at DaphneDSL code are not needed, however the user is required to start the workers, either manually or using an HPC tool as SLURM (scripts that start the workers locally or remotely, natively or not, can be found here ). Scope This document focuses on: how to start distributed workers executing Daphne scripts on the distributed runtime DAPHNE's distributed runtime has two different backends. This page explains how things work with the gRPC backend . A brief introduction to the other backend using OpenMPI can be viewed in this document . Build the Daphne prototype First you need to build the Daphne prototype. This doc assumes that you already built Daphne and can run it locally. If you need help building or running Daphne see here . Building the Distributed Worker The Daphne distributed worker is a different executable which can be build using the build-script and providing the --target argument: ./build.sh --target DistributedWorker Start distributed workers Before executing Daphne on the distributed runtime, worker nodes must first be up and running. You can start a distributed worker within the Daphne prototype directory as follows: # IP:PORT is the IP and PORT the worker server will be listening too ./bin/DistributedWorker IP:PORT There are scripts that automate this task and can help running multiple workers at once locally or even utilizing tools (like SLURM) in HPC environments. Each worker can be left running and reused for multiple scripts and pipeline executions (however, for now they might run into memory issues, see Limitations section below). Each worker can be terminated by sending a SIGINT (Ctrl+C) or by using the scripts mentioned above. Set up environmental variables After setting up the workers, before we run Daphne we need to specify which IPs and ports the workers are listening too. For now we use an environmental variable called DISTRIBUTED_WORKERS where we list IPs and ports of the workers separated by a comma. # Example for 2 workers. # Worker1 listens to localhost:5000 # Worker2 listens to localhost:5001 export DISTRIBUTED_WORKERS = localhost:5000,localhost:5001 Run DAPHNE using the distributed runtime Now that we have all workers up and running and the environmental variable is set we can run Daphne. We enable the distributed runtime by specifying the flag argument --distributed . (*) Note that we execute Daphne from the same bash shell we've set up the environmental variable DISTRIBUTED_WORKERS . ./bin/daphne --distributed ./example.script For now only asynchronous-gRPC is implemented as a distributed backend and selection is hardcoded here . Example On one terminal with start up a Distributed Worker: $./bin/DistributedWorker localhost:5000 Started Distributed Worker on ` localhost:5000 ` On another terminal we set the environment variable and execute script distributed.daph : $ export DISTRIBUTED_WORKERS = localhost:5000 $ ./bin/daphne --distributed ./scripts/example/distributed.daph Current limitations Distributed Runtime is still under development and currently there are various limitations. Most of these limitations will be fixed in the future. Distributed runtime for now heavily depends on the vectorized engine of Daphne and how pipelines are created and multiple operations are fused together (more here - section 4 ). This causes some limitations related to pipeline creation (e.g. not supporting pipelines with different result outputs or pipelines with no outputs). For now distributed runtime only supports DenseMatrix types and value types double - DenseMatrix<double> (issue #194 ). A Daphne pipeline input might exist multiple times in the input array. For now this is not supported. In the future similar pipelines will simply omit multiple pipeline inputs and each one will be provided only once. Garbage collection at worker (node) level is not implemented yet. This means that after some time the workers can fill up their memory completely, requiring a restart. What Next? You might want to have a look at the distributed runtime development guideline the contribution guidelines the open distributed related issues","title":"DistributedRuntime"},{"location":"DistributedRuntime/#distributed-runtime","text":"Running DAPHNE on the Distributed Runtime","title":"Distributed Runtime"},{"location":"DistributedRuntime/#background","text":"Daphne supports execution in a distributed fashion. Utilizing the Daphne Distributed Runtime does not require any changes to the DaphneDSL script. Similar to the local vectorized engine ( here, section 4 ), the compiler automatically fuses operations and creates pipelines for the distributed runtime, which then uses multiple distributed nodes (workers) that work on their local data, while a main node, the coordinator, is responsible for transferring the data and code to be executed. As mentioned above, changes at DaphneDSL code are not needed, however the user is required to start the workers, either manually or using an HPC tool as SLURM (scripts that start the workers locally or remotely, natively or not, can be found here ).","title":"Background"},{"location":"DistributedRuntime/#scope","text":"This document focuses on: how to start distributed workers executing Daphne scripts on the distributed runtime DAPHNE's distributed runtime has two different backends. This page explains how things work with the gRPC backend . A brief introduction to the other backend using OpenMPI can be viewed in this document .","title":"Scope"},{"location":"DistributedRuntime/#build-the-daphne-prototype","text":"First you need to build the Daphne prototype. This doc assumes that you already built Daphne and can run it locally. If you need help building or running Daphne see here .","title":"Build the Daphne prototype"},{"location":"DistributedRuntime/#building-the-distributed-worker","text":"The Daphne distributed worker is a different executable which can be build using the build-script and providing the --target argument: ./build.sh --target DistributedWorker","title":"Building the Distributed Worker"},{"location":"DistributedRuntime/#start-distributed-workers","text":"Before executing Daphne on the distributed runtime, worker nodes must first be up and running. You can start a distributed worker within the Daphne prototype directory as follows: # IP:PORT is the IP and PORT the worker server will be listening too ./bin/DistributedWorker IP:PORT There are scripts that automate this task and can help running multiple workers at once locally or even utilizing tools (like SLURM) in HPC environments. Each worker can be left running and reused for multiple scripts and pipeline executions (however, for now they might run into memory issues, see Limitations section below). Each worker can be terminated by sending a SIGINT (Ctrl+C) or by using the scripts mentioned above.","title":"Start distributed workers"},{"location":"DistributedRuntime/#set-up-environmental-variables","text":"After setting up the workers, before we run Daphne we need to specify which IPs and ports the workers are listening too. For now we use an environmental variable called DISTRIBUTED_WORKERS where we list IPs and ports of the workers separated by a comma. # Example for 2 workers. # Worker1 listens to localhost:5000 # Worker2 listens to localhost:5001 export DISTRIBUTED_WORKERS = localhost:5000,localhost:5001","title":"Set up environmental variables"},{"location":"DistributedRuntime/#run-daphne-using-the-distributed-runtime","text":"Now that we have all workers up and running and the environmental variable is set we can run Daphne. We enable the distributed runtime by specifying the flag argument --distributed . (*) Note that we execute Daphne from the same bash shell we've set up the environmental variable DISTRIBUTED_WORKERS . ./bin/daphne --distributed ./example.script For now only asynchronous-gRPC is implemented as a distributed backend and selection is hardcoded here .","title":"Run DAPHNE using the distributed runtime"},{"location":"DistributedRuntime/#example","text":"On one terminal with start up a Distributed Worker: $./bin/DistributedWorker localhost:5000 Started Distributed Worker on ` localhost:5000 ` On another terminal we set the environment variable and execute script distributed.daph : $ export DISTRIBUTED_WORKERS = localhost:5000 $ ./bin/daphne --distributed ./scripts/example/distributed.daph","title":"Example"},{"location":"DistributedRuntime/#current-limitations","text":"Distributed Runtime is still under development and currently there are various limitations. Most of these limitations will be fixed in the future. Distributed runtime for now heavily depends on the vectorized engine of Daphne and how pipelines are created and multiple operations are fused together (more here - section 4 ). This causes some limitations related to pipeline creation (e.g. not supporting pipelines with different result outputs or pipelines with no outputs). For now distributed runtime only supports DenseMatrix types and value types double - DenseMatrix<double> (issue #194 ). A Daphne pipeline input might exist multiple times in the input array. For now this is not supported. In the future similar pipelines will simply omit multiple pipeline inputs and each one will be provided only once. Garbage collection at worker (node) level is not implemented yet. This means that after some time the workers can fill up their memory completely, requiring a restart.","title":"Current limitations"},{"location":"DistributedRuntime/#what-next","text":"You might want to have a look at the distributed runtime development guideline the contribution guidelines the open distributed related issues","title":"What Next?"},{"location":"FPGAconfiguration/","text":"FPGA Configuration FPGA configuration for usage in DAPHNE System requirments Daphne build script for FPGA kernels support requires additional QUARTUSDIR system variable definition. Example command is presented in fpga-build-env.sh or in the following command: export QUARTUSDIR=/opt/intel/intelFPGA_pro/21.4 To build the Daphne with the FPGA support -fpgaopencl flag has to be used: ./build.sh --fpgaopenc To run developed or precompiled, included in Daphne repository FPGA OpenCL kernels an installedand configured FPGA device is required. Our example kernels have been tested using Intel(R) PAC D5005 card DAPHNE contains some example linear algebra kernels developed using T2SP framework . Example precompiled FPGA kernels can be usedon DAPHNE DSL description level. To prepare the system for the precompiled FPGA kernels some FPGA and OpenCL system variables are required. The easiest way to set up required varables is to use the init_opencl.sh script from installed Intel(R) Quartus sowtware or from the Intel(R) OpenCL RTE or Intel(R) OpenCL SDK packages. Example script usage: source /opt/intel/intelFPGA_pro/21.4/hld/init_opencl.sh For additional details please look into the Intel guide or SDK for openCL . Precompiled FPGA Kernels To use a precompiled FPGA kernel a FPGA image is required (*.aocx). FPGA device has to programmed with particular image which contains required kernel implementation. Example FPGA programming command using example FPGA image: aocl program acl0 src/runtime/local/kernels/FPGAOPENCL/bitstreams/sgemm.aocx Additionally the BITSTREAM variable has to be defind in the system. Please look into the following example: export BITSTREAM=src/runtime/local/kernels/FPGAOPENCL/bitstreams/sgemm.aocx When another FPGA image contains implementation for another required computational kernel then FPGA device has to be reprogrammed and BITSTREAM variable value has to be changed.","title":"FPGAconfiguration"},{"location":"FPGAconfiguration/#fpga-configuration","text":"FPGA configuration for usage in DAPHNE","title":"FPGA Configuration"},{"location":"FPGAconfiguration/#system-requirments","text":"Daphne build script for FPGA kernels support requires additional QUARTUSDIR system variable definition. Example command is presented in fpga-build-env.sh or in the following command: export QUARTUSDIR=/opt/intel/intelFPGA_pro/21.4 To build the Daphne with the FPGA support -fpgaopencl flag has to be used: ./build.sh --fpgaopenc To run developed or precompiled, included in Daphne repository FPGA OpenCL kernels an installedand configured FPGA device is required. Our example kernels have been tested using Intel(R) PAC D5005 card DAPHNE contains some example linear algebra kernels developed using T2SP framework . Example precompiled FPGA kernels can be usedon DAPHNE DSL description level. To prepare the system for the precompiled FPGA kernels some FPGA and OpenCL system variables are required. The easiest way to set up required varables is to use the init_opencl.sh script from installed Intel(R) Quartus sowtware or from the Intel(R) OpenCL RTE or Intel(R) OpenCL SDK packages. Example script usage: source /opt/intel/intelFPGA_pro/21.4/hld/init_opencl.sh For additional details please look into the Intel guide or SDK for openCL .","title":"System requirments"},{"location":"FPGAconfiguration/#precompiled-fpga-kernels","text":"To use a precompiled FPGA kernel a FPGA image is required (*.aocx). FPGA device has to programmed with particular image which contains required kernel implementation. Example FPGA programming command using example FPGA image: aocl program acl0 src/runtime/local/kernels/FPGAOPENCL/bitstreams/sgemm.aocx Additionally the BITSTREAM variable has to be defind in the system. Please look into the following example: export BITSTREAM=src/runtime/local/kernels/FPGAOPENCL/bitstreams/sgemm.aocx When another FPGA image contains implementation for another required computational kernel then FPGA device has to be reprogrammed and BITSTREAM variable value has to be changed.","title":"Precompiled FPGA Kernels"},{"location":"FileMetaDataFormat/","text":"Read and Write Data Reading and writing (meta) data in Daphne. When loading data with read() in a DaphneDSL script, the system expects a file with the same file name in the same directory as the data file with an additional extension .meta . This file contains a description of meta data stored in JSON format. There are two slightly varying ways of specifying meta data depending on whether there is a schema for the columns (e.g., a data frame - the corresponding C++ type is the Frame class) or not (this data can currently (as of version 0.1) be loaded as DenseMatrix<VT> or CSRMatrix<VT> where VT is the value type template parameter). If data is written from a DaphneDSL script via write() , the meta data file will be written to the corresponding filename.meta . Currently supported JSON fields Name Expected Data Allowed values numRows Integer # number of rows numCols Integer # number of columns valueType String si8, si32, si64, // signed integers (intX_t) ui8, ui32, ui64, // unsigned integers (uintx_t) f32, f64, // floating point (float, double) Contained within schema this may be an empty string. In this case all columns of a data frame will have the same valueType defined outside of the schema data field numNonZeros Integer # number of non-zeros (optional) schema JSON nested elements of \"label\" and \"valueType\" fields label String column name/header (optional, may be empty string \"\") Matrix Example The example below describes a 2 by 4 dense matrix of double precision values. Matrix CSV -0.1,-0.2,0.1,0.2 3.14,5.41,6.22216,5 Matrix Metadata { \"numRows\" : 2 , \"numCols\" : 4 , \"valueType\" : \"f64\" , \"numNonZeros\" : 0 } Data Frame Example The example below describes a 2 by 2 Frame with signed integers in the first columns named foo and double precision values in the second column named bar. Data Frame CSV 1,0.5 2,1.0 Data Frame Metadata { \"numRows\" : 2 , \"numCols\" : 2 , \"schema\" : [ { \"label\" : \"foo\" , \"valueType\" : \"si64\" }, { \"label\" : \"bar\" , \"valueType\" : \"f64\" } ] } Data Frame Meta Data with Default ValueType { \"numRows\" : 5 , \"numCols\" : 3 , \"valueType\" : \"f32\" , \"schema\" : [ { \"label\" : \"a\" , \"valueType\" : \"\" }, { \"label\" : \"bc\" , \"valueType\" : \"\" }, { \"label\" : \"def\" , \"valueType\" : \"\" } ] } Data Frame Meta Data with Empty Labels { \"numRows\" : 5 , \"numCols\" : 3 , \"schema\" : [ { \"label\" : \"\" , \"valueType\" : \"si64\" }, { \"label\" : \"\" , \"valueType\" : \"f64\" }, { \"label\" : \"\" , \"valueType\" : \"ui32\" } ] }","title":"FileMetaDataFormat"},{"location":"FileMetaDataFormat/#read-and-write-data","text":"Reading and writing (meta) data in Daphne. When loading data with read() in a DaphneDSL script, the system expects a file with the same file name in the same directory as the data file with an additional extension .meta . This file contains a description of meta data stored in JSON format. There are two slightly varying ways of specifying meta data depending on whether there is a schema for the columns (e.g., a data frame - the corresponding C++ type is the Frame class) or not (this data can currently (as of version 0.1) be loaded as DenseMatrix<VT> or CSRMatrix<VT> where VT is the value type template parameter). If data is written from a DaphneDSL script via write() , the meta data file will be written to the corresponding filename.meta .","title":"Read and Write Data"},{"location":"FileMetaDataFormat/#currently-supported-json-fields","text":"Name Expected Data Allowed values numRows Integer # number of rows numCols Integer # number of columns valueType String si8, si32, si64, // signed integers (intX_t) ui8, ui32, ui64, // unsigned integers (uintx_t) f32, f64, // floating point (float, double) Contained within schema this may be an empty string. In this case all columns of a data frame will have the same valueType defined outside of the schema data field numNonZeros Integer # number of non-zeros (optional) schema JSON nested elements of \"label\" and \"valueType\" fields label String column name/header (optional, may be empty string \"\")","title":"Currently supported JSON fields"},{"location":"FileMetaDataFormat/#matrix-example","text":"The example below describes a 2 by 4 dense matrix of double precision values.","title":"Matrix Example"},{"location":"FileMetaDataFormat/#matrix-csv","text":"-0.1,-0.2,0.1,0.2 3.14,5.41,6.22216,5","title":"Matrix CSV"},{"location":"FileMetaDataFormat/#matrix-metadata","text":"{ \"numRows\" : 2 , \"numCols\" : 4 , \"valueType\" : \"f64\" , \"numNonZeros\" : 0 }","title":"Matrix Metadata"},{"location":"FileMetaDataFormat/#data-frame-example","text":"The example below describes a 2 by 2 Frame with signed integers in the first columns named foo and double precision values in the second column named bar.","title":"Data Frame Example"},{"location":"FileMetaDataFormat/#data-frame-csv","text":"1,0.5 2,1.0","title":"Data Frame CSV"},{"location":"FileMetaDataFormat/#data-frame-metadata","text":"{ \"numRows\" : 2 , \"numCols\" : 2 , \"schema\" : [ { \"label\" : \"foo\" , \"valueType\" : \"si64\" }, { \"label\" : \"bar\" , \"valueType\" : \"f64\" } ] }","title":"Data Frame Metadata"},{"location":"FileMetaDataFormat/#data-frame-meta-data-with-default-valuetype","text":"{ \"numRows\" : 5 , \"numCols\" : 3 , \"valueType\" : \"f32\" , \"schema\" : [ { \"label\" : \"a\" , \"valueType\" : \"\" }, { \"label\" : \"bc\" , \"valueType\" : \"\" }, { \"label\" : \"def\" , \"valueType\" : \"\" } ] }","title":"Data Frame Meta Data with Default ValueType"},{"location":"FileMetaDataFormat/#data-frame-meta-data-with-empty-labels","text":"{ \"numRows\" : 5 , \"numCols\" : 3 , \"schema\" : [ { \"label\" : \"\" , \"valueType\" : \"si64\" }, { \"label\" : \"\" , \"valueType\" : \"f64\" }, { \"label\" : \"\" , \"valueType\" : \"ui32\" } ] }","title":"Data Frame Meta Data with Empty Labels"},{"location":"GettingStarted/","text":"Getting Started This document summarizes everything you need to know to get started with using or extending the DAPHNE system. System Requirements Please ensure that your development system meets the following requirements before trying to build the system. (*) You can view the version numbers as an orientation rather than a strict requirement. Newer versions should work as well, older versions might work as well. Operating system OS distribution/version known to work (*) Comment GNU/Linux Manjaro Last checked in January 2023 GNU/Linux Ubuntu 20.04 - 22.10 All versions in that range work. 20.04 needs CMake installed from Snap. GNU/Linux Ubuntu 18.04 Used with Intel PAC D5005 FPGA, custom toolchain needed MS Windows 10 Build 19041, 11 Should work in Ubuntu WSL, using the provided Docker images is recommended Windows Installing WSL and Docker should be straight forward using the documentation proveded by Microsoft . On an installed WSL container launching DAPHNE via Docker (see below) should work the same way as in a native installation. Software tool/lib version known to work (*) comment GCC/G++ 9.3.0 Last checked version: 12.2 clang 10.0.0 cmake 3.20 On Ubuntu 20.04, install by sudo snap install cmake --classic to fulfill the version requirement; apt provides only version 3.16.3. git 2.25.1 libssl-dev 1.1.1 Dependency introduced while optimizing grpc build (which used to build ssl unnecessarily) libpfm4-dev 4.10 This dependency is needed for profiling support [DAPHNE-#479] lld 10.0.0 ninja 1.10.0 pkg-config 0.29.1 python3 3.8.5 numpy 1.19.5 pandas 0.25.3 java (e.g. openjdk) 11 (1.7 should be fine) gfortran 9.3.0 uuid-dev wget Used to fetch additional dependencies and other artefacts jq json commandline processor used in docker image generation scripts *** *** *** CUDA SDK 11.7.1 Optional for CUDA ops OneAPI SDK 2022.x Optional for OneAPI ops Intel FPGA SDK or OneAPI FPGA Add-On 2022.x Optional for FPGAOPENCL ops Hardware about 7.5 GB of free disk space to build from source (mostly due to dependencies) Optional: NVidia GPU for CUDA ops (tested on Pascal and newer architectures); 8GB for CUDA SDK Intel GPU for OneAPI ops (tested on Coffeelake graphics); 23 GB for OneAPI Intel FPGA for FPGAOPENCL ops (tested on PAC D5005 accelerator); 23 GB for OneAPI Obtaining the Source Code The DAPHNE system is based on MLIR, which is a part of the LLVM monorepo. The LLVM monorepo is included in this repository as a submodule. Thus, clone this repository as follows to also clone the submodule: git clone --recursive https://github.com/daphne-eu/daphne.git Upstream changes to this repository might contain changes to the submodule (we might have upgraded to a newer version of MLIR/LLVM). Thus, please pull as follows: # in git >= 2.14 git pull --recurse-submodules # in git < 2.14 git pull && git submodule update --init --recursive # or use this little convenience script ./pull.sh Building the DAPHNE system Simply build the system using the build-script without any arguments: ./build.sh When you do this the first time, or when there were updates to the LLVM submodule, this will also download and build the third-party material, which might increase the build time significantly. Subsequent builds, e.g., when you changed something in this repository, will be much faster. If the build fails in between (e.g., due to missing packages), multiple build directories (e.g., daphne, antlr, llvm) require cleanup. To only remove build output use the following two commands: ./build.sh --clean ./build.sh --cleanDeps If you want to remove downloaded and extracted artifacts, use this: ./build.sh --cleanCache For convenience, you can call the following to remove them all. ./build.sh --cleanAll See this page for more information. Setting up the environment As DAPHNE uses shared libraries, these need to be found by the operating system's loader to link them at runtime. Since most DAPHNE setups will not end up in one of the standard directories (e.g., /usr/local/lib ), environment variables are a convenient way to set everything up without interfering with system installations (where you might not even have administrative privileges to do so). # from your cloned DAPHNE repo or your otherwise extracted sources/binaries: export DAPHNE_ROOT = $PWD export LD_LIBRARY_PATH = $DAPHNE_ROOT /lib: $DAPHNE_ROOT /thirdparty/installed/lib: $LD_LIBRARY_PATH # optionally, you can add the location of the DAPHNE executable to your PATH: export PATH = $DAPHNE_ROOT /bin: $PATH If you're running/compiling DAPHNE from a container you'll most probably * not * need to set these environment variables (unless you have reason to customize your setup - then it is assumed that you know what you are doing). Running the Tests ./test.sh We use catch2 as the unit test framework. You can use all command line arguments of catch2 with test.sh . Running the DAPHNE system Write a little DaphneDSL script or use scripts/examples/hello-world.daph ... x = 1 ; y = 2 ; print ( x + y ); m = rand ( 2 , 3 , 100.0 , 200.0 , 1.0 , - 1 ); print ( m ); print ( m + m ); print ( t ( m )); ... and execute it as follows: bin/daphne scripts/examples/hello-world.daph (This command works if Daphne is run after building from source. Omit \"build\" in the path to the Daphne binary if executed from the binary distribution). Optionally flags like --cuda can be added after the daphne command and before the script file to activate support for accelerated ops (see software requirements above and build instructions ). For further flags that can be set at runtime to activate additional functionality, run daphne --help . Building and Running with Containers [Alternative path for building and running the system and the tests] If one wants to avoid installing dependencies and avoid conflicting with his/her existing installed libraries, one may use containers. you need to install Docker or Singularity: Docker version 20.10.2 or higher | Singularity version 3.7.0-1.el7 or higher are sufficient you can use the provided docker files and scripts to create and run DAPHNE. A full description on containers is available in the containers subdirectory. The following recreates all images provided by daphneeu cd container ./build-containers.sh Running in an interactive container can be done with this run script, which takes care of mounting your current directory and handling permissions: # please customize this script first ./containers/run-docker-example.sh For more about building and running with containers, refer (once again) to the directory containers/ and its README.md . For documentation about using containers in conjunction with our cluster deployment scripts, refer to Deploy.md . Exploring the Source Code As an entry point for exploring the source code , you might want to have a look at the code behind the daphne executable, which can be found in src/api/cli/daphne.cpp . On the top-level, there are the following directories: bin : after compilation, generated binaries will be placed here (e.g., daphne) build : temporary build output containers : scripts and configuration files to get/build/run with Docker or Singularity containers deploy : shell scripts to ease deployment in SLURM clusters doc : documentation written in markdown (e.g., what you are reading at the moment) lib : after compilation, generated library files will be placed here (e.g., libAllKernels.so, libCUDAKernels.so, ...) scripts : a collection of algorithms and examples written in DAPHNE's own domain specific language ( DaphneDSL ) src : the actual source code, subdivided into the individual components of the system test : test cases thirdparty : required external software What Next? You might want to have a look at the documentation the contribution guidelines the open issues","title":"GettingStarted"},{"location":"GettingStarted/#getting-started","text":"This document summarizes everything you need to know to get started with using or extending the DAPHNE system.","title":"Getting Started"},{"location":"GettingStarted/#system-requirements","text":"Please ensure that your development system meets the following requirements before trying to build the system. (*) You can view the version numbers as an orientation rather than a strict requirement. Newer versions should work as well, older versions might work as well.","title":"System Requirements"},{"location":"GettingStarted/#operating-system","text":"OS distribution/version known to work (*) Comment GNU/Linux Manjaro Last checked in January 2023 GNU/Linux Ubuntu 20.04 - 22.10 All versions in that range work. 20.04 needs CMake installed from Snap. GNU/Linux Ubuntu 18.04 Used with Intel PAC D5005 FPGA, custom toolchain needed MS Windows 10 Build 19041, 11 Should work in Ubuntu WSL, using the provided Docker images is recommended","title":"Operating system"},{"location":"GettingStarted/#windows","text":"Installing WSL and Docker should be straight forward using the documentation proveded by Microsoft . On an installed WSL container launching DAPHNE via Docker (see below) should work the same way as in a native installation.","title":"Windows"},{"location":"GettingStarted/#software","text":"tool/lib version known to work (*) comment GCC/G++ 9.3.0 Last checked version: 12.2 clang 10.0.0 cmake 3.20 On Ubuntu 20.04, install by sudo snap install cmake --classic to fulfill the version requirement; apt provides only version 3.16.3. git 2.25.1 libssl-dev 1.1.1 Dependency introduced while optimizing grpc build (which used to build ssl unnecessarily) libpfm4-dev 4.10 This dependency is needed for profiling support [DAPHNE-#479] lld 10.0.0 ninja 1.10.0 pkg-config 0.29.1 python3 3.8.5 numpy 1.19.5 pandas 0.25.3 java (e.g. openjdk) 11 (1.7 should be fine) gfortran 9.3.0 uuid-dev wget Used to fetch additional dependencies and other artefacts jq json commandline processor used in docker image generation scripts *** *** *** CUDA SDK 11.7.1 Optional for CUDA ops OneAPI SDK 2022.x Optional for OneAPI ops Intel FPGA SDK or OneAPI FPGA Add-On 2022.x Optional for FPGAOPENCL ops","title":"Software"},{"location":"GettingStarted/#hardware","text":"about 7.5 GB of free disk space to build from source (mostly due to dependencies) Optional: NVidia GPU for CUDA ops (tested on Pascal and newer architectures); 8GB for CUDA SDK Intel GPU for OneAPI ops (tested on Coffeelake graphics); 23 GB for OneAPI Intel FPGA for FPGAOPENCL ops (tested on PAC D5005 accelerator); 23 GB for OneAPI","title":"Hardware"},{"location":"GettingStarted/#obtaining-the-source-code","text":"The DAPHNE system is based on MLIR, which is a part of the LLVM monorepo. The LLVM monorepo is included in this repository as a submodule. Thus, clone this repository as follows to also clone the submodule: git clone --recursive https://github.com/daphne-eu/daphne.git Upstream changes to this repository might contain changes to the submodule (we might have upgraded to a newer version of MLIR/LLVM). Thus, please pull as follows: # in git >= 2.14 git pull --recurse-submodules # in git < 2.14 git pull && git submodule update --init --recursive # or use this little convenience script ./pull.sh","title":"Obtaining the Source Code"},{"location":"GettingStarted/#building-the-daphne-system","text":"Simply build the system using the build-script without any arguments: ./build.sh When you do this the first time, or when there were updates to the LLVM submodule, this will also download and build the third-party material, which might increase the build time significantly. Subsequent builds, e.g., when you changed something in this repository, will be much faster. If the build fails in between (e.g., due to missing packages), multiple build directories (e.g., daphne, antlr, llvm) require cleanup. To only remove build output use the following two commands: ./build.sh --clean ./build.sh --cleanDeps If you want to remove downloaded and extracted artifacts, use this: ./build.sh --cleanCache For convenience, you can call the following to remove them all. ./build.sh --cleanAll See this page for more information.","title":"Building the DAPHNE system"},{"location":"GettingStarted/#setting-up-the-environment","text":"As DAPHNE uses shared libraries, these need to be found by the operating system's loader to link them at runtime. Since most DAPHNE setups will not end up in one of the standard directories (e.g., /usr/local/lib ), environment variables are a convenient way to set everything up without interfering with system installations (where you might not even have administrative privileges to do so). # from your cloned DAPHNE repo or your otherwise extracted sources/binaries: export DAPHNE_ROOT = $PWD export LD_LIBRARY_PATH = $DAPHNE_ROOT /lib: $DAPHNE_ROOT /thirdparty/installed/lib: $LD_LIBRARY_PATH # optionally, you can add the location of the DAPHNE executable to your PATH: export PATH = $DAPHNE_ROOT /bin: $PATH If you're running/compiling DAPHNE from a container you'll most probably * not * need to set these environment variables (unless you have reason to customize your setup - then it is assumed that you know what you are doing).","title":"Setting up the environment"},{"location":"GettingStarted/#running-the-tests","text":"./test.sh We use catch2 as the unit test framework. You can use all command line arguments of catch2 with test.sh .","title":"Running the Tests"},{"location":"GettingStarted/#running-the-daphne-system","text":"Write a little DaphneDSL script or use scripts/examples/hello-world.daph ... x = 1 ; y = 2 ; print ( x + y ); m = rand ( 2 , 3 , 100.0 , 200.0 , 1.0 , - 1 ); print ( m ); print ( m + m ); print ( t ( m )); ... and execute it as follows: bin/daphne scripts/examples/hello-world.daph (This command works if Daphne is run after building from source. Omit \"build\" in the path to the Daphne binary if executed from the binary distribution). Optionally flags like --cuda can be added after the daphne command and before the script file to activate support for accelerated ops (see software requirements above and build instructions ). For further flags that can be set at runtime to activate additional functionality, run daphne --help .","title":"Running the DAPHNE system"},{"location":"GettingStarted/#building-and-running-with-containers-alternative-path-for-building-and-running-the-system-and-the-tests","text":"If one wants to avoid installing dependencies and avoid conflicting with his/her existing installed libraries, one may use containers. you need to install Docker or Singularity: Docker version 20.10.2 or higher | Singularity version 3.7.0-1.el7 or higher are sufficient you can use the provided docker files and scripts to create and run DAPHNE. A full description on containers is available in the containers subdirectory. The following recreates all images provided by daphneeu cd container ./build-containers.sh Running in an interactive container can be done with this run script, which takes care of mounting your current directory and handling permissions: # please customize this script first ./containers/run-docker-example.sh For more about building and running with containers, refer (once again) to the directory containers/ and its README.md . For documentation about using containers in conjunction with our cluster deployment scripts, refer to Deploy.md .","title":"Building and Running with Containers [Alternative path for building and running the system and the tests]"},{"location":"GettingStarted/#exploring-the-source-code","text":"As an entry point for exploring the source code , you might want to have a look at the code behind the daphne executable, which can be found in src/api/cli/daphne.cpp . On the top-level, there are the following directories: bin : after compilation, generated binaries will be placed here (e.g., daphne) build : temporary build output containers : scripts and configuration files to get/build/run with Docker or Singularity containers deploy : shell scripts to ease deployment in SLURM clusters doc : documentation written in markdown (e.g., what you are reading at the moment) lib : after compilation, generated library files will be placed here (e.g., libAllKernels.so, libCUDAKernels.so, ...) scripts : a collection of algorithms and examples written in DAPHNE's own domain specific language ( DaphneDSL ) src : the actual source code, subdivided into the individual components of the system test : test cases thirdparty : required external software","title":"Exploring the Source Code"},{"location":"GettingStarted/#what-next","text":"You might want to have a look at the documentation the contribution guidelines the open issues","title":"What Next?"},{"location":"MPI-Usage/","text":"MPI Usage About employing MPI as a distributed runtime backend. The DAPHNE runtime system is designed with the goal of supporting various distributed runtime that relies on various technologies, e.g. MPI and RPC. This document shows how a DAPHNE user can execute DAPHNE scripts on a distributed computing environment with the MPI backend implementation of the DAPHNE runtime system. This document assumes that the DAPHNE was build with the --mpi options, if this is not the case please rebuild DAPHNE with the --mpi option ./build.sh --mpi The DAPHNE build script uses Open MPI . The DAPHNE build script does not configure the Open MPI installation with the SLURM support option. For users who want to add the SLURM, please visit the Open MPI documentation (adding --with-slurm to the build command of the Open MPI libbrary) and edit the DAPHNE build script. Also, users who wants to use other MPI implementations e.g., Intel MPI may edit the corresponding part in the DAPHNE build script. When DAPHNE is Installed Natively (w/o Container) Ensure that your system knows about the installed MPI -- The PATH and LD_LIBRARY_PATH environment variable has to be updated as follows export PATH = $PATH :<DAPHNE_INSTALLATION>/thirdparty/installed/bin/ export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :<DAPHNE_INSTALLATION>//thirdparty/installed/lib/ Please do not forget to replace <DAPHNE_INSTALLATION> with the actual path Run basic example @ /examples/matrix_addition_for_mpi.daph as follows mpirun -np 10 ./bin/daphne --distributed --dist_backend = MPI scripts/examples/matrix_addition_for_mpi.daph The command above executes 10 processes locally on one machine. In order to run on a distributed system , you need to provide the machine names or the machinefile which contains the machine names. For instance assuming that my_hostfile is a text file that contains machine names mpirun -np 10 --hostfile my_hostfile ./bin/daphne --distributed --dist_backend = MPI scripts/examples/matrix_addition_for_mpi.daph The command above starts 10 processes distributed on following the hosts in the my_hostfile. For more options, please check the Open MPI documentation . From a DAPHNE runtime point of view, the --distributed option tells the DAPHNE runtime system to utilize the distributed backend, while the --dist_backend=MPI indicate the type of the backend implementation. When DAPHNE is Installed with Containers (e.g. singularity) The main difference is that the mpirun command is called at the level of the container as follows mpirun -np 10 singularity exec <singularity-image> daphne/bin/daphne --distributed --dist_backend = MPI --vec --num-threads = 2 daphne/scripts/examples/matrix_addition_for_mpi.daph Please do not forget to replace <singularity-image> with the actual singularity image.","title":"MPI Usage"},{"location":"MPI-Usage/#mpi-usage","text":"About employing MPI as a distributed runtime backend. The DAPHNE runtime system is designed with the goal of supporting various distributed runtime that relies on various technologies, e.g. MPI and RPC. This document shows how a DAPHNE user can execute DAPHNE scripts on a distributed computing environment with the MPI backend implementation of the DAPHNE runtime system. This document assumes that the DAPHNE was build with the --mpi options, if this is not the case please rebuild DAPHNE with the --mpi option ./build.sh --mpi The DAPHNE build script uses Open MPI . The DAPHNE build script does not configure the Open MPI installation with the SLURM support option. For users who want to add the SLURM, please visit the Open MPI documentation (adding --with-slurm to the build command of the Open MPI libbrary) and edit the DAPHNE build script. Also, users who wants to use other MPI implementations e.g., Intel MPI may edit the corresponding part in the DAPHNE build script.","title":"MPI Usage"},{"location":"MPI-Usage/#when-daphne-is-installed-natively-wo-container","text":"Ensure that your system knows about the installed MPI -- The PATH and LD_LIBRARY_PATH environment variable has to be updated as follows export PATH = $PATH :<DAPHNE_INSTALLATION>/thirdparty/installed/bin/ export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :<DAPHNE_INSTALLATION>//thirdparty/installed/lib/ Please do not forget to replace <DAPHNE_INSTALLATION> with the actual path Run basic example @ /examples/matrix_addition_for_mpi.daph as follows mpirun -np 10 ./bin/daphne --distributed --dist_backend = MPI scripts/examples/matrix_addition_for_mpi.daph The command above executes 10 processes locally on one machine. In order to run on a distributed system , you need to provide the machine names or the machinefile which contains the machine names. For instance assuming that my_hostfile is a text file that contains machine names mpirun -np 10 --hostfile my_hostfile ./bin/daphne --distributed --dist_backend = MPI scripts/examples/matrix_addition_for_mpi.daph The command above starts 10 processes distributed on following the hosts in the my_hostfile. For more options, please check the Open MPI documentation . From a DAPHNE runtime point of view, the --distributed option tells the DAPHNE runtime system to utilize the distributed backend, while the --dist_backend=MPI indicate the type of the backend implementation.","title":"When DAPHNE is Installed Natively (w/o Container)"},{"location":"MPI-Usage/#when-daphne-is-installed-with-containers-eg-singularity","text":"The main difference is that the mpirun command is called at the level of the container as follows mpirun -np 10 singularity exec <singularity-image> daphne/bin/daphne --distributed --dist_backend = MPI --vec --num-threads = 2 daphne/scripts/examples/matrix_addition_for_mpi.daph Please do not forget to replace <singularity-image> with the actual singularity image.","title":"When DAPHNE is Installed with Containers (e.g. singularity)"},{"location":"Profiling/","text":"Profiling DAPHNE using PAPI You can profile your DAPHNE script by using the --enable-profiling CLI switch. DAPHNE supports profiling via the PAPI profiling library, specifically the high-level (HL) PAPI API . When run with profiling enabled, the DAPHNE compiler will generate code that automatically starts and stops profiling (via PAPI) at the start and end of the DAPHNE script. You can configure which events to profile via the PAPI_EVENTS environmental variable, e.g.: $ PAPI_EVENTS = \"perf::CYCLES,perf::INSTRUCTIONS,perf::CACHE-REFERENCES,perf::CACHE MISSES,perf::BRANCHES,perf::BRANCH-MI SSES\" PAPI_REPORT = 1 ./daphne --enable-profiling script.daph For more details about the supported events as well as other PAPI-HL configuration options you can check the PAPI HL API documentation . You can also get a list of the supported events on your machine via the papi_native_avail PAPI utility (included in the papi-tools package on Debian-based systems).","title":"Profiling"},{"location":"Profiling/#profiling-daphne-using-papi","text":"You can profile your DAPHNE script by using the --enable-profiling CLI switch. DAPHNE supports profiling via the PAPI profiling library, specifically the high-level (HL) PAPI API . When run with profiling enabled, the DAPHNE compiler will generate code that automatically starts and stops profiling (via PAPI) at the start and end of the DAPHNE script. You can configure which events to profile via the PAPI_EVENTS environmental variable, e.g.: $ PAPI_EVENTS = \"perf::CYCLES,perf::INSTRUCTIONS,perf::CACHE-REFERENCES,perf::CACHE MISSES,perf::BRANCHES,perf::BRANCH-MI SSES\" PAPI_REPORT = 1 ./daphne --enable-profiling script.daph For more details about the supported events as well as other PAPI-HL configuration options you can check the PAPI HL API documentation . You can also get a list of the supported events on your machine via the papi_native_avail PAPI utility (included in the papi-tools package on Debian-based systems).","title":"Profiling DAPHNE using PAPI"},{"location":"Quickstart/","text":"Quickstarting DAPHNE These reduced instructions should get you started by firing up a hello world script from the latest binary release. Download a Binary Release Download and extract daphne-<version>-bin.tgz from the release page . Optionally choose the daphne-cuda-<version>-bin.tgz archive if you want to run DAPHNE with CUDA support (Nvidia Pascal hardware or newer and an installed CUDA SDK are required). Run DAPHNE DAPHNE offers two ways to define integrated data analysis pipelines: DaphneDSL (DAPHNE's domain-specific language) DaphneLib (DAPHNE's Python API) For both ways, we provide lightweight run-scripts that set up the required environment (so your system's shared library loader finds the required .so files) and pass the provided parameters to the daphne / python3 executable. Running a DaphneDSL Script In a bash (or compatible) shell, from the extracted DAPHNE directory, execute this command ./run-daphne.sh scripts/examples/hello-world.daph Optionally you can activate CUDA ops by including --cuda : ./run-daphne.sh --cuda scripts/examples/hello-world.daph Running a Python Script Using DaphneLib In a bash (or compatible) shell, from the extracted DAPHNE directory, execute this command ./run-python.sh scripts/examples/daphnelib/shift-and-scale.py More Details If you are interested in the details, you could have a look at the run-scripts: run-daphne.sh and run-python.sh the example DaphneDSL and DaphneLib scripts: scripts/examples/hello-world.daph scripts/examples/daphnelib/shift-and-scale.py the DAPHNE user configuration: UserConfig.json the DAPHNE help: run-daphne.sh --help What Next? You might want to have a look at a more elaborate getting started guide the documentation DaphneDSL and DaphneLib example scripts in scripts/algorithms/ and scripts/examples/","title":"Quickstart"},{"location":"Quickstart/#quickstarting-daphne","text":"These reduced instructions should get you started by firing up a hello world script from the latest binary release.","title":"Quickstarting DAPHNE"},{"location":"Quickstart/#download-a-binary-release","text":"Download and extract daphne-<version>-bin.tgz from the release page . Optionally choose the daphne-cuda-<version>-bin.tgz archive if you want to run DAPHNE with CUDA support (Nvidia Pascal hardware or newer and an installed CUDA SDK are required).","title":"Download a Binary Release"},{"location":"Quickstart/#run-daphne","text":"DAPHNE offers two ways to define integrated data analysis pipelines: DaphneDSL (DAPHNE's domain-specific language) DaphneLib (DAPHNE's Python API) For both ways, we provide lightweight run-scripts that set up the required environment (so your system's shared library loader finds the required .so files) and pass the provided parameters to the daphne / python3 executable.","title":"Run DAPHNE"},{"location":"Quickstart/#running-a-daphnedsl-script","text":"In a bash (or compatible) shell, from the extracted DAPHNE directory, execute this command ./run-daphne.sh scripts/examples/hello-world.daph Optionally you can activate CUDA ops by including --cuda : ./run-daphne.sh --cuda scripts/examples/hello-world.daph","title":"Running a DaphneDSL Script"},{"location":"Quickstart/#running-a-python-script-using-daphnelib","text":"In a bash (or compatible) shell, from the extracted DAPHNE directory, execute this command ./run-python.sh scripts/examples/daphnelib/shift-and-scale.py","title":"Running a Python Script Using DaphneLib"},{"location":"Quickstart/#more-details","text":"If you are interested in the details, you could have a look at the run-scripts: run-daphne.sh and run-python.sh the example DaphneDSL and DaphneLib scripts: scripts/examples/hello-world.daph scripts/examples/daphnelib/shift-and-scale.py the DAPHNE user configuration: UserConfig.json the DAPHNE help: run-daphne.sh --help","title":"More Details"},{"location":"Quickstart/#what-next","text":"You might want to have a look at a more elaborate getting started guide the documentation DaphneDSL and DaphneLib example scripts in scripts/algorithms/ and scripts/examples/","title":"What Next?"},{"location":"ReleaseScripts/","text":"Release Scripts How to use the release scripts to create binary artifacts This is a quick write-up of how the scripts to create a binary release artifact are meant to be used. The release.sh script will call pack.sh which will call build.sh and test.sh. Only if testing completes successfully, the artifact, a gzipped tar archive (format open for discussion) is created. The command after --githash fetches the git hash of the current commit. The script checks out this git hash and restores the current commit after successful completion. This is a bit of a shortcoming as you have to issue a manual git checkout - if the script fails and terminates early. Signing The release manager will have to sign the artifacts to verify that the provided software has been created by that person. To create an appropriate GPG key, these instructions can be adapted to our needs. The keys of Daphne release managers will be provided in this file . Ideally, future release managers sign each others keys. Key signing is a form of showing that the one key owner trusts the other. The Procedure (Preliminary for v0.1) Get into a bash shell and change to your working copy (aka daphne root) directory. Create the artifacts (plain Daphne): ./release.sh --version 0.1 --githash `git rev-parse HEAD` Create additional artifacts with extra features compiled in: ./release.sh --version 0.1 --githash `git rev-parse HEAD` --feature cuda Note that this adds additional constraints on the binaries (e.g., if CUDA support is compiled in, the executable will fail to load on a system without the CUDA SDK properly installed)_ Copy the artifacts to a machine where you have your top secret signing key installed (can be skipped if this is the build machine): rsync -vuPah <hostname>:path/to/daphne/artifacts . Signing and checksumming: cd artifacts ~/path/to/daphne/release.sh --version 0 .1 --artifact ./daphne-0.1-bin.tgz --gpgkey <GPG_KEY_ID> --githash ` cat daphne-0.1-bin.githash ` repeat for other feature artifacts Tag & push The previous signing command will provide you with two more git commands to tag the commit that the artfiacts were made from and to push these tags to github. This should look something like this: git tag -a -u B28F8F4D 0 .1 312b2b50b4e60b3c5157c3365ec38383d35e28d8 git push git@github.com:corepointer/daphne.git --tags Upload & release : Click the \"create new release\" link on the front page of the Daphne github repository (right column under \"Releases\"). Select the tag for the release, create a title, add release notes (highlights of this release, list of contributors, maybe a detailed change log at the end) Upload the artifacts: All the <filename>.{tgz,tgz.asc,tgz.sha512sum} files before either saving as draft for further polishing or finally release the new version.","title":"ReleaseScripts"},{"location":"ReleaseScripts/#release-scripts","text":"How to use the release scripts to create binary artifacts This is a quick write-up of how the scripts to create a binary release artifact are meant to be used. The release.sh script will call pack.sh which will call build.sh and test.sh. Only if testing completes successfully, the artifact, a gzipped tar archive (format open for discussion) is created. The command after --githash fetches the git hash of the current commit. The script checks out this git hash and restores the current commit after successful completion. This is a bit of a shortcoming as you have to issue a manual git checkout - if the script fails and terminates early.","title":"Release Scripts"},{"location":"ReleaseScripts/#signing","text":"The release manager will have to sign the artifacts to verify that the provided software has been created by that person. To create an appropriate GPG key, these instructions can be adapted to our needs. The keys of Daphne release managers will be provided in this file . Ideally, future release managers sign each others keys. Key signing is a form of showing that the one key owner trusts the other.","title":"Signing"},{"location":"ReleaseScripts/#the-procedure-preliminary-for-v01","text":"Get into a bash shell and change to your working copy (aka daphne root) directory. Create the artifacts (plain Daphne): ./release.sh --version 0.1 --githash `git rev-parse HEAD` Create additional artifacts with extra features compiled in: ./release.sh --version 0.1 --githash `git rev-parse HEAD` --feature cuda Note that this adds additional constraints on the binaries (e.g., if CUDA support is compiled in, the executable will fail to load on a system without the CUDA SDK properly installed)_ Copy the artifacts to a machine where you have your top secret signing key installed (can be skipped if this is the build machine): rsync -vuPah <hostname>:path/to/daphne/artifacts . Signing and checksumming: cd artifacts ~/path/to/daphne/release.sh --version 0 .1 --artifact ./daphne-0.1-bin.tgz --gpgkey <GPG_KEY_ID> --githash ` cat daphne-0.1-bin.githash ` repeat for other feature artifacts Tag & push The previous signing command will provide you with two more git commands to tag the commit that the artfiacts were made from and to push these tags to github. This should look something like this: git tag -a -u B28F8F4D 0 .1 312b2b50b4e60b3c5157c3365ec38383d35e28d8 git push git@github.com:corepointer/daphne.git --tags Upload & release : Click the \"create new release\" link on the front page of the Daphne github repository (right column under \"Releases\"). Select the tag for the release, create a title, add release notes (highlights of this release, list of contributors, maybe a detailed change log at the end) Upload the artifacts: All the <filename>.{tgz,tgz.asc,tgz.sha512sum} files before either saving as draft for further polishing or finally release the new version.","title":"The Procedure (Preliminary for v0.1)"},{"location":"RunningDaphneLocally/","text":"Running DAPHNE Locally Running DAPHNE in a local environment. This document explains how to run DAPHNE on a local machine. For more details on running DAPHNE in a distributed setup, please see the documentation on the distributed runtime and distributed deployment . Before DAPHNE can be executed, the system must be built using ./build.sh (for more details see Getting Started ). The main executable of the DAPHNE system is bin/daphne . The general scheme of an invocation of DAPHNE looks as follows: bin/daphne [ options ] script [ arguments ] Where script is a DaphneDSL file. Example: bin/daphne scripts/examples/hello-world.daph Note that the present working directory should be the root directory daphne/ when invoking the system (this requirement will be relaxed in the future). Passing Script Arguments Arguments to the DaphneDSL script can be provided as space-separated pairs of the form key=value . These can the accessed as $key in the DaphneDSL script. Example: bin/daphne test/api/cli/algorithms/kmeans.daphne r = 1000 f = 20 c = 5 i = 10 This example executes a simplified variant of the k-means clustering algorithm on random data with 1000 rows and 20 features using 5 centroids and a fixed number of 10 iterations. value must be a valid DaphneDSL literal, e.g., key=123 (signed 64-bit integer), key=-12.3 (double-precision floating-point), or key=\"hello\" (string). Note that the quotation marks \" are part of the string literal, so they must be escaped on a terminal, e.g., by key=\\\"hello\\\" . Command-Line Arguments The behavior of daphne can be influenced by numerous command-line arguments (the options mentioned above). To see the full list of available options, invoke bin/daphne --help . In the following, a few noteworthy general options are mentioned. Note that some of the more specific options are described in the documentation pages on the respective topics, e.g., distributed execution , scheduling , configuration , FPGA configuration , etc. --explain Prints the MLIR-based intermediate representation (IR), the so-called DaphneIR , after the specified compiler passes. For instance, to see the IR after parsing (and some initial simplifications) and after property inference, invoke bin/daphne --explain parsing_simplified,property_inference test/api/cli/algorithms/kmeans.daphne r = 1000 f = 20 c = 5 i = 10 --vec Turns on DAPHNE's vectorized execution engine, which fuses qualifying operations into vectorized pipelines. Experimental feature. --select-matrix-repr Turns on the automatic selection of a suitable matrix representation (currently dense or sparse (CSR)). Experimental feature. Return Codes If daphne terminates normally, one of the following status codes is returned: code meaning example 0 success everything went well 1 parser error a syntax error in a DaphneDSL script 2 compiler/pass error an operation was provided with inputs of incompatible shapes 3 runtime/execution error a kernel was invoked with invalid arguments Typical Errors and Troubleshooting Parser error: ... / Pass error: ... / Execution error: ... One of the three types of errors mentioned above occured. In many (but not yet all) cases, there will be an error message indicating what went wrong. Examples: Wrong way of passing string literals as DaphneDSL script arguments. line 1:0 mismatched input 'foo' expecting {'true', 'false', INT_LITERAL, FLOAT_LITERAL, STRING_LITERAL} Parser error: unexpected literal Maybe you tried to pass a string as an argument to a DaphneDSL script and forgot the quotation marks or they got lost. Pass strings as bin/daphne script.daphne foo=\\\"abc\\\" (not foo=abc or foo=\"abc\" ) on a terminal. Missing metadata file. Parser error: Could not open file 'data/foo.csv.meta' for reading meta data. Maybe you try to read a dataset called data/foo.csv , but the required metadata file data/foo.csv.meta does not exist. Using the old file metadata format. Parser error: [json.exception.parse_error.101] parse error at line 1, column 7: syntax error while parsing value - unexpected ','; expected end of input Maybe you try to read a dataset with readMatrix() or readFrame() in DaphneDSL, but the file metadata file does not have the right structure. Note that we changed the initial one-line text-based format to a more human-readable JSON-based format . JIT session error: Symbols not found: ... This error occurs when the execution of a DaphneDSL script requires invoking a kernel with an input/output type combination that was not pre-compiled. The first line indicates which kernel is missing for which type combination. Ultimately, DAPHNE will circumvent this situation automatically by knowing which kernels were pre-compiled and utilizing only those (while employing casts to adapt the types of the arguments and results, where necessary). At the moment, users can try to work around this by introducing casts in the DaphneDSL script. Developers can fix this problem by adding the respective instantiation in src/runtime/local/kernels/kernels.json . Example: JIT session error: Symbols not found: [ _ewAdd__int32_t__int32_t__int32_t ] JIT-Engine invocation failed: Failed to materialize symbols: { (main, { _mlir_ciface_main, _mlir_main, _mlir__mlir_ciface_main, main }) }Program aborted due to an unhandled Error: Failed to materialize symbols: { (main, { _mlir_ciface_main, _mlir_main, _mlir__mlir_ciface_main, main }) } Aborted (core dumped) Failed to create MemoryBuffer for: ... This error occurs when daphne is not invoked from the repository's root directory daphne/ as bin/daphne . It will be fixed in the future (see issue #445). In the meantime, please always invoke daphne from the repository's root directory daphne/ . Example: Failed to create MemoryBuffer for: lib/libAllKernels.so Error: No such file or directory Typically followed by an error or the type JIT session error: Symbols not found: ... , which is described above.","title":"RunningDaphneLocally"},{"location":"RunningDaphneLocally/#running-daphne-locally","text":"Running DAPHNE in a local environment. This document explains how to run DAPHNE on a local machine. For more details on running DAPHNE in a distributed setup, please see the documentation on the distributed runtime and distributed deployment . Before DAPHNE can be executed, the system must be built using ./build.sh (for more details see Getting Started ). The main executable of the DAPHNE system is bin/daphne . The general scheme of an invocation of DAPHNE looks as follows: bin/daphne [ options ] script [ arguments ] Where script is a DaphneDSL file. Example: bin/daphne scripts/examples/hello-world.daph Note that the present working directory should be the root directory daphne/ when invoking the system (this requirement will be relaxed in the future).","title":"Running DAPHNE Locally"},{"location":"RunningDaphneLocally/#passing-script-arguments","text":"Arguments to the DaphneDSL script can be provided as space-separated pairs of the form key=value . These can the accessed as $key in the DaphneDSL script. Example: bin/daphne test/api/cli/algorithms/kmeans.daphne r = 1000 f = 20 c = 5 i = 10 This example executes a simplified variant of the k-means clustering algorithm on random data with 1000 rows and 20 features using 5 centroids and a fixed number of 10 iterations. value must be a valid DaphneDSL literal, e.g., key=123 (signed 64-bit integer), key=-12.3 (double-precision floating-point), or key=\"hello\" (string). Note that the quotation marks \" are part of the string literal, so they must be escaped on a terminal, e.g., by key=\\\"hello\\\" .","title":"Passing Script Arguments"},{"location":"RunningDaphneLocally/#command-line-arguments","text":"The behavior of daphne can be influenced by numerous command-line arguments (the options mentioned above). To see the full list of available options, invoke bin/daphne --help . In the following, a few noteworthy general options are mentioned. Note that some of the more specific options are described in the documentation pages on the respective topics, e.g., distributed execution , scheduling , configuration , FPGA configuration , etc. --explain Prints the MLIR-based intermediate representation (IR), the so-called DaphneIR , after the specified compiler passes. For instance, to see the IR after parsing (and some initial simplifications) and after property inference, invoke bin/daphne --explain parsing_simplified,property_inference test/api/cli/algorithms/kmeans.daphne r = 1000 f = 20 c = 5 i = 10 --vec Turns on DAPHNE's vectorized execution engine, which fuses qualifying operations into vectorized pipelines. Experimental feature. --select-matrix-repr Turns on the automatic selection of a suitable matrix representation (currently dense or sparse (CSR)). Experimental feature.","title":"Command-Line Arguments"},{"location":"RunningDaphneLocally/#return-codes","text":"If daphne terminates normally, one of the following status codes is returned: code meaning example 0 success everything went well 1 parser error a syntax error in a DaphneDSL script 2 compiler/pass error an operation was provided with inputs of incompatible shapes 3 runtime/execution error a kernel was invoked with invalid arguments","title":"Return Codes"},{"location":"RunningDaphneLocally/#typical-errors-and-troubleshooting","text":"","title":"Typical Errors and Troubleshooting"},{"location":"RunningDaphneLocally/#parser-error-pass-error-execution-error","text":"One of the three types of errors mentioned above occured. In many (but not yet all) cases, there will be an error message indicating what went wrong. Examples: Wrong way of passing string literals as DaphneDSL script arguments. line 1:0 mismatched input 'foo' expecting {'true', 'false', INT_LITERAL, FLOAT_LITERAL, STRING_LITERAL} Parser error: unexpected literal Maybe you tried to pass a string as an argument to a DaphneDSL script and forgot the quotation marks or they got lost. Pass strings as bin/daphne script.daphne foo=\\\"abc\\\" (not foo=abc or foo=\"abc\" ) on a terminal. Missing metadata file. Parser error: Could not open file 'data/foo.csv.meta' for reading meta data. Maybe you try to read a dataset called data/foo.csv , but the required metadata file data/foo.csv.meta does not exist. Using the old file metadata format. Parser error: [json.exception.parse_error.101] parse error at line 1, column 7: syntax error while parsing value - unexpected ','; expected end of input Maybe you try to read a dataset with readMatrix() or readFrame() in DaphneDSL, but the file metadata file does not have the right structure. Note that we changed the initial one-line text-based format to a more human-readable JSON-based format .","title":"Parser error: .../Pass error: .../Execution error: ..."},{"location":"RunningDaphneLocally/#jit-session-error-symbols-not-found","text":"This error occurs when the execution of a DaphneDSL script requires invoking a kernel with an input/output type combination that was not pre-compiled. The first line indicates which kernel is missing for which type combination. Ultimately, DAPHNE will circumvent this situation automatically by knowing which kernels were pre-compiled and utilizing only those (while employing casts to adapt the types of the arguments and results, where necessary). At the moment, users can try to work around this by introducing casts in the DaphneDSL script. Developers can fix this problem by adding the respective instantiation in src/runtime/local/kernels/kernels.json . Example: JIT session error: Symbols not found: [ _ewAdd__int32_t__int32_t__int32_t ] JIT-Engine invocation failed: Failed to materialize symbols: { (main, { _mlir_ciface_main, _mlir_main, _mlir__mlir_ciface_main, main }) }Program aborted due to an unhandled Error: Failed to materialize symbols: { (main, { _mlir_ciface_main, _mlir_main, _mlir__mlir_ciface_main, main }) } Aborted (core dumped)","title":"JIT session error: Symbols not found: ..."},{"location":"RunningDaphneLocally/#failed-to-create-memorybuffer-for","text":"This error occurs when daphne is not invoked from the repository's root directory daphne/ as bin/daphne . It will be fixed in the future (see issue #445). In the meantime, please always invoke daphne from the repository's root directory daphne/ . Example: Failed to create MemoryBuffer for: lib/libAllKernels.so Error: No such file or directory Typically followed by an error or the type JIT session error: Symbols not found: ... , which is described above.","title":"Failed to create MemoryBuffer for: ..."},{"location":"SchedulingOptions/","text":"DAPHNE Scheduling This document describes the use of the pipeline and task scheduling mechanisms currently supported in the DAPHNE system. Scheduling Decisions The DAPHNE system considers four types of scheduling decisions: work partitioning, assignment, ordering and timing. Work partitioning refers to the partitioning of the work into units of work (or tasks) according to a certain granularity (fine or coarse, equal or variable). Work assignment refers to mapping (or placing) the units of work (or tasks) onto individual units of execution (processes or threads). Work ordering refers to the order in which the tasks are executed. We rely on the vectorized execution engine, therefore, tasks within a vectorized pipeline have no dependencies and can be executed in any order. Work timing refers to the times at which the units of work are set to begin execution on the assigned units of execution. Work Partitioning : The the DAPHNE prototype supports twelve partitioning schemes: Static (STATIC), Self-scheduling (SS), Guided self-scheduling (GSS), Trapezoid self-scheduling (TSS), Trapezoid Factoring self-scheduling (TFSS), Fixed-increase self-scheduling (FISS), Variable-increase self-scheduling (VISS), Performance loop-based self-scheduling (PLS), Modified version of Static (MSTATIC), Modified version of fixed size chunk self-scheduling (MFSC), and Probabilistic self-scheduling (PSS). The granularity of the tasks generated and scheduled by the DAPHNE system follows one of these partitioning schemes (See Section 4.1.1.1 in Deliverable 5.1 [D5.1]. Work Assignment : The current snapshot of the DAPHNE prototype supports two main assignment mechanisms: Single centralized work queue and Multiple work queues. When work assignment relies on a centralized work queue (CENTRALIZED), workers follow the self-scheduling principle, i.e., whenever a worker is free and idle, it obtains a task from a central queue. When work assignment relies on multiple work queues, workers follow the work-stealing principle, i.e., whenever workers are free, idle, and have no tasks in their queues, they steal tasks from the work queue of each other. Work queues can be per worker (PERCPU) or per group of workers (PERGROUP). In work-stealing, workers need to apply a victim selection mechanism to find a queue and steal work from it. The currently supported victim selection mechanisms are SEQ (steal from the next adjacent worker), SEQPRI (Steal from the next adjacent worker, but prioritize same NUMA domain), RANDOM (Steal from a random worker), RANDOMPRI (Steal from a random worker, but prioritize same NUMA domain). Scheduling Options To list all possible execution options of the DAPHNE system, one needs to execute the following $ ./bin/daphne --help The output of this command shows all DAPHNE compilation and execution parameters including the scheduling options that are currently support. The output below shows only the scheduling options that we will cover in this document. > This program compiles and executes a DaphneDSL script. USAGE: daphne [ options ] script [ arguments ] OPTIONS: Advanced Scheduling Knobs: Choose task partitioning scheme: --STATIC - Static ( default ) --SS - Self-scheduling --GSS - Guided self-scheduling --TSS - Trapezoid self-scheduling --FAC2 - Factoring self-scheduling --TFSS - Trapezoid Factoring self-scheduling --FISS - Fixed-increase self-scheduling --VISS - Variable-increase self-scheduling --PLS - Performance loop-based self-scheduling --MSTATIC - Modified version of Static, i.e., instead of n/p, it uses n/ ( 4 *p ) where n is number of tasks and p is number of threads --MFSC - Modified version of fixed size chunk self-scheduling, i.e., MFSC does not require profiling information as FSC --PSS - Probabilistic self-scheduling Choose queue setup scheme: --CENTRALIZED - One queue ( default ) --PERGROUP - One queue per CPU group --PERCPU - One queue per CPU core Choose work stealing victim selection logic: --SEQ - Steal from next adjacent worker --SEQPRI - Steal from next adjacent worker, prioritize same NUMA domain --RANDOM - Steal from random worker --RANDOMPRI - Steal from random worker, prioritize same NUMA domain --debug-mt - Prints debug information about the Multithreading Wrapper --grain-size = <int> - Define the minimum grain size of a task ( default is 1 ) --hyperthreading - Utilize multiple logical CPUs located on the same physical CPU --num-threads = <int> - Define the number of the CPU threads used by the vectorized execution engine ( default is equal to the number of physcial cores on the target node that executes the code ) --pin-workers - Pin workers to CPU cores --pre-partition - Partition rows into the number of queues before applying scheduling technique --vec - Enable vectorized execution engine DAPHNE Options: --args = <string> - Alternative way of specifying arguments to the DaphneDSL script ; must be a comma-separated list of name-value-pairs, e.g., ` --args x = 1 ,y = 2 .2 ` --config = <filename> - A JSON file that contains the DAPHNE configuration --cuda - Use CUDA --distributed - Enable distributed runtime --explain = <value> - Show DaphneIR after certain compiler passes ( separate multiple values by comma, the order is irrelevant ) = parsing - Show DaphneIR after parsing = parsing_simplified - Show DaphneIR after parsing and some simplifications = sql - Show DaphneIR after SQL parsing = property_inference - Show DaphneIR after property inference = vectorized - Show DaphneIR after vectorization = obj_ref_mgnt - Show DaphneIR after managing object references = kernels - Show DaphneIR after kernel lowering = llvm - Show DaphneIR after llvm lowering --libdir = <string> - The directory containing kernel libraries --no-obj-ref-mgnt - Switch off garbage collection by not managing data objects ' reference counters --select-matrix-repr - Automatically choose physical matrix representations ( e.g., dense/sparse ) Generic Options: --help - Display available options ( --help-hidden for more ) --help-list - Display list of available options ( --help-list-hidden for more ) --version - Display the version of this program EXAMPLES: daphne example.daphne daphne --vec example.daphne x = 1 y = 2 .2 z = \"foo\" daphne --vec --args x = 1 ,y = 2 .2,z = \"foo\" example.daphne NOTE: the DAPHNE system relies on the vectorized (tile) execution engine to support parallelism at the node level. The vectorized execution engine takes decision concerning work partition and assignment during applications\u2019 execution. Therefore, one needs always to use the option --vec with any of the scheduling options that we present in this document. Multithreading Options Number of threads : A DAPHNE user can control the total number of threads spawn by the DAPHNE runtime system use the following parameter --num-threads . This parameter should be non-zero positive value. Illegal integer values will be ignored by the system and the default value will be used. The default value of --num-threads is equal to the total number of physical cores of the host machine. The option can be used as below, e.g., the DAPHNE system spawns only 4 threads. ./bin/daphne --vec --num-threads = 4 some_daphne_script.daphne Thread Pinning : A DAPHNE user can decide if the DAPHNE system pins its threads to the physical cores. Currently, the DAPHNE system supports one simple pining strategy, namely, round-robin strategy. By default, the DAPHNE system does not pin its threads. The option --pin-workers can be used to activate thread pinning as follows ./bin/daphne --vec --pin-threads some_daphne_script.daphne Hyperthreading : if a host machine supports hyperthreading, a DAPHNE user can decide to use logical cores, i.e., if the \u2013num-threads is not specified, the DAPHNE system sets the total number of threads to the count of the physical cores. However, when the user specify the following parameter --hyperthreading , the DAPHNE system sets the number of threads to the count of the logical cores. ./bin/daphne --vec --hyperthreading some_daphne_script.daphne Work Partitioning Options Partition Scheme : A DAPHNE user selects the partition scheme by passing the name of the partition scheme as an argument to the DAPHNE system. If the user does not specify a partition scheme, the default partition scheme (STATIC) will be used. As an example, the following command uses GSS as a partition scheme. ./bin/daphne --vec --GSS some_daphne_script.daphne Task granularity : The DAPHNE user can exploit the --grain-size parameter to set the minimum size of the tasks generated by the DAPHNE system. This parameter should be non-zero positive value. Illegal integer values will be ignored by the system and the default value will be used. The default value of --grain-size is 1, i.e., the data associated with a task represents 1 row of the input matrix. As an example, the following command uses SS as a partition scheme with minimum task size of 100 ./bin/daphne --vec --SS --grain-size = 100 some_daphne_script.daphne Work Assignment Options Single centralized work queue : By default, the DAPHNE system uses a single centralized work queue. However, the user may explicitly use the following parameter --CENTRALIZED to ensure the use of single centralized work queue. ./bin/daphne --vec --GSS --CENTRALIZED some_daphne_script.daphne Multiple work queues : a DAPHNE user can exploit the use of multiple work queues by passing one of the following parameters --PERCPU or --PERGROUP . The two parameters cannot be used together, and if --CENTRALIZED is used with any of them, --CENTRALIZED will be ignored by the system. parameter --PERGROUP ensures that the DAPHNE system creates a number of groups equals to the number of NUMA domains on the target host machine. The DAPHNE system assigns equal number of workers (threads) to each of the groups. Workers within the same group share one work queue. The \u2013-PERGROUP can be used as follows ./bin/daphne --vec --PERGROUP some_daphne_script.daphne The parameter --PERCPU ensures that the DAPHNE system creates a number of queues equal to the total number of workers (threads), i.e., each worker is assigned to a single work queue. The parameter --PERCPU can be used as follows ./bin/daphne --vec --PERCPU some_daphne_script.daphne Victim Selection : A DAPHNE user can choose a victim selection strategy by passing one of the following parameters --SEQ, --SEQPRI, --RANDOM, and --RANDOMPRI. These parameters activate different victim selection strategies as follows --SEQ activates a sequential victim selection strategy, i.e., the ith worker steals form the (i+1)th worker. The last worker steals from the first worker. --SEQPRI is similar to --SEQ except that --SEQPRI priorities workers assigned to the same NUMA domain. When the host machine has one NUMA domain, --SEQ and --SEQPRI have no difference. --RANDOM activates a random victim selection strategy, i.e., the ith worker steals form a randomly chosen worker. --RANDOMPRI is similar to --RANDOM except that --RANDOM priorities workers assigned to the same NUMA domain. When the host machine has one NUMA domain, --RANDOMPRI and --RANDOMPRI have no difference. NOTE: When the user does not choose one of these parameters, the DAPHNE system considers --SEQ as a default victim selection strategy. As an example, the following command uses --SEQPRI as a victim selection strategy. ./bin/daphne --vec --PERGROUP --SEQPRI some_daphne_script.daphne References D4.1 DAPHNE: D4.1 DSL Runtime Design, 11/2021 D5.1 DAPHNE: D5.1 Scheduler Design for Pipelines and Tasks, 11/2021","title":"SchedulingOptions"},{"location":"SchedulingOptions/#daphne-scheduling","text":"This document describes the use of the pipeline and task scheduling mechanisms currently supported in the DAPHNE system.","title":"DAPHNE Scheduling"},{"location":"SchedulingOptions/#scheduling-decisions","text":"The DAPHNE system considers four types of scheduling decisions: work partitioning, assignment, ordering and timing. Work partitioning refers to the partitioning of the work into units of work (or tasks) according to a certain granularity (fine or coarse, equal or variable). Work assignment refers to mapping (or placing) the units of work (or tasks) onto individual units of execution (processes or threads). Work ordering refers to the order in which the tasks are executed. We rely on the vectorized execution engine, therefore, tasks within a vectorized pipeline have no dependencies and can be executed in any order. Work timing refers to the times at which the units of work are set to begin execution on the assigned units of execution. Work Partitioning : The the DAPHNE prototype supports twelve partitioning schemes: Static (STATIC), Self-scheduling (SS), Guided self-scheduling (GSS), Trapezoid self-scheduling (TSS), Trapezoid Factoring self-scheduling (TFSS), Fixed-increase self-scheduling (FISS), Variable-increase self-scheduling (VISS), Performance loop-based self-scheduling (PLS), Modified version of Static (MSTATIC), Modified version of fixed size chunk self-scheduling (MFSC), and Probabilistic self-scheduling (PSS). The granularity of the tasks generated and scheduled by the DAPHNE system follows one of these partitioning schemes (See Section 4.1.1.1 in Deliverable 5.1 [D5.1]. Work Assignment : The current snapshot of the DAPHNE prototype supports two main assignment mechanisms: Single centralized work queue and Multiple work queues. When work assignment relies on a centralized work queue (CENTRALIZED), workers follow the self-scheduling principle, i.e., whenever a worker is free and idle, it obtains a task from a central queue. When work assignment relies on multiple work queues, workers follow the work-stealing principle, i.e., whenever workers are free, idle, and have no tasks in their queues, they steal tasks from the work queue of each other. Work queues can be per worker (PERCPU) or per group of workers (PERGROUP). In work-stealing, workers need to apply a victim selection mechanism to find a queue and steal work from it. The currently supported victim selection mechanisms are SEQ (steal from the next adjacent worker), SEQPRI (Steal from the next adjacent worker, but prioritize same NUMA domain), RANDOM (Steal from a random worker), RANDOMPRI (Steal from a random worker, but prioritize same NUMA domain).","title":"Scheduling Decisions"},{"location":"SchedulingOptions/#scheduling-options","text":"To list all possible execution options of the DAPHNE system, one needs to execute the following $ ./bin/daphne --help The output of this command shows all DAPHNE compilation and execution parameters including the scheduling options that are currently support. The output below shows only the scheduling options that we will cover in this document. > This program compiles and executes a DaphneDSL script. USAGE: daphne [ options ] script [ arguments ] OPTIONS: Advanced Scheduling Knobs: Choose task partitioning scheme: --STATIC - Static ( default ) --SS - Self-scheduling --GSS - Guided self-scheduling --TSS - Trapezoid self-scheduling --FAC2 - Factoring self-scheduling --TFSS - Trapezoid Factoring self-scheduling --FISS - Fixed-increase self-scheduling --VISS - Variable-increase self-scheduling --PLS - Performance loop-based self-scheduling --MSTATIC - Modified version of Static, i.e., instead of n/p, it uses n/ ( 4 *p ) where n is number of tasks and p is number of threads --MFSC - Modified version of fixed size chunk self-scheduling, i.e., MFSC does not require profiling information as FSC --PSS - Probabilistic self-scheduling Choose queue setup scheme: --CENTRALIZED - One queue ( default ) --PERGROUP - One queue per CPU group --PERCPU - One queue per CPU core Choose work stealing victim selection logic: --SEQ - Steal from next adjacent worker --SEQPRI - Steal from next adjacent worker, prioritize same NUMA domain --RANDOM - Steal from random worker --RANDOMPRI - Steal from random worker, prioritize same NUMA domain --debug-mt - Prints debug information about the Multithreading Wrapper --grain-size = <int> - Define the minimum grain size of a task ( default is 1 ) --hyperthreading - Utilize multiple logical CPUs located on the same physical CPU --num-threads = <int> - Define the number of the CPU threads used by the vectorized execution engine ( default is equal to the number of physcial cores on the target node that executes the code ) --pin-workers - Pin workers to CPU cores --pre-partition - Partition rows into the number of queues before applying scheduling technique --vec - Enable vectorized execution engine DAPHNE Options: --args = <string> - Alternative way of specifying arguments to the DaphneDSL script ; must be a comma-separated list of name-value-pairs, e.g., ` --args x = 1 ,y = 2 .2 ` --config = <filename> - A JSON file that contains the DAPHNE configuration --cuda - Use CUDA --distributed - Enable distributed runtime --explain = <value> - Show DaphneIR after certain compiler passes ( separate multiple values by comma, the order is irrelevant ) = parsing - Show DaphneIR after parsing = parsing_simplified - Show DaphneIR after parsing and some simplifications = sql - Show DaphneIR after SQL parsing = property_inference - Show DaphneIR after property inference = vectorized - Show DaphneIR after vectorization = obj_ref_mgnt - Show DaphneIR after managing object references = kernels - Show DaphneIR after kernel lowering = llvm - Show DaphneIR after llvm lowering --libdir = <string> - The directory containing kernel libraries --no-obj-ref-mgnt - Switch off garbage collection by not managing data objects ' reference counters --select-matrix-repr - Automatically choose physical matrix representations ( e.g., dense/sparse ) Generic Options: --help - Display available options ( --help-hidden for more ) --help-list - Display list of available options ( --help-list-hidden for more ) --version - Display the version of this program EXAMPLES: daphne example.daphne daphne --vec example.daphne x = 1 y = 2 .2 z = \"foo\" daphne --vec --args x = 1 ,y = 2 .2,z = \"foo\" example.daphne NOTE: the DAPHNE system relies on the vectorized (tile) execution engine to support parallelism at the node level. The vectorized execution engine takes decision concerning work partition and assignment during applications\u2019 execution. Therefore, one needs always to use the option --vec with any of the scheduling options that we present in this document.","title":"Scheduling Options"},{"location":"SchedulingOptions/#multithreading-options","text":"Number of threads : A DAPHNE user can control the total number of threads spawn by the DAPHNE runtime system use the following parameter --num-threads . This parameter should be non-zero positive value. Illegal integer values will be ignored by the system and the default value will be used. The default value of --num-threads is equal to the total number of physical cores of the host machine. The option can be used as below, e.g., the DAPHNE system spawns only 4 threads. ./bin/daphne --vec --num-threads = 4 some_daphne_script.daphne Thread Pinning : A DAPHNE user can decide if the DAPHNE system pins its threads to the physical cores. Currently, the DAPHNE system supports one simple pining strategy, namely, round-robin strategy. By default, the DAPHNE system does not pin its threads. The option --pin-workers can be used to activate thread pinning as follows ./bin/daphne --vec --pin-threads some_daphne_script.daphne Hyperthreading : if a host machine supports hyperthreading, a DAPHNE user can decide to use logical cores, i.e., if the \u2013num-threads is not specified, the DAPHNE system sets the total number of threads to the count of the physical cores. However, when the user specify the following parameter --hyperthreading , the DAPHNE system sets the number of threads to the count of the logical cores. ./bin/daphne --vec --hyperthreading some_daphne_script.daphne","title":"Multithreading Options"},{"location":"SchedulingOptions/#work-partitioning-options","text":"Partition Scheme : A DAPHNE user selects the partition scheme by passing the name of the partition scheme as an argument to the DAPHNE system. If the user does not specify a partition scheme, the default partition scheme (STATIC) will be used. As an example, the following command uses GSS as a partition scheme. ./bin/daphne --vec --GSS some_daphne_script.daphne Task granularity : The DAPHNE user can exploit the --grain-size parameter to set the minimum size of the tasks generated by the DAPHNE system. This parameter should be non-zero positive value. Illegal integer values will be ignored by the system and the default value will be used. The default value of --grain-size is 1, i.e., the data associated with a task represents 1 row of the input matrix. As an example, the following command uses SS as a partition scheme with minimum task size of 100 ./bin/daphne --vec --SS --grain-size = 100 some_daphne_script.daphne","title":"Work Partitioning Options"},{"location":"SchedulingOptions/#work-assignment-options","text":"Single centralized work queue : By default, the DAPHNE system uses a single centralized work queue. However, the user may explicitly use the following parameter --CENTRALIZED to ensure the use of single centralized work queue. ./bin/daphne --vec --GSS --CENTRALIZED some_daphne_script.daphne Multiple work queues : a DAPHNE user can exploit the use of multiple work queues by passing one of the following parameters --PERCPU or --PERGROUP . The two parameters cannot be used together, and if --CENTRALIZED is used with any of them, --CENTRALIZED will be ignored by the system. parameter --PERGROUP ensures that the DAPHNE system creates a number of groups equals to the number of NUMA domains on the target host machine. The DAPHNE system assigns equal number of workers (threads) to each of the groups. Workers within the same group share one work queue. The \u2013-PERGROUP can be used as follows ./bin/daphne --vec --PERGROUP some_daphne_script.daphne The parameter --PERCPU ensures that the DAPHNE system creates a number of queues equal to the total number of workers (threads), i.e., each worker is assigned to a single work queue. The parameter --PERCPU can be used as follows ./bin/daphne --vec --PERCPU some_daphne_script.daphne Victim Selection : A DAPHNE user can choose a victim selection strategy by passing one of the following parameters --SEQ, --SEQPRI, --RANDOM, and --RANDOMPRI. These parameters activate different victim selection strategies as follows --SEQ activates a sequential victim selection strategy, i.e., the ith worker steals form the (i+1)th worker. The last worker steals from the first worker. --SEQPRI is similar to --SEQ except that --SEQPRI priorities workers assigned to the same NUMA domain. When the host machine has one NUMA domain, --SEQ and --SEQPRI have no difference. --RANDOM activates a random victim selection strategy, i.e., the ith worker steals form a randomly chosen worker. --RANDOMPRI is similar to --RANDOM except that --RANDOM priorities workers assigned to the same NUMA domain. When the host machine has one NUMA domain, --RANDOMPRI and --RANDOMPRI have no difference. NOTE: When the user does not choose one of these parameters, the DAPHNE system considers --SEQ as a default victim selection strategy. As an example, the following command uses --SEQPRI as a victim selection strategy. ./bin/daphne --vec --PERGROUP --SEQPRI some_daphne_script.daphne","title":"Work Assignment Options"},{"location":"SchedulingOptions/#references","text":"D4.1 DAPHNE: D4.1 DSL Runtime Design, 11/2021 D5.1 DAPHNE: D5.1 Scheduler Design for Pipelines and Tasks, 11/2021","title":"References"},{"location":"DaphneDSL/Builtins/","text":"Built-in Functions DaphneDSL offers numerous built-in functions, which can be used in every DaphneDSL script without requiring any imports. The general syntax for calling a built-in function is func(param1, param2, ...) (see the DaphneDSL Language Reference ). This document provides an overview of the DaphneDSL built-in functions. Note that we are still extending this set of built-in functions. Furthermore, we also plan to create a library of higher-level ML primitives allowing users to productively implement integrated data analysis pipelines at a much higher level of abstraction. Those library functions will internally be implemented using the built-in functions described in this document. We use the following notation (deviating from the DaphneDSL function syntax): square brackets [] mean that a parameter is optional ... stands for an arbitrary repetition of the previous parameter (including zero). / means alternative options, e.g., matrix/frame means the parameter could be a matrix or a frame List of categories DaphneDSL's built-in functions can be categorized as follows: Data generation Matrix/frame dimensions Elementwise unary Elementwise binary Outer binary (generalized outer product) Aggregation and statistical Reorganization Matrix decomposition & co Deep neural network Other matrix operations Extended relational algebra Conversions, casts and copying Input/output Data preprocessing Measurements Data generation fill (value:scalar, numRows:size, numCols:size) Creates a ( numRows x numCols ) matrix and sets all elements to value . createFrame (column:matrix, ...[, labels:str, ...]) Creates a frame from an arbitrary number of column matrices. Optionally, a label can be specified for each column (the number of provided columns and labels must be equal). diagMatrix (arg:matrix) Creates an (n x n) diagonal matrix by placing the elements of the given (n x 1) column-matrix arg on the diagonal of an otherwise empty (zero) square matrix. rand (numRows:size, numCols:size, min:scalar, max:scalar, sparsity:double, seed:si64) Generates a ( numRows x numCols ) matrix of random values. The values are drawn uniformly from the range [ min , max ] (both inclusive). The sparsity can be chosen between 0.0 (all zeros) and 1.0 (all non-zeros). The seed can be set to -1 (randomly chooses a seed), or be provided explicitly to enable reproducible random values. sample (range:scalar, size:size, withReplacement:bool, seed:si64) Generates a ( size x 1) column-matrix of values drawn from the range [0, range - 1] . The parameter withReplacement determines if a value can be drawn multiple times ( true ) or not ( false ). The seed can be set to -1 (randomly chooses a seed), or be provided explicitly to enable reproducible random values. seq (from:scalar, to:scalar, inc:scalar) Generates a column matrix containing an arithmetic sequence of values starting at from , going through to , in increments of inc . Note that from may be greater than to , and inc may be negative. Matrix/frame dimensions The following built-in functions allow to find out the shape/dimensions of matrices and frames. nrow (arg:matrix/frame) Returns the number of rows in arg . ncol (arg:matrix/frame) Returns the number of columns in arg . ncell (arg:matrix/frame) Returns the number of cells in arg . This is the product of the number of rows and the number of columns. Elementwise unary The following built-in functions all follow the same scheme: unaryFunc (arg:scalar/matrix) Applies the respective unary function (see table below) to the given scalar arg or to each element of the given matrix arg . Arithmetic/general math function meaning abs absolute value sign signum ( 1 for positive, 0 for zero, -1 for negative) exp exponentiation ( e to the power of arg ) ln natural logarithm (logarithm of arg to the base of e ) sqrt square root Rounding function meaning round round to nearest floor round down ceil round up Trigonometric The typical trigonometric functions: sin , cos , tan , sinh , cosh , tanh , asin , acos , atan Elementwise binary DaphneDSL supports various elementwise binary operations. Some of those can be used through operators in infix notation , e.g., + ; and some through built-in functions . Some operations even support both, e.g., pow(a, b) and a^b have the same semantics. The built-in functions all follow the same scheme: binaryFunc (lhs:scalar/matrix, rhs:scalar/matrix) Applies the respective binary function (see table below) to the corresponding pairs of a value in the left-hand-side argument lhs and the right-hand-side argument rhs . Regarding the combinations of scalars and matrices, the same broadcasting semantics apply as for binary operations like + , * , etc. (see the DaphneDSL Language Reference ). Arithmetic function operator meaning + addition - subtraction * multiplication / division pow ^ exponentiation ( lhs to the power of rhs ) log logarithm (logarithm of lhs to the base of rhs ) mod % modulo Min/max function operator meaning min minimum max maximum Logical function operator meaning && logical conjunction || logical disjunction Strings function operator meaning concat + string concatenation Comparison function operator meaning == equal != not equal < less than <= less or equal > greater than >= greater or equal Outer binary (generalized outer product) The following built-in functions all follow the same scheme: outerBinaryFunc (lhs:matrix, rhs:matrix) The argument lhs is expected to be a column (m x 1) matrix, and the argument rhs is expected to be a row (1 x n) matrix. The result is a (m x n) matrix, whereby the element at position (i, j) is calculated by applying the respective binary function (see the table below) to the i -th element in lhs and the j -th element in rhs . Schematically, this looks as follows (where \u2218 is some binary operation): | b0 b1 ... bn rhs ---+---------------------- lhs a0 | a0\u2218b0 a0\u2218b1 ... a0\u2218bn res a1 | a1\u2218b0 a1\u2218b1 ... a1\u2218bn .. | ..... ..... ..... am | am\u2218b0 am\u2218b1 ... am\u2218bn Arithmetic function meaning outerAdd addition outerSub subtraction outerMul multiplication (the well-known outer product ) outerDiv division outerPow exponentiation ( lhs to the power of rhs ) outerLog logarithm (logarithm of lhs to the base of rhs ) outerMod modulo Min/max function meaning outerMin minimum outerMax maximum Logical function meaning outerAnd logical conjunction outerOr logical disjunction outerXor logical exclusive disjunction Strings function meaning outerConcat string concatenation Comparison function meaning outerEq equal outerNeq not equal outerLt less than outerLe less or equal outerGt greater than outerGe greater or equal Aggregation and statistical Full/row/column aggregation The following built-in functions all follow the same scheme: agg (arg:matrix) Full aggregation over all elements of the matrix arg using aggregation function agg (see table below). Returns a scalar. agg (arg:matrix, axis:si64) Row or column aggregation over a (n x m) matrix arg using aggregation function agg (see table below). - axis == 0: calculate one aggregate per row; the result is a (n x 1) (column) matrix - axis == 1: calculate one aggregate per column; the result is a (1 x m) (row) matrix function meaning sum summation aggMin minimum aggMax maximum mean arithmetic mean var variance stddev standard deviation idxMin argmin (the index of the minimum value, only for row/column-wise aggregation) idxMax argmax (the index of the maximum value, only for row/column-wise aggregation) Cumulative aggregation The following built-in functions all follow the same scheme: cumAgg (arg:matrix) Cumulative aggregation over each column of the matrix arg . Returns a matrix of the same shape as arg . function meaning cumSum cumulative sum cumProd cumulative product cumMin cumulative minimum cumMax cumulative maximum Reorganization reshape (arg:matrix, numRows:size, numCols:size) Changes the shape of arg to ( numRows x numCols ) . Note that the number of cells must be retained, i.e., the product of numRows and numCols must be equal to the product of the number of rows in arg and the number of columns in arg . transpose/t (arg:matrix) Transposes the given matrix arg . cbind (lhs:matrix/frame, rhs:matrix/frame) Concatenates two matrices or two frames horizontally. The two inputs must have the same number of rows. rbind (lhs:matrix/frame, rhs:matrix/frame) Concatenates two matrices or two frames vertically. The two inputs must have the same number of columns. reverse (arg:matrix) Reverses the rows in the given matrix arg . order (arg:matrix/frame, colIdxs:size, ..., ascs:bool, ..., returnIndexes:bool) Sorts the given matrix or frame by an arbitrary number of columns. The columns are specified in terms of their indexes (counting starts at zero). Each column can be sorted either in ascending ( true ) or descending ( false ) order (as determined by parameter ascs ). The provided number of columns and sort orders must match. The parameter returnIndexes determines whether to return the sorted data ( false ) or a column-matrix of positions representing the permutation applied by the sorting ( true ). Matrix decomposition & co We plan to support various matrix decompositions like eigen , lu , qr , and svd . Deep neural network Note that most of these operations only have a CUDNN-based kernel for GPU execution at the moment. affine (inputData:matrix, weightData:matrix, biasData:matrix) avg_pool2d (inputData:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, poolHeight:size, poolWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size) Performs average pooling operation. max_pool2d (inputData:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, poolHeight:size, poolWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size) Performs max pooling operation. batch_norm2d (inputData:matrix, gamma, beta, emaMean, emaVar, eps) Performs batch normalization operation. biasAdd (input:matrix, bias:matrix) Adds the (1 x numChannels ) row-matrix bias to the input with the given number of channels. conv2d (input:matrix, filter:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, filterHeight:size, filterWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size) 2D convolution. relu (inputData:matrix) softmax (inputData:matrix) Other matrix operations diagVector (arg:matrix) Extracts the diagonal of the given (n x n) matrix arg as a (n x 1) column-matrix. lowerTri (arg:matrix, diag:bool, values:bool) Extracts the lower triangle of the given square matrix arg by setting all elements in the upper triangle to zero. If diag is true , the elements on the diagonal are retained; otherwise, they are set to zero, too. If values is true , the non-zero elements in the lower triangle are retained; otherwise, they are set to one. upperTri (arg:matrix, diag:bool, values:bool) Extracts the upper triangle of the given square matrix arg by setting all elements in the lower triangle to zero. If diag is true , the elements on the diagonal are retained; otherwise, they are set to zero, too. If values is true , the non-zero elements in the upper triangle are retained; otherwise, they are set to one. solve (A:matrix, b:matrix) Solves the system of linear equations given by the (n x n) matrix A and the (n x 1) column-matrix b and returns the result as a (n x 1) column-matrix. replace (arg:matrix, pattern:scalar, replacement:scalar) Replaces all occurrences of the element pattern in the matrix arg by the element replacement . ctable (ys:matrix, xs:matrix[, weight:scalar][, numRows:int, numCols:int]) Returns the contingency table of two (n x 1) column-matrices ys and xs . The resulting matrix res consists of max(ys) + 1 rows and max(xs) + 1 columns. More precisely, res[x, y] = |{ k | ys[k, 0] = y and xs[k, 0] = x, 0 \u2264 k \u2264 n-1 }| * weight . In other words, starting with an all-zero result matrix, ys and xs can be thought of as lists of y / x -coordinates which indicate the result matrix's cells whose value shall be increased by weight . Note that ys and xs must not contain negative numbers. The scalar weight is an optional argument and defaults to 1.0. The weight also determines the value type of the result. Moreover, optionally, the result shape in terms of the number of rows and columns can be specified. If omited, it defaults to the smallest numbers required to accommodate all given y / x -coordinates, as expressed above. If specified, the result can be either cropped or padded with zeros to the desired shape. If a value less than zero is provided as the number of rows/columns, the respective dimension will also be determined from the input data. This built-in function can be called with 2, 3, 4, or 5 arguments, depending on which optional arguments are given. syrk (A:martix) Calculates t(A) @ A by symmetric rank-k update operations. gemv (A:matrix, x:matrix) Calcuates t(A) @ x for the given (n x m) matrix A and (n x 1) column-matrix x . Extended relational algebra DaphneDSL supports relational algebra on frames in two ways: On the one hand, entire SQL queries can be executed over previously registered views . This aspect is described in detail in a separate tutorial . On the other hand, built-in functions for individual operations of extended relational algebra can be used on frames in DaphneDSL. Entire SQL query registerView (viewName:str, arg:frame) Registers the frame arg to be accessible to SQL queries by the name viewName . sql (query:str) Executes the SQL query query on the frames previously registered with registerView() and returns the result as a frame. Set Operations We will support set operations such as intersect , merge , and except . Cartesian product and joins cartesian (lhs:frame, rhs:frame) Calculates the cartesian (cross) product of the two input frames. innerJoin (lhs:frame, rhs:frame, lhsOn:str, rhsOn:str) Performs an inner join of the two input frames on lhs . lhsOn == rhs . rhsOn . semiJoin (lhs:frame, rhs:frame, lhsOn:str, rhsOn:str) Performs a semi join of the two input frames on lhs . lhsOn == rhs . rhsOn . Returns only the columns belonging to lhs . groupJoin (lhs:frame, rhs:frame, lhsOn:str, rhsOn:str, rhsAgg:str) Group-join of lhs and rhs on lhs.lhsOn == rhs.rhsOn with summation of rhs.rhsAgg . We will support more variants of joins, including (left/right) outer joins, theta joins, anti-joins, etc. Frame label manipulation setColLabels (arg:frame, labels:str, ...) Sets the column labels of the given frame arg to the given labels . There must be as many labels as columns in arg . setColLabelsPrefix (arg:frame, predfix:str, ...) Prepends the given prefix to the labels of all columns in arg . Conversions, casts and copying Note that DaphneDSL offers casts in form of the as.() -expression. See the DaphneDSL Language Reference for details. copy (arg:matrix/frame) Creates a copy of arg . quantize (arg:matrix<f32>, min:f32, max:f32) Performs a min / max quantization of the values in arg . The result matrix is of value type ui8 . Input/output DAPHNE supports local file I/O for various file formats. The format is determined by the specified file name extension. Currently, the following formats are supported: \".csv\": comma-separated values \".mtx\": matrix market \".parquet\": Apache Parquet format \".dbdf\": DAPHNE's binary data format For both reading and writing, file names can be specified as absolute or relative paths. For most formats, DAPHNE requires additional information on the data and value types as well as dimensions, when reading files . These must be provided in a separate .meta -file . print (arg:scalar/matrix/frame[, newline:bool[, toStderr:bool]]) Prints the given scalar, matrix, or frame arg to stdout . The parameter newline is optional; true (the default) means a new line is started after arg , false means no new line is started. The parameter toStderr is optional; true means the text is printed to stderr , false (the default) means it is printed to stdout . readFrame (filename:str) Reads the file filename into a frame. Assumes that a .meta -file is present for the specified filename . readMatrix (filename:str) Reads the file filename into a matrix. Assumes that a .meta -file is present for the specified filename . write/writeFrame/writeMatrix (arg:matrix/frame, filename:str) Writes the given matrix or frame arg into the specified file filename . Note that the type of arg determines how to store the data; thus, it suffices to call write() (but writeFrame() and writeMatrix() can be used synonymously for consistency with reading). At the same time, this creates a .meta -file for the written file, so that it can be read again using readMatrix() / readFrame() . Data preprocessing oneHot (arg:matrix, info:matrix<si64>) Applies one-hot-encoding to the given (n x m) matrix arg . The (1 x m) row-matrix info specifies the details (in the following, d[j] is short for info[0, j] ): If d[j] == -1, then the j -th column of arg will remain as it is. If d[j] >= 0, then the j -th column of arg will be encoded. More precisely, the j -th column of arg must contain only integral values in the range [0, d[j] - 1] , and will be replaced by d[j] columns containing only zeros and ones. For each row i in arg , the value in the as.scalar(arg[i, j]) -th of those columns is set to 1, while all others are set to 0. Measurements now () Returns the current time since the epoch in nano seconds.","title":"Builtins"},{"location":"DaphneDSL/Builtins/#built-in-functions","text":"DaphneDSL offers numerous built-in functions, which can be used in every DaphneDSL script without requiring any imports. The general syntax for calling a built-in function is func(param1, param2, ...) (see the DaphneDSL Language Reference ). This document provides an overview of the DaphneDSL built-in functions. Note that we are still extending this set of built-in functions. Furthermore, we also plan to create a library of higher-level ML primitives allowing users to productively implement integrated data analysis pipelines at a much higher level of abstraction. Those library functions will internally be implemented using the built-in functions described in this document. We use the following notation (deviating from the DaphneDSL function syntax): square brackets [] mean that a parameter is optional ... stands for an arbitrary repetition of the previous parameter (including zero). / means alternative options, e.g., matrix/frame means the parameter could be a matrix or a frame","title":"Built-in Functions"},{"location":"DaphneDSL/Builtins/#list-of-categories","text":"DaphneDSL's built-in functions can be categorized as follows: Data generation Matrix/frame dimensions Elementwise unary Elementwise binary Outer binary (generalized outer product) Aggregation and statistical Reorganization Matrix decomposition & co Deep neural network Other matrix operations Extended relational algebra Conversions, casts and copying Input/output Data preprocessing Measurements","title":"List of categories"},{"location":"DaphneDSL/Builtins/#data-generation","text":"fill (value:scalar, numRows:size, numCols:size) Creates a ( numRows x numCols ) matrix and sets all elements to value . createFrame (column:matrix, ...[, labels:str, ...]) Creates a frame from an arbitrary number of column matrices. Optionally, a label can be specified for each column (the number of provided columns and labels must be equal). diagMatrix (arg:matrix) Creates an (n x n) diagonal matrix by placing the elements of the given (n x 1) column-matrix arg on the diagonal of an otherwise empty (zero) square matrix. rand (numRows:size, numCols:size, min:scalar, max:scalar, sparsity:double, seed:si64) Generates a ( numRows x numCols ) matrix of random values. The values are drawn uniformly from the range [ min , max ] (both inclusive). The sparsity can be chosen between 0.0 (all zeros) and 1.0 (all non-zeros). The seed can be set to -1 (randomly chooses a seed), or be provided explicitly to enable reproducible random values. sample (range:scalar, size:size, withReplacement:bool, seed:si64) Generates a ( size x 1) column-matrix of values drawn from the range [0, range - 1] . The parameter withReplacement determines if a value can be drawn multiple times ( true ) or not ( false ). The seed can be set to -1 (randomly chooses a seed), or be provided explicitly to enable reproducible random values. seq (from:scalar, to:scalar, inc:scalar) Generates a column matrix containing an arithmetic sequence of values starting at from , going through to , in increments of inc . Note that from may be greater than to , and inc may be negative.","title":"Data generation"},{"location":"DaphneDSL/Builtins/#matrixframe-dimensions","text":"The following built-in functions allow to find out the shape/dimensions of matrices and frames. nrow (arg:matrix/frame) Returns the number of rows in arg . ncol (arg:matrix/frame) Returns the number of columns in arg . ncell (arg:matrix/frame) Returns the number of cells in arg . This is the product of the number of rows and the number of columns.","title":"Matrix/frame dimensions"},{"location":"DaphneDSL/Builtins/#elementwise-unary","text":"The following built-in functions all follow the same scheme: unaryFunc (arg:scalar/matrix) Applies the respective unary function (see table below) to the given scalar arg or to each element of the given matrix arg .","title":"Elementwise unary"},{"location":"DaphneDSL/Builtins/#arithmeticgeneral-math","text":"function meaning abs absolute value sign signum ( 1 for positive, 0 for zero, -1 for negative) exp exponentiation ( e to the power of arg ) ln natural logarithm (logarithm of arg to the base of e ) sqrt square root","title":"Arithmetic/general math"},{"location":"DaphneDSL/Builtins/#rounding","text":"function meaning round round to nearest floor round down ceil round up","title":"Rounding"},{"location":"DaphneDSL/Builtins/#trigonometric","text":"The typical trigonometric functions: sin , cos , tan , sinh , cosh , tanh , asin , acos , atan","title":"Trigonometric"},{"location":"DaphneDSL/Builtins/#elementwise-binary","text":"DaphneDSL supports various elementwise binary operations. Some of those can be used through operators in infix notation , e.g., + ; and some through built-in functions . Some operations even support both, e.g., pow(a, b) and a^b have the same semantics. The built-in functions all follow the same scheme: binaryFunc (lhs:scalar/matrix, rhs:scalar/matrix) Applies the respective binary function (see table below) to the corresponding pairs of a value in the left-hand-side argument lhs and the right-hand-side argument rhs . Regarding the combinations of scalars and matrices, the same broadcasting semantics apply as for binary operations like + , * , etc. (see the DaphneDSL Language Reference ).","title":"Elementwise binary"},{"location":"DaphneDSL/Builtins/#arithmetic","text":"function operator meaning + addition - subtraction * multiplication / division pow ^ exponentiation ( lhs to the power of rhs ) log logarithm (logarithm of lhs to the base of rhs ) mod % modulo","title":"Arithmetic"},{"location":"DaphneDSL/Builtins/#minmax","text":"function operator meaning min minimum max maximum","title":"Min/max"},{"location":"DaphneDSL/Builtins/#logical","text":"function operator meaning && logical conjunction || logical disjunction","title":"Logical"},{"location":"DaphneDSL/Builtins/#strings","text":"function operator meaning concat + string concatenation","title":"Strings"},{"location":"DaphneDSL/Builtins/#comparison","text":"function operator meaning == equal != not equal < less than <= less or equal > greater than >= greater or equal","title":"Comparison"},{"location":"DaphneDSL/Builtins/#outer-binary-generalized-outer-product","text":"The following built-in functions all follow the same scheme: outerBinaryFunc (lhs:matrix, rhs:matrix) The argument lhs is expected to be a column (m x 1) matrix, and the argument rhs is expected to be a row (1 x n) matrix. The result is a (m x n) matrix, whereby the element at position (i, j) is calculated by applying the respective binary function (see the table below) to the i -th element in lhs and the j -th element in rhs . Schematically, this looks as follows (where \u2218 is some binary operation): | b0 b1 ... bn rhs ---+---------------------- lhs a0 | a0\u2218b0 a0\u2218b1 ... a0\u2218bn res a1 | a1\u2218b0 a1\u2218b1 ... a1\u2218bn .. | ..... ..... ..... am | am\u2218b0 am\u2218b1 ... am\u2218bn","title":"Outer binary (generalized outer product)"},{"location":"DaphneDSL/Builtins/#arithmetic_1","text":"function meaning outerAdd addition outerSub subtraction outerMul multiplication (the well-known outer product ) outerDiv division outerPow exponentiation ( lhs to the power of rhs ) outerLog logarithm (logarithm of lhs to the base of rhs ) outerMod modulo","title":"Arithmetic"},{"location":"DaphneDSL/Builtins/#minmax_1","text":"function meaning outerMin minimum outerMax maximum","title":"Min/max"},{"location":"DaphneDSL/Builtins/#logical_1","text":"function meaning outerAnd logical conjunction outerOr logical disjunction outerXor logical exclusive disjunction","title":"Logical"},{"location":"DaphneDSL/Builtins/#strings_1","text":"function meaning outerConcat string concatenation","title":"Strings"},{"location":"DaphneDSL/Builtins/#comparison_1","text":"function meaning outerEq equal outerNeq not equal outerLt less than outerLe less or equal outerGt greater than outerGe greater or equal","title":"Comparison"},{"location":"DaphneDSL/Builtins/#aggregation-and-statistical","text":"","title":"Aggregation and statistical"},{"location":"DaphneDSL/Builtins/#fullrowcolumn-aggregation","text":"The following built-in functions all follow the same scheme: agg (arg:matrix) Full aggregation over all elements of the matrix arg using aggregation function agg (see table below). Returns a scalar. agg (arg:matrix, axis:si64) Row or column aggregation over a (n x m) matrix arg using aggregation function agg (see table below). - axis == 0: calculate one aggregate per row; the result is a (n x 1) (column) matrix - axis == 1: calculate one aggregate per column; the result is a (1 x m) (row) matrix function meaning sum summation aggMin minimum aggMax maximum mean arithmetic mean var variance stddev standard deviation idxMin argmin (the index of the minimum value, only for row/column-wise aggregation) idxMax argmax (the index of the maximum value, only for row/column-wise aggregation)","title":"Full/row/column aggregation"},{"location":"DaphneDSL/Builtins/#cumulative-aggregation","text":"The following built-in functions all follow the same scheme: cumAgg (arg:matrix) Cumulative aggregation over each column of the matrix arg . Returns a matrix of the same shape as arg . function meaning cumSum cumulative sum cumProd cumulative product cumMin cumulative minimum cumMax cumulative maximum","title":"Cumulative aggregation"},{"location":"DaphneDSL/Builtins/#reorganization","text":"reshape (arg:matrix, numRows:size, numCols:size) Changes the shape of arg to ( numRows x numCols ) . Note that the number of cells must be retained, i.e., the product of numRows and numCols must be equal to the product of the number of rows in arg and the number of columns in arg . transpose/t (arg:matrix) Transposes the given matrix arg . cbind (lhs:matrix/frame, rhs:matrix/frame) Concatenates two matrices or two frames horizontally. The two inputs must have the same number of rows. rbind (lhs:matrix/frame, rhs:matrix/frame) Concatenates two matrices or two frames vertically. The two inputs must have the same number of columns. reverse (arg:matrix) Reverses the rows in the given matrix arg . order (arg:matrix/frame, colIdxs:size, ..., ascs:bool, ..., returnIndexes:bool) Sorts the given matrix or frame by an arbitrary number of columns. The columns are specified in terms of their indexes (counting starts at zero). Each column can be sorted either in ascending ( true ) or descending ( false ) order (as determined by parameter ascs ). The provided number of columns and sort orders must match. The parameter returnIndexes determines whether to return the sorted data ( false ) or a column-matrix of positions representing the permutation applied by the sorting ( true ).","title":"Reorganization"},{"location":"DaphneDSL/Builtins/#matrix-decomposition-co","text":"We plan to support various matrix decompositions like eigen , lu , qr , and svd .","title":"Matrix decomposition &amp; co"},{"location":"DaphneDSL/Builtins/#deep-neural-network","text":"Note that most of these operations only have a CUDNN-based kernel for GPU execution at the moment. affine (inputData:matrix, weightData:matrix, biasData:matrix) avg_pool2d (inputData:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, poolHeight:size, poolWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size) Performs average pooling operation. max_pool2d (inputData:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, poolHeight:size, poolWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size) Performs max pooling operation. batch_norm2d (inputData:matrix, gamma, beta, emaMean, emaVar, eps) Performs batch normalization operation. biasAdd (input:matrix, bias:matrix) Adds the (1 x numChannels ) row-matrix bias to the input with the given number of channels. conv2d (input:matrix, filter:matrix, numImages:size, numChannels:size, imgHeight:size, imgWidth:size, filterHeight:size, filterWidth:size, strideHeight:size, strideWidth:size, paddingHeight:size, paddingWidth:size) 2D convolution. relu (inputData:matrix) softmax (inputData:matrix)","title":"Deep neural network"},{"location":"DaphneDSL/Builtins/#other-matrix-operations","text":"diagVector (arg:matrix) Extracts the diagonal of the given (n x n) matrix arg as a (n x 1) column-matrix. lowerTri (arg:matrix, diag:bool, values:bool) Extracts the lower triangle of the given square matrix arg by setting all elements in the upper triangle to zero. If diag is true , the elements on the diagonal are retained; otherwise, they are set to zero, too. If values is true , the non-zero elements in the lower triangle are retained; otherwise, they are set to one. upperTri (arg:matrix, diag:bool, values:bool) Extracts the upper triangle of the given square matrix arg by setting all elements in the lower triangle to zero. If diag is true , the elements on the diagonal are retained; otherwise, they are set to zero, too. If values is true , the non-zero elements in the upper triangle are retained; otherwise, they are set to one. solve (A:matrix, b:matrix) Solves the system of linear equations given by the (n x n) matrix A and the (n x 1) column-matrix b and returns the result as a (n x 1) column-matrix. replace (arg:matrix, pattern:scalar, replacement:scalar) Replaces all occurrences of the element pattern in the matrix arg by the element replacement . ctable (ys:matrix, xs:matrix[, weight:scalar][, numRows:int, numCols:int]) Returns the contingency table of two (n x 1) column-matrices ys and xs . The resulting matrix res consists of max(ys) + 1 rows and max(xs) + 1 columns. More precisely, res[x, y] = |{ k | ys[k, 0] = y and xs[k, 0] = x, 0 \u2264 k \u2264 n-1 }| * weight . In other words, starting with an all-zero result matrix, ys and xs can be thought of as lists of y / x -coordinates which indicate the result matrix's cells whose value shall be increased by weight . Note that ys and xs must not contain negative numbers. The scalar weight is an optional argument and defaults to 1.0. The weight also determines the value type of the result. Moreover, optionally, the result shape in terms of the number of rows and columns can be specified. If omited, it defaults to the smallest numbers required to accommodate all given y / x -coordinates, as expressed above. If specified, the result can be either cropped or padded with zeros to the desired shape. If a value less than zero is provided as the number of rows/columns, the respective dimension will also be determined from the input data. This built-in function can be called with 2, 3, 4, or 5 arguments, depending on which optional arguments are given. syrk (A:martix) Calculates t(A) @ A by symmetric rank-k update operations. gemv (A:matrix, x:matrix) Calcuates t(A) @ x for the given (n x m) matrix A and (n x 1) column-matrix x .","title":"Other matrix operations"},{"location":"DaphneDSL/Builtins/#extended-relational-algebra","text":"DaphneDSL supports relational algebra on frames in two ways: On the one hand, entire SQL queries can be executed over previously registered views . This aspect is described in detail in a separate tutorial . On the other hand, built-in functions for individual operations of extended relational algebra can be used on frames in DaphneDSL.","title":"Extended relational algebra"},{"location":"DaphneDSL/Builtins/#entire-sql-query","text":"registerView (viewName:str, arg:frame) Registers the frame arg to be accessible to SQL queries by the name viewName . sql (query:str) Executes the SQL query query on the frames previously registered with registerView() and returns the result as a frame.","title":"Entire SQL query"},{"location":"DaphneDSL/Builtins/#set-operations","text":"We will support set operations such as intersect , merge , and except .","title":"Set Operations"},{"location":"DaphneDSL/Builtins/#cartesian-product-and-joins","text":"cartesian (lhs:frame, rhs:frame) Calculates the cartesian (cross) product of the two input frames. innerJoin (lhs:frame, rhs:frame, lhsOn:str, rhsOn:str) Performs an inner join of the two input frames on lhs . lhsOn == rhs . rhsOn . semiJoin (lhs:frame, rhs:frame, lhsOn:str, rhsOn:str) Performs a semi join of the two input frames on lhs . lhsOn == rhs . rhsOn . Returns only the columns belonging to lhs . groupJoin (lhs:frame, rhs:frame, lhsOn:str, rhsOn:str, rhsAgg:str) Group-join of lhs and rhs on lhs.lhsOn == rhs.rhsOn with summation of rhs.rhsAgg . We will support more variants of joins, including (left/right) outer joins, theta joins, anti-joins, etc.","title":"Cartesian product and joins"},{"location":"DaphneDSL/Builtins/#frame-label-manipulation","text":"setColLabels (arg:frame, labels:str, ...) Sets the column labels of the given frame arg to the given labels . There must be as many labels as columns in arg . setColLabelsPrefix (arg:frame, predfix:str, ...) Prepends the given prefix to the labels of all columns in arg .","title":"Frame label manipulation"},{"location":"DaphneDSL/Builtins/#conversions-casts-and-copying","text":"Note that DaphneDSL offers casts in form of the as.() -expression. See the DaphneDSL Language Reference for details. copy (arg:matrix/frame) Creates a copy of arg . quantize (arg:matrix<f32>, min:f32, max:f32) Performs a min / max quantization of the values in arg . The result matrix is of value type ui8 .","title":"Conversions, casts and copying"},{"location":"DaphneDSL/Builtins/#inputoutput","text":"DAPHNE supports local file I/O for various file formats. The format is determined by the specified file name extension. Currently, the following formats are supported: \".csv\": comma-separated values \".mtx\": matrix market \".parquet\": Apache Parquet format \".dbdf\": DAPHNE's binary data format For both reading and writing, file names can be specified as absolute or relative paths. For most formats, DAPHNE requires additional information on the data and value types as well as dimensions, when reading files . These must be provided in a separate .meta -file . print (arg:scalar/matrix/frame[, newline:bool[, toStderr:bool]]) Prints the given scalar, matrix, or frame arg to stdout . The parameter newline is optional; true (the default) means a new line is started after arg , false means no new line is started. The parameter toStderr is optional; true means the text is printed to stderr , false (the default) means it is printed to stdout . readFrame (filename:str) Reads the file filename into a frame. Assumes that a .meta -file is present for the specified filename . readMatrix (filename:str) Reads the file filename into a matrix. Assumes that a .meta -file is present for the specified filename . write/writeFrame/writeMatrix (arg:matrix/frame, filename:str) Writes the given matrix or frame arg into the specified file filename . Note that the type of arg determines how to store the data; thus, it suffices to call write() (but writeFrame() and writeMatrix() can be used synonymously for consistency with reading). At the same time, this creates a .meta -file for the written file, so that it can be read again using readMatrix() / readFrame() .","title":"Input/output"},{"location":"DaphneDSL/Builtins/#data-preprocessing","text":"oneHot (arg:matrix, info:matrix<si64>) Applies one-hot-encoding to the given (n x m) matrix arg . The (1 x m) row-matrix info specifies the details (in the following, d[j] is short for info[0, j] ): If d[j] == -1, then the j -th column of arg will remain as it is. If d[j] >= 0, then the j -th column of arg will be encoded. More precisely, the j -th column of arg must contain only integral values in the range [0, d[j] - 1] , and will be replaced by d[j] columns containing only zeros and ones. For each row i in arg , the value in the as.scalar(arg[i, j]) -th of those columns is set to 1, while all others are set to 0.","title":"Data preprocessing"},{"location":"DaphneDSL/Builtins/#measurements","text":"now () Returns the current time since the epoch in nano seconds.","title":"Measurements"},{"location":"DaphneDSL/Imports/","text":"Imports How to import functions from other Daphne scripts Example usage: import \"bar.daphne\" ; import \"foo.daphne\" as \"utils\" ; print ( bar . x ); print ( utils . x ); UserConfig.json now has a new field daphnedsl_import_paths , which maps e.g., library names to a list of paths, see example: \"daphnedsl_import_paths\" : { \"default_dirs\" : [ \"test/api/cli/import/sandbox\" , \"some/other/path\" ], \"algorithms\" : [ \"test/api/cli/import/sandbox/algos\" ] } NOTE: default_dirs can hold many paths and it will look for the one specified file in each, whereas any other library names have a list consisting of one directory, from which all files will be imported (can be easily extended to multiple directories). Example: import \"a.daphne\" ; import \"algorithms\" ; print ( a . x ); print ( algorithms . kmeans . someVar ); The first import will first check if the relative path exists, then it will look for it relative to paths in default_dirs . If the specified file exists for more than one relative path, an error will be thrown. The second import goes to algorithms directory from UserConfig and imports all files from it. Paths from UserConfig get to DaphneDSLVisitor from daphne.cpp via DaphneUserConfig . Variable name collision resolution: Whenever we stumble upon equal prefixes (e.g., files with the same name in different directories), a parent directory of the file where conflict is detected is prepended before prefix. Example: import \"somedir/a.daphne\" ; import \"otherdir/a.daphne\" ; print ( a . x ); print ( otherdir . a . x ); NOTE: the parent directory may be prepended even though you never specified it (e.g., the import script is in the same directory as the original script). Example: import \"somedir/a.daphne\" ; import \"a.daphne\" ; print ( a . x ); print ( otherdir . a . x ); Libraries and aliases: Currently, the following example is allowed: import \"algorithms\" ; import \"sandbox/b.daphne\" as \"algorithms\" ; print ( algorithms . x ); print ( algorithms . kmeans1 . someVar ); Even though both prefixes will begin with algorithms. , the entire library content's prefix is extended with filenames. It is up to user to not confuse yourself. Cascade imports: Any variables/functions imported into the script we are currently importing will be discarded. Example import scheme: A<-B<-C . A imports B, B imports C. B uses some vars/functions from C, but A doesn't \"see\" any of C's content.","title":"Imports"},{"location":"DaphneDSL/Imports/#imports","text":"How to import functions from other Daphne scripts Example usage: import \"bar.daphne\" ; import \"foo.daphne\" as \"utils\" ; print ( bar . x ); print ( utils . x ); UserConfig.json now has a new field daphnedsl_import_paths , which maps e.g., library names to a list of paths, see example: \"daphnedsl_import_paths\" : { \"default_dirs\" : [ \"test/api/cli/import/sandbox\" , \"some/other/path\" ], \"algorithms\" : [ \"test/api/cli/import/sandbox/algos\" ] } NOTE: default_dirs can hold many paths and it will look for the one specified file in each, whereas any other library names have a list consisting of one directory, from which all files will be imported (can be easily extended to multiple directories). Example: import \"a.daphne\" ; import \"algorithms\" ; print ( a . x ); print ( algorithms . kmeans . someVar ); The first import will first check if the relative path exists, then it will look for it relative to paths in default_dirs . If the specified file exists for more than one relative path, an error will be thrown. The second import goes to algorithms directory from UserConfig and imports all files from it. Paths from UserConfig get to DaphneDSLVisitor from daphne.cpp via DaphneUserConfig . Variable name collision resolution: Whenever we stumble upon equal prefixes (e.g., files with the same name in different directories), a parent directory of the file where conflict is detected is prepended before prefix. Example: import \"somedir/a.daphne\" ; import \"otherdir/a.daphne\" ; print ( a . x ); print ( otherdir . a . x ); NOTE: the parent directory may be prepended even though you never specified it (e.g., the import script is in the same directory as the original script). Example: import \"somedir/a.daphne\" ; import \"a.daphne\" ; print ( a . x ); print ( otherdir . a . x ); Libraries and aliases: Currently, the following example is allowed: import \"algorithms\" ; import \"sandbox/b.daphne\" as \"algorithms\" ; print ( algorithms . x ); print ( algorithms . kmeans1 . someVar ); Even though both prefixes will begin with algorithms. , the entire library content's prefix is extended with filenames. It is up to user to not confuse yourself. Cascade imports: Any variables/functions imported into the script we are currently importing will be discarded. Example import scheme: A<-B<-C . A imports B, B imports C. B uses some vars/functions from C, but A doesn't \"see\" any of C's content.","title":"Imports"},{"location":"DaphneDSL/LanguageRef/","text":"Language Reference DaphneDSL is DAPHNE's domain-specific language (DSL). DaphneDSL is written in plain text files, typically ending with .daphne or .daph . It is a case-sensitive language inspired by ML systems as well as languages and libraries for numerical computation like Julia, Python NumPy, R, and SystemDS DML. Its syntax is inspired by C/Java-like languages. Hello World A simple hello-world script can look as follows: print ( \"hello world\" ); Assuming this script is stored in the file hello.daphne , it can be executed by the following command: bin/daphne hello.daphne The remainder of this document discusses the language features of DaphneDSL in detail as they are right now , but note that DaphneDSL is still evolving . Variables Variables are used to refer to values. Valid identifiers start with a letter ( a-z , A-Z ) or an underscore ( _ ) that can be followed by any number of letters ( a-z , A-Z ), underscores ( _ ), and decimal digits ( 0-9 ). The following reserved keywords must not be used as identifiers: if , else , while , do , for , in , true , false , as , def , return , import , matrix , frame , scalar , f64 , f32 , si64 , si8 , ui64 , ui32 , ui8 , str , nan , and inf . Examples: X y _hello123 a_long_Variable123_456NAME Variables do not need to be (and cannot be) declared. Instead, simply assign a value to a variable and its type will be inferred. Variables must have been assigned to before they are used in an expression. Types DaphneDSL differentiates data types and value types . Currently, DaphneDSL supports the following abstract data types : matrix : homogeneous value type for all cells frame : a table with columns of potentially different value types scalar : a single value Value types specify the representation of individual values. We currently support: floating-point numbers of various widths: f64 , f32 signed and unsigned integers of various widths: si64 , si32 , si8 , ui64 , ui32 , ui8 strings str (currently only for scalars, support for matrix elements is still experimental) booleans bool (currently only for scalars) Data types and value types can be combined, e.g.: matrix<f64> is a matrix of double-precision floating point values Comments DaphneDSL supports single-line comments (starting with # or // ) and multi-line comments (everything enclosed in /* and */ ). Examples: # this is a comment print ( \"Hello World!\" ); // this is also a comment /* comments can span multiple lines */ Expressions Simple Expressions Simple expressions constitute the basis of all expressions, and DaphneDSL offers three kinds: Literals Literals represent hard-coded values and can be of various data and value types: Scalar literals Integer literals are specified in decimal notation and have the type si64 . Examples : 0 , 123 , -456 Floating-point literals are specified in decimal notation and have the type f64 . Furthermore, the following literals stand for special floating-point values: nan , inf , -inf . Examples : 0.0 , 123.0 , -456.78 , inf , nan Boolean literals can be false and true . String literals are enclosed in quotation marks \" . Special characters must be escaped using a backslash: \\n : new line \\t : tab \\\" : quotation mark \\\\ : backslash \\b : backspace \\f : line feed \\r : carriage return Examples : \"Hello World!\" \"line 1\\nline 2\\nline 3\" \"This is \\\"hello.daphne\\\".\" Matrix literals A matrix literal consists of a comma-separated list of scalar literals, enclosed in square braces. All scalars specified for the elements must be of the same type. Furthermore, all specified elements must be actual literals, i.e., expressions are not supported yet. The resulting matrix is always a column matrix, i.e., if n elements are specified, its shape is (n x 1) . Note that the built-in function reshape can be used to modify the shape. Examples: [ 1.0 , 0.0 , -4.0 ] # matrix<f64> with shape (3 x 1) reshape ([ 1 , 2 , 3 , 4 ], 1 , 4 ) # matrix<si64> with shape (1 x 4) Variable Expressions Variables are referenced by their name. Examples: x Script arguments Script arguments are named literals that can be passed to a DaphneDSL script. They are referenced by a dollar sign $ followed by the argument's name. Examples: $ x Note that matrix literals are not supported as script arguments yet. Complex Expressions DaphneDSL offers several ways to build more complex expressions. Operators DaphneDSL currently supports the following binary operators: Operator Meaning @ matrix multiplication (highest precedence) ^ exponentiation % modulo * , / multiplication, division + , - addition/string concatenation, subtraction == , != , < , <= , > , >= comparison && logical AND \\|\\| logical OR (lowest precedence) We plan to add more operators, including unary operators. Matrix multiplication ( @ ): The inputs must be matrices of compatible shapes, and the output is always a matrix. All other operators: The following table shows which combinations of inputs are allowed and which result they yield: Left input Right input Result Details scalar scalar scalar binary operation of two scalars matrix (n x m) scalar matrix (n x m) element-wise operation of each value with scalar scalar matrix (n x m) matrix (n x m) element-wise operation of scalar with each value (*) matrix (n x m) matrix (n x m) matrix (n x m) element-wise operation on corresponding values matrix (n x m) matrix (1 x m) matrix (n x m) broadcasting of row-vector matrix (n x m) matrix (n x 1) matrix (n x m) broadcasting of column-vector (*) Scalar- op -matrix operations are so far only supported for + , - , * , / ; for / only if the matrix is of a floating-point value type. In the future, we will fully support scalar- op -matrix operations as well as row/column-matrices as the left-hand-side operands. Examples: 1.5 * X @ y + 0.001 x == 1 && y < 3.5 Parentheses Parentheses can be used to manually control operator precedence. Examples: 1 * ( 2 + 3 ) Indexing (Right) indexing enables the extraction of a part of the rows and/or columns of a data object (matrix/frame) into a new data object. The result is always a data object of the same data type as the input (even 1 x 1 results need to be casted to scalars explicitly, if needed). The rows and columns to extract can be specified independently in any of the following ways: Omit indexing Omitting the specification of rows/columns means extracting all rows/columns. Examples: X [, ] # same as X (all rows and columns) Indexing by position This is supported for addressing rows and columns in matrices and frames. Single row/column position: Extracts only the specified row/column. Examples: X [ 2 , 3 ] # extracts the cell in row 2, column 3 as a 1 x 1 matrix Row/column range: Extracts all rows/columns between a lower bound (inclusive) and an upper bound (exclusive). The lower and upper bounds can be omitted independently of each other. In that case, they are replaced by zero and the number of rows/columns, respectively. Examples: X [ 2 : 5 , 3 ] # extracts rows 2, 3, 4 of column 3 X [ 2 , 3 : ] # extracts row 2 of all columns from column 3 onward X [ : 5 , 3 ] # extracts rows 0, 1, 2, 3, 4 of column 3 X [ : , 3 ] # extracts all rows of column 3, same as X[, 3] Arbitrary sequence of row/column positions: Expects a sequence of row/column positions as a column (n x 1) matrix . There are no restrictions on these positions, except that they must be in bounds. In particular, they do not need to be contiguous, sorted, or unique. Examples: X [ [ 5 , 1 , 3 ], ] # extracts rows 5, 1, and 3 X [, [ 2 , 2 , 2 ] ] # extracts column 2 three times Note that, when using matrix literals to specify the positions, a space must be left between the opening/closing bracket [ / ] of the indexing and that of the matrix literal, in order to avoid confusion with the indexing by bit vector. A few remarks on positions: Counting starts at zero. For instance, a 5 x 3 matrix has row positions 0, 1, 2, 3, and 4, and column positions 0, 1, and 2. They must be non-negative. They can be provided as integers or floating-point numbers (the latter are rounded down to integers). They can be given as literals or as any expression evaluating to a suitable value. Examples: X [ 1.2 , ] # same as X[1, ] X [ 1.9 , ] # same as X[1, ] X [ i , ( j + 2 * sum ( Y )) : ] # expressions Indexing by label So far, this is only supported for addressing columns of frames. Single column label: Extracts only the column with the given label. Examples: X [, \"revenue\" ] # extracts the column labeled \"revenue\" X [ 100 : 200 , \"revenue\" ] # extracts rows 100 through 199 of the column labeled \"revenue\" Indexing by bit vector This is not supported for addressing columns of frames yet. For each row/column, a single zero/one entry (\"bit\") must be provided. More precisely, a ( r x 1 ) matrix is required on data objects with r rows, and a ( c x 1 ) matrix is required on data objects with c columns. Only the rows/columns with a corresponding 1-value in the bit vector are present in the result. Note that double square brackets ( [[...]] ) must be used to distinguish indexing by bit vector from indexing by an arbitrary sequence of positions. Examples: # Assume X is a 4x3 matrix. X [[[ 0 , 1 , 1 , 0 ], ]] # extracts rows 1 and 2 # same as X[[1, 2], ] X [[, [ 1 , 0 , 1 ] ]] # extracts columns 0 and 2 # same as X[, [0, 2]] X [[[ 0 , 1 , 1 , 0 ], [ 1 , 0 , 1 ] ]] # extracts columns 0 and 2 of rows 1 and 2 # same as X[[1, 2], [0, 2]] Note that, when using a matrix literal to provide the column bit vector, there must be a space between the closing bracket ] of the matrix literal and the closing double bracket ]] of the indexing expression, e.g., X[[, [0] ]] instead of X[[, [0]]] . Casts Values can be casted to a particular type explicitly. Currently, it is possible to cast: between scalars of different types between matrices of different value types between matrix and frame between scalar and 1x1 matrix/frame Casts can either fully specify the target data and value type, or specify only the target data type or the target value type. In the latter case, the unspecified part of the type will be retained from the argument. Examples: as.scalar < f64 > ( x ) # casts x to f64 scalar as.matrix < ui32 > ( x ) # casts x to a matrix of ui32 as.scalar ( x ) # casts x to a scalar of the same value type as x as.matrix ( x ) # casts x to a matrix of the same value type as x as.frame ( x ) # casts x to a frame whose column types are the value type of x as.f32 ( x ) # casts x to the same data type as x, but with value type f32 as.ui8 ( x ) # casts x to the same data type as x, but with value type ui8 Note that casting to frames does not support changing the value/column type yet, i.e., expressions like as.frame<f64, si32, f32>(x) and as.f64(x) (on a frame x ) do not work yet. Function calls Function calls can address built-in functions as well as user-defined functions , but the syntax is the same in both cases: The name of the function followed by a comma-separated list of positional parameters in parentheses. Examples: print ( \"hello\" ); t ( myMatrix ); seq ( 0 , 10 , 2 ); Conditional expression DaphneDSL supports the conditional expression with the general syntax: condition ? then - value : else - value The condition can be either a scalar or a matrix. Condition is a scalar: If the condition is true (when casted to boolean), then the result is the then-value . Otherwise, the result is the else-value . The then-value and the else-value must have the same type. Condition is a matrix (elementwise application): In this case, the condition matrix can be of any value type, but must only contain 0 or 1 values of that type (for all other values, the behavior is unspecified). The then-value and else-value must be matrices of the same shape as the condition and must have the same value type as each other. The ?: -operator is applied in an elementwise fashion, i.e., individually for each triple of corresponding elements in condition/ then-value / else-value . The then-value and else-value may also be scalars, in which case they are treated like matrices with a constant value. The result is a matrix of the same shape as the condition and the same value type as the then-value / else-value . Examples: ( i > 5 ) ? 42.0 : -42.0 # 42.0 if i > 5, -42.0 otherwise [ 1 , 0 , 0 , 1 ] ? [ 1.0 , 2.0 , 3.0 , 4.0 ] : 99.9 # [1.0, 99.9, 99.9, 4.0] Statements At the highest level, a DaphneDSL script is a sequence of statements. Statements comprise assignments, various forms of control flow, and declarations of user-defined functions. Expression statement Every expression followed by a semicolon ; can be used as a statement. This is useful for expressions (especially function calls) which do not return a value. Nevertheless, it can also be used for expressions with one or more return values, in which case these values are ignored. Examples: print ( \"hello\" ); # built-in function without return value 1 + 2 ; # value is ignored, useless but possible doSomething (); # possible return values are ignored, but the execution # of the user-defined function could have side effects Assignment statement The return value(s) of an expression can be assigned to one (or more) variable(s). Single-assignments are used for expressions with exactly one return value. Examples: x = 1 + 2 ; Multi-assignments are used for expressions with more than one return value. Examples: evals , evecs = eigen ( A ); # eigen() returns two values, the (n x 1)-matrix of # eigen-values and the (n x n)-matrix of eigen-vectors # of the input matrix A. Indexing The value of an expression can also be assigned to a partition of an existing data object . This is done by (left) indexing, whose syntax is similar to (right) indexing in expressions. Currently, left indexing is supported only for matrices. Furthermore, the rows/columns cannot be addressed by arbitrary positions lists or bit vectors (yet). Examples: X [ 5 , 2 ] = [ 123 ]; # insert (1 x 1)-matrix X [ 10 : 20 , 2 : 5 ] = fill ( 123 , 10 , 3 ); # insert (10 x 3)-matrix The following conditions must be fulfilled: The left-hand-side variable must have been initialized. The left-hand-side variable must be of data type matrix. The right-hand-side expression must return a matrix. The shapes of the partition addressed on the left-hand side and the return value of the right-hand-side expression must match. The value type of the left-hand-side and right-hand-side matrices must match. Left indexing can be used with both single and multi-assignments. With the latter, it can be used with each variable on the left-hand side individually and independently. Examples: x , Y [ 3 , : ], Z = calculateSomething (); Copy-on-write semantics Left indexing enables the modification of existing data objects, whereby the semantics is copy-on-write . That is, if two different variables represent the same runtime data object, then left indexing on one of these variables does not have any effects on the other one. This is achieved by transparently copying the data as necessary. Examples: A = ... ; # some matrix B = A ; # copy-by-reference B [ ... , ... ] = ... ; # copy-on-write: changes B, but no effect on A A [ ... , ... ] = ... ; # copy-on-write: changes A, but no effect on B Control Flow statements DaphneDSL supports block statements, conditional branching, and various kinds of loops. These control flow constructs can be nested arbitrarily. Block statement A block statement allows to view an enclosed sequence of statements like a single statement. This is very useful in combination with the control flow statements described below. Besides that, a block statement starts a new scope in terms of visibility of variables. Within a block, all variables from outside the block can be read and written. However, variables created inside a block are not visible anymore after the block. The syntax of a block statement is: { statement1 statement2 ... } Examples: x = 1 ; { print ( x ); # read access x = 2 ; # write access y = 1 ; # variable created inside the block } print ( x ); # prints 2 print ( y ); # error If-then-else The syntax of an if-then-else statement is as follows: if ( condition ) then - statement else else - statement condition is an expression returning a single value. If this value is true (when casted to value type bool , if necessary), the then-statement is executed. Otherwise, the else-statement is executed, if it is present . Note that the else -branch (keyword and statement) may be omitted. Furthermore, then-statement and else-statement can be block statements, to allow any number of statements in the then and else-branches. Examples: if ( sum ( X ) == 0 ) X = X + 1 ; if ( 2 * x > y ) { z = z / 2 ; a = true ; } else z = z * 2 ; if ( a ) print ( \"a\" ); else if ( b ) print ( \"not a, but b\" ); else print ( \"neither a nor b\" ); Loops DaphneDSL supports for-loops, while-loops, and do-while-loops. In the future we plan to support also parfor-loops as well as break and continue statements. For-Loops For-loops are used to iterate over the elements of a sequence of integers. The syntax of a for-loop is as follows: for ( var in start : end [ : step ]) body - statement var must be a valid identifier and is assigned the values from start to end in increments of step . start , end , and step are expressions evaluating to a single number. step is optional and defaults to 1 if end is greater than start , or -1 otherwise. In that sense, for-loops can also be used to count backwards by setting start greater than end . The body-statement is executed for each value in the sequence, and within the body-statement , this value is accessible via the read-only variable var . Note that the body-statement may be a block statement enclosing an arbitrary number of statements. Examples: for ( i in 1 : 3 ) print ( i ); # 1 2 3 x = 0 ; y = 0 ; for ( i in 10 : 1 : -3 ) { x = x + i ; y = y + 1 ; } print ( x ); # 22 print ( y ); # 4 While-Loops While loops are used to execute a (block of) statement(s) as long as an arbitrary condition holds true. The syntax of a while-loop is as follows: while ( condition ) body - statement condition is an expression returning a single value, and is evaluated before each iteration. If this value is true (when casted to value type bool , if necessary), the body-statement is executed, and the loop starts anew. Otherwise, the program continues after the loop. Note that the body-statement may be a block statement enclosing an arbitrary number of statements. Examples: i = 0 ; while ( i < 10 && ! converged ) { A = A @ B ; converged = sum ( A ); i = i + 1 ; } Do-While-Loops Do-while-loops are a variant of while-loops, which checks the condition after each iteration. Consequently, a do-while-loop always executes at least one iteration. The syntax of a do-while-loop is as follows: do body - statement while ( condition ); The semicolon at the end is optional. Note that the body-statement may be a block statement enclosing an arbitrary number of statements. Examples: i = 5 ; do { A = sqrt ( A ); i = i - 1 ; } while ( mean ( A ) > 100 && i > 0 ); User-defined Functions (UDFs) DaphneDSL allows users to define their own functions. The syntax of a function definition looks as follows: def funcName ( paramName1 [: paramType1 ], paramName2 [: paramType2 ], ...) [ -> returnType1 , returnType2 , ...] { statement1 statement2 ... } The function name must be a valid and unique identifier. A function can have zero, one, or more parameters, and their names must be valid and unique identifiers. Furthermore, a function may return zero, one, or more values. The types of parameters are optional and can be provided or omitted for each parameter individually. The types of the return values are optional, but if omitted, exactly one return value is implicitly assumed. Functions with multiple return values must specify the types of all return values. See also typed and untyped functions below. The body of a function definition is always a block statement, i.e., it must be enclosed in curly braces {} even if it is just a single statement. So far, DaphneDSL supports only positional parameters to functions, but in the future, we plan to support named keyword arguments as well. Functions must be defined in the top-level scope of a DaphneDSL script, i.e., a function definition must not be nested within a control-flow statement or within another function definition. Returning Values User-defined functions can return zero, one, or more (comma-separated) values using the return -statement. The number of returned values must match the function signature. Examples: return ; # don ' t return any values return x ; # return exactly one value return x , y ; # return two values Currently, the return statement must be the last statement of a function. Alternatively, it can be nested into if-then-else (early return), as long as it is ensured that there is exactly one return statement at the end of each path through the function (experimental). Examples: def fib ( n : si64 ) -> si64 { if ( n <= 0 ) return 0 ; if ( n <= 1 ) return 1 ; return fib ( n - 1 ) + fib ( n - 2 ); } def nextTwo(a: si64) -> si64, si64 { return a + 1, a + 2; } Calling User-defined Functions A user-defined function can be called like any other (built-in) function (see function-call expressions above). Examples: x = 2 * fib ( 5 ) + 123 ; y , z = nextTwo ( 123 ); Typed and Untyped Functions (experimental) DaphneDSL supports both typed and untyped functions. The definition of a typed function specifies the data and value types of all parameters and return values. Hence, a typed function can only be called with inputs of the specified types (if a provided input has an unexpected type, it is automatically casted to the expected type, if possible), and always returns outputs of the specified types. A typed function is compiled exactly once and specialized to the specified parameter and return types. In contrast to that, the definition of an untyped function leaves the data and value type, or just the value type, of one or more parameters and/or return values unspecified. At call sites, a value of any type, or any value type, can be passed to an untyped parameter. As a consequence, an untyped function is compiled and specialized on demand according to the types at a call site. Consistently, the types of untyped return values are infered from the parameter types and operations. Example Scripts A few example DaphneDSL scripts can be found in: scripts/algorithms/ scripts/examples/ test/api/cli/algorithms/","title":"LanguageRef"},{"location":"DaphneDSL/LanguageRef/#language-reference","text":"DaphneDSL is DAPHNE's domain-specific language (DSL). DaphneDSL is written in plain text files, typically ending with .daphne or .daph . It is a case-sensitive language inspired by ML systems as well as languages and libraries for numerical computation like Julia, Python NumPy, R, and SystemDS DML. Its syntax is inspired by C/Java-like languages.","title":"Language Reference"},{"location":"DaphneDSL/LanguageRef/#hello-world","text":"A simple hello-world script can look as follows: print ( \"hello world\" ); Assuming this script is stored in the file hello.daphne , it can be executed by the following command: bin/daphne hello.daphne The remainder of this document discusses the language features of DaphneDSL in detail as they are right now , but note that DaphneDSL is still evolving .","title":"Hello World"},{"location":"DaphneDSL/LanguageRef/#variables","text":"Variables are used to refer to values. Valid identifiers start with a letter ( a-z , A-Z ) or an underscore ( _ ) that can be followed by any number of letters ( a-z , A-Z ), underscores ( _ ), and decimal digits ( 0-9 ). The following reserved keywords must not be used as identifiers: if , else , while , do , for , in , true , false , as , def , return , import , matrix , frame , scalar , f64 , f32 , si64 , si8 , ui64 , ui32 , ui8 , str , nan , and inf . Examples: X y _hello123 a_long_Variable123_456NAME Variables do not need to be (and cannot be) declared. Instead, simply assign a value to a variable and its type will be inferred. Variables must have been assigned to before they are used in an expression.","title":"Variables"},{"location":"DaphneDSL/LanguageRef/#types","text":"DaphneDSL differentiates data types and value types . Currently, DaphneDSL supports the following abstract data types : matrix : homogeneous value type for all cells frame : a table with columns of potentially different value types scalar : a single value Value types specify the representation of individual values. We currently support: floating-point numbers of various widths: f64 , f32 signed and unsigned integers of various widths: si64 , si32 , si8 , ui64 , ui32 , ui8 strings str (currently only for scalars, support for matrix elements is still experimental) booleans bool (currently only for scalars) Data types and value types can be combined, e.g.: matrix<f64> is a matrix of double-precision floating point values","title":"Types"},{"location":"DaphneDSL/LanguageRef/#comments","text":"DaphneDSL supports single-line comments (starting with # or // ) and multi-line comments (everything enclosed in /* and */ ). Examples: # this is a comment print ( \"Hello World!\" ); // this is also a comment /* comments can span multiple lines */","title":"Comments"},{"location":"DaphneDSL/LanguageRef/#expressions","text":"","title":"Expressions"},{"location":"DaphneDSL/LanguageRef/#simple-expressions","text":"Simple expressions constitute the basis of all expressions, and DaphneDSL offers three kinds:","title":"Simple Expressions"},{"location":"DaphneDSL/LanguageRef/#literals","text":"Literals represent hard-coded values and can be of various data and value types:","title":"Literals"},{"location":"DaphneDSL/LanguageRef/#scalar-literals","text":"Integer literals are specified in decimal notation and have the type si64 . Examples : 0 , 123 , -456 Floating-point literals are specified in decimal notation and have the type f64 . Furthermore, the following literals stand for special floating-point values: nan , inf , -inf . Examples : 0.0 , 123.0 , -456.78 , inf , nan Boolean literals can be false and true . String literals are enclosed in quotation marks \" . Special characters must be escaped using a backslash: \\n : new line \\t : tab \\\" : quotation mark \\\\ : backslash \\b : backspace \\f : line feed \\r : carriage return Examples : \"Hello World!\" \"line 1\\nline 2\\nline 3\" \"This is \\\"hello.daphne\\\".\"","title":"Scalar literals"},{"location":"DaphneDSL/LanguageRef/#matrix-literals","text":"A matrix literal consists of a comma-separated list of scalar literals, enclosed in square braces. All scalars specified for the elements must be of the same type. Furthermore, all specified elements must be actual literals, i.e., expressions are not supported yet. The resulting matrix is always a column matrix, i.e., if n elements are specified, its shape is (n x 1) . Note that the built-in function reshape can be used to modify the shape. Examples: [ 1.0 , 0.0 , -4.0 ] # matrix<f64> with shape (3 x 1) reshape ([ 1 , 2 , 3 , 4 ], 1 , 4 ) # matrix<si64> with shape (1 x 4)","title":"Matrix literals"},{"location":"DaphneDSL/LanguageRef/#variable-expressions","text":"Variables are referenced by their name. Examples: x","title":"Variable Expressions"},{"location":"DaphneDSL/LanguageRef/#script-arguments","text":"Script arguments are named literals that can be passed to a DaphneDSL script. They are referenced by a dollar sign $ followed by the argument's name. Examples: $ x Note that matrix literals are not supported as script arguments yet.","title":"Script arguments"},{"location":"DaphneDSL/LanguageRef/#complex-expressions","text":"DaphneDSL offers several ways to build more complex expressions.","title":"Complex Expressions"},{"location":"DaphneDSL/LanguageRef/#operators","text":"DaphneDSL currently supports the following binary operators: Operator Meaning @ matrix multiplication (highest precedence) ^ exponentiation % modulo * , / multiplication, division + , - addition/string concatenation, subtraction == , != , < , <= , > , >= comparison && logical AND \\|\\| logical OR (lowest precedence) We plan to add more operators, including unary operators. Matrix multiplication ( @ ): The inputs must be matrices of compatible shapes, and the output is always a matrix. All other operators: The following table shows which combinations of inputs are allowed and which result they yield: Left input Right input Result Details scalar scalar scalar binary operation of two scalars matrix (n x m) scalar matrix (n x m) element-wise operation of each value with scalar scalar matrix (n x m) matrix (n x m) element-wise operation of scalar with each value (*) matrix (n x m) matrix (n x m) matrix (n x m) element-wise operation on corresponding values matrix (n x m) matrix (1 x m) matrix (n x m) broadcasting of row-vector matrix (n x m) matrix (n x 1) matrix (n x m) broadcasting of column-vector (*) Scalar- op -matrix operations are so far only supported for + , - , * , / ; for / only if the matrix is of a floating-point value type. In the future, we will fully support scalar- op -matrix operations as well as row/column-matrices as the left-hand-side operands. Examples: 1.5 * X @ y + 0.001 x == 1 && y < 3.5","title":"Operators"},{"location":"DaphneDSL/LanguageRef/#parentheses","text":"Parentheses can be used to manually control operator precedence. Examples: 1 * ( 2 + 3 )","title":"Parentheses"},{"location":"DaphneDSL/LanguageRef/#indexing","text":"(Right) indexing enables the extraction of a part of the rows and/or columns of a data object (matrix/frame) into a new data object. The result is always a data object of the same data type as the input (even 1 x 1 results need to be casted to scalars explicitly, if needed). The rows and columns to extract can be specified independently in any of the following ways:","title":"Indexing"},{"location":"DaphneDSL/LanguageRef/#omit-indexing","text":"Omitting the specification of rows/columns means extracting all rows/columns. Examples: X [, ] # same as X (all rows and columns)","title":"Omit indexing"},{"location":"DaphneDSL/LanguageRef/#indexing-by-position","text":"This is supported for addressing rows and columns in matrices and frames. Single row/column position: Extracts only the specified row/column. Examples: X [ 2 , 3 ] # extracts the cell in row 2, column 3 as a 1 x 1 matrix Row/column range: Extracts all rows/columns between a lower bound (inclusive) and an upper bound (exclusive). The lower and upper bounds can be omitted independently of each other. In that case, they are replaced by zero and the number of rows/columns, respectively. Examples: X [ 2 : 5 , 3 ] # extracts rows 2, 3, 4 of column 3 X [ 2 , 3 : ] # extracts row 2 of all columns from column 3 onward X [ : 5 , 3 ] # extracts rows 0, 1, 2, 3, 4 of column 3 X [ : , 3 ] # extracts all rows of column 3, same as X[, 3] Arbitrary sequence of row/column positions: Expects a sequence of row/column positions as a column (n x 1) matrix . There are no restrictions on these positions, except that they must be in bounds. In particular, they do not need to be contiguous, sorted, or unique. Examples: X [ [ 5 , 1 , 3 ], ] # extracts rows 5, 1, and 3 X [, [ 2 , 2 , 2 ] ] # extracts column 2 three times Note that, when using matrix literals to specify the positions, a space must be left between the opening/closing bracket [ / ] of the indexing and that of the matrix literal, in order to avoid confusion with the indexing by bit vector. A few remarks on positions: Counting starts at zero. For instance, a 5 x 3 matrix has row positions 0, 1, 2, 3, and 4, and column positions 0, 1, and 2. They must be non-negative. They can be provided as integers or floating-point numbers (the latter are rounded down to integers). They can be given as literals or as any expression evaluating to a suitable value. Examples: X [ 1.2 , ] # same as X[1, ] X [ 1.9 , ] # same as X[1, ] X [ i , ( j + 2 * sum ( Y )) : ] # expressions","title":"Indexing by position"},{"location":"DaphneDSL/LanguageRef/#indexing-by-label","text":"So far, this is only supported for addressing columns of frames. Single column label: Extracts only the column with the given label. Examples: X [, \"revenue\" ] # extracts the column labeled \"revenue\" X [ 100 : 200 , \"revenue\" ] # extracts rows 100 through 199 of the column labeled \"revenue\"","title":"Indexing by label"},{"location":"DaphneDSL/LanguageRef/#indexing-by-bit-vector","text":"This is not supported for addressing columns of frames yet. For each row/column, a single zero/one entry (\"bit\") must be provided. More precisely, a ( r x 1 ) matrix is required on data objects with r rows, and a ( c x 1 ) matrix is required on data objects with c columns. Only the rows/columns with a corresponding 1-value in the bit vector are present in the result. Note that double square brackets ( [[...]] ) must be used to distinguish indexing by bit vector from indexing by an arbitrary sequence of positions. Examples: # Assume X is a 4x3 matrix. X [[[ 0 , 1 , 1 , 0 ], ]] # extracts rows 1 and 2 # same as X[[1, 2], ] X [[, [ 1 , 0 , 1 ] ]] # extracts columns 0 and 2 # same as X[, [0, 2]] X [[[ 0 , 1 , 1 , 0 ], [ 1 , 0 , 1 ] ]] # extracts columns 0 and 2 of rows 1 and 2 # same as X[[1, 2], [0, 2]] Note that, when using a matrix literal to provide the column bit vector, there must be a space between the closing bracket ] of the matrix literal and the closing double bracket ]] of the indexing expression, e.g., X[[, [0] ]] instead of X[[, [0]]] .","title":"Indexing by bit vector"},{"location":"DaphneDSL/LanguageRef/#casts","text":"Values can be casted to a particular type explicitly. Currently, it is possible to cast: between scalars of different types between matrices of different value types between matrix and frame between scalar and 1x1 matrix/frame Casts can either fully specify the target data and value type, or specify only the target data type or the target value type. In the latter case, the unspecified part of the type will be retained from the argument. Examples: as.scalar < f64 > ( x ) # casts x to f64 scalar as.matrix < ui32 > ( x ) # casts x to a matrix of ui32 as.scalar ( x ) # casts x to a scalar of the same value type as x as.matrix ( x ) # casts x to a matrix of the same value type as x as.frame ( x ) # casts x to a frame whose column types are the value type of x as.f32 ( x ) # casts x to the same data type as x, but with value type f32 as.ui8 ( x ) # casts x to the same data type as x, but with value type ui8 Note that casting to frames does not support changing the value/column type yet, i.e., expressions like as.frame<f64, si32, f32>(x) and as.f64(x) (on a frame x ) do not work yet.","title":"Casts"},{"location":"DaphneDSL/LanguageRef/#function-calls","text":"Function calls can address built-in functions as well as user-defined functions , but the syntax is the same in both cases: The name of the function followed by a comma-separated list of positional parameters in parentheses. Examples: print ( \"hello\" ); t ( myMatrix ); seq ( 0 , 10 , 2 );","title":"Function calls"},{"location":"DaphneDSL/LanguageRef/#conditional-expression","text":"DaphneDSL supports the conditional expression with the general syntax: condition ? then - value : else - value The condition can be either a scalar or a matrix. Condition is a scalar: If the condition is true (when casted to boolean), then the result is the then-value . Otherwise, the result is the else-value . The then-value and the else-value must have the same type. Condition is a matrix (elementwise application): In this case, the condition matrix can be of any value type, but must only contain 0 or 1 values of that type (for all other values, the behavior is unspecified). The then-value and else-value must be matrices of the same shape as the condition and must have the same value type as each other. The ?: -operator is applied in an elementwise fashion, i.e., individually for each triple of corresponding elements in condition/ then-value / else-value . The then-value and else-value may also be scalars, in which case they are treated like matrices with a constant value. The result is a matrix of the same shape as the condition and the same value type as the then-value / else-value . Examples: ( i > 5 ) ? 42.0 : -42.0 # 42.0 if i > 5, -42.0 otherwise [ 1 , 0 , 0 , 1 ] ? [ 1.0 , 2.0 , 3.0 , 4.0 ] : 99.9 # [1.0, 99.9, 99.9, 4.0]","title":"Conditional expression"},{"location":"DaphneDSL/LanguageRef/#statements","text":"At the highest level, a DaphneDSL script is a sequence of statements. Statements comprise assignments, various forms of control flow, and declarations of user-defined functions.","title":"Statements"},{"location":"DaphneDSL/LanguageRef/#expression-statement","text":"Every expression followed by a semicolon ; can be used as a statement. This is useful for expressions (especially function calls) which do not return a value. Nevertheless, it can also be used for expressions with one or more return values, in which case these values are ignored. Examples: print ( \"hello\" ); # built-in function without return value 1 + 2 ; # value is ignored, useless but possible doSomething (); # possible return values are ignored, but the execution # of the user-defined function could have side effects","title":"Expression statement"},{"location":"DaphneDSL/LanguageRef/#assignment-statement","text":"The return value(s) of an expression can be assigned to one (or more) variable(s). Single-assignments are used for expressions with exactly one return value. Examples: x = 1 + 2 ; Multi-assignments are used for expressions with more than one return value. Examples: evals , evecs = eigen ( A ); # eigen() returns two values, the (n x 1)-matrix of # eigen-values and the (n x n)-matrix of eigen-vectors # of the input matrix A.","title":"Assignment statement"},{"location":"DaphneDSL/LanguageRef/#indexing_1","text":"The value of an expression can also be assigned to a partition of an existing data object . This is done by (left) indexing, whose syntax is similar to (right) indexing in expressions. Currently, left indexing is supported only for matrices. Furthermore, the rows/columns cannot be addressed by arbitrary positions lists or bit vectors (yet). Examples: X [ 5 , 2 ] = [ 123 ]; # insert (1 x 1)-matrix X [ 10 : 20 , 2 : 5 ] = fill ( 123 , 10 , 3 ); # insert (10 x 3)-matrix The following conditions must be fulfilled: The left-hand-side variable must have been initialized. The left-hand-side variable must be of data type matrix. The right-hand-side expression must return a matrix. The shapes of the partition addressed on the left-hand side and the return value of the right-hand-side expression must match. The value type of the left-hand-side and right-hand-side matrices must match. Left indexing can be used with both single and multi-assignments. With the latter, it can be used with each variable on the left-hand side individually and independently. Examples: x , Y [ 3 , : ], Z = calculateSomething (); Copy-on-write semantics Left indexing enables the modification of existing data objects, whereby the semantics is copy-on-write . That is, if two different variables represent the same runtime data object, then left indexing on one of these variables does not have any effects on the other one. This is achieved by transparently copying the data as necessary. Examples: A = ... ; # some matrix B = A ; # copy-by-reference B [ ... , ... ] = ... ; # copy-on-write: changes B, but no effect on A A [ ... , ... ] = ... ; # copy-on-write: changes A, but no effect on B","title":"Indexing"},{"location":"DaphneDSL/LanguageRef/#control-flow-statements","text":"DaphneDSL supports block statements, conditional branching, and various kinds of loops. These control flow constructs can be nested arbitrarily.","title":"Control Flow statements"},{"location":"DaphneDSL/LanguageRef/#block-statement","text":"A block statement allows to view an enclosed sequence of statements like a single statement. This is very useful in combination with the control flow statements described below. Besides that, a block statement starts a new scope in terms of visibility of variables. Within a block, all variables from outside the block can be read and written. However, variables created inside a block are not visible anymore after the block. The syntax of a block statement is: { statement1 statement2 ... } Examples: x = 1 ; { print ( x ); # read access x = 2 ; # write access y = 1 ; # variable created inside the block } print ( x ); # prints 2 print ( y ); # error","title":"Block statement"},{"location":"DaphneDSL/LanguageRef/#if-then-else","text":"The syntax of an if-then-else statement is as follows: if ( condition ) then - statement else else - statement condition is an expression returning a single value. If this value is true (when casted to value type bool , if necessary), the then-statement is executed. Otherwise, the else-statement is executed, if it is present . Note that the else -branch (keyword and statement) may be omitted. Furthermore, then-statement and else-statement can be block statements, to allow any number of statements in the then and else-branches. Examples: if ( sum ( X ) == 0 ) X = X + 1 ; if ( 2 * x > y ) { z = z / 2 ; a = true ; } else z = z * 2 ; if ( a ) print ( \"a\" ); else if ( b ) print ( \"not a, but b\" ); else print ( \"neither a nor b\" );","title":"If-then-else"},{"location":"DaphneDSL/LanguageRef/#loops","text":"DaphneDSL supports for-loops, while-loops, and do-while-loops. In the future we plan to support also parfor-loops as well as break and continue statements.","title":"Loops"},{"location":"DaphneDSL/LanguageRef/#for-loops","text":"For-loops are used to iterate over the elements of a sequence of integers. The syntax of a for-loop is as follows: for ( var in start : end [ : step ]) body - statement var must be a valid identifier and is assigned the values from start to end in increments of step . start , end , and step are expressions evaluating to a single number. step is optional and defaults to 1 if end is greater than start , or -1 otherwise. In that sense, for-loops can also be used to count backwards by setting start greater than end . The body-statement is executed for each value in the sequence, and within the body-statement , this value is accessible via the read-only variable var . Note that the body-statement may be a block statement enclosing an arbitrary number of statements. Examples: for ( i in 1 : 3 ) print ( i ); # 1 2 3 x = 0 ; y = 0 ; for ( i in 10 : 1 : -3 ) { x = x + i ; y = y + 1 ; } print ( x ); # 22 print ( y ); # 4","title":"For-Loops"},{"location":"DaphneDSL/LanguageRef/#while-loops","text":"While loops are used to execute a (block of) statement(s) as long as an arbitrary condition holds true. The syntax of a while-loop is as follows: while ( condition ) body - statement condition is an expression returning a single value, and is evaluated before each iteration. If this value is true (when casted to value type bool , if necessary), the body-statement is executed, and the loop starts anew. Otherwise, the program continues after the loop. Note that the body-statement may be a block statement enclosing an arbitrary number of statements. Examples: i = 0 ; while ( i < 10 && ! converged ) { A = A @ B ; converged = sum ( A ); i = i + 1 ; }","title":"While-Loops"},{"location":"DaphneDSL/LanguageRef/#do-while-loops","text":"Do-while-loops are a variant of while-loops, which checks the condition after each iteration. Consequently, a do-while-loop always executes at least one iteration. The syntax of a do-while-loop is as follows: do body - statement while ( condition ); The semicolon at the end is optional. Note that the body-statement may be a block statement enclosing an arbitrary number of statements. Examples: i = 5 ; do { A = sqrt ( A ); i = i - 1 ; } while ( mean ( A ) > 100 && i > 0 );","title":"Do-While-Loops"},{"location":"DaphneDSL/LanguageRef/#user-defined-functions-udfs","text":"DaphneDSL allows users to define their own functions. The syntax of a function definition looks as follows: def funcName ( paramName1 [: paramType1 ], paramName2 [: paramType2 ], ...) [ -> returnType1 , returnType2 , ...] { statement1 statement2 ... } The function name must be a valid and unique identifier. A function can have zero, one, or more parameters, and their names must be valid and unique identifiers. Furthermore, a function may return zero, one, or more values. The types of parameters are optional and can be provided or omitted for each parameter individually. The types of the return values are optional, but if omitted, exactly one return value is implicitly assumed. Functions with multiple return values must specify the types of all return values. See also typed and untyped functions below. The body of a function definition is always a block statement, i.e., it must be enclosed in curly braces {} even if it is just a single statement. So far, DaphneDSL supports only positional parameters to functions, but in the future, we plan to support named keyword arguments as well. Functions must be defined in the top-level scope of a DaphneDSL script, i.e., a function definition must not be nested within a control-flow statement or within another function definition.","title":"User-defined Functions (UDFs)"},{"location":"DaphneDSL/LanguageRef/#returning-values","text":"User-defined functions can return zero, one, or more (comma-separated) values using the return -statement. The number of returned values must match the function signature. Examples: return ; # don ' t return any values return x ; # return exactly one value return x , y ; # return two values Currently, the return statement must be the last statement of a function. Alternatively, it can be nested into if-then-else (early return), as long as it is ensured that there is exactly one return statement at the end of each path through the function (experimental). Examples: def fib ( n : si64 ) -> si64 { if ( n <= 0 ) return 0 ; if ( n <= 1 ) return 1 ; return fib ( n - 1 ) + fib ( n - 2 ); } def nextTwo(a: si64) -> si64, si64 { return a + 1, a + 2; }","title":"Returning Values"},{"location":"DaphneDSL/LanguageRef/#calling-user-defined-functions","text":"A user-defined function can be called like any other (built-in) function (see function-call expressions above). Examples: x = 2 * fib ( 5 ) + 123 ; y , z = nextTwo ( 123 );","title":"Calling User-defined Functions"},{"location":"DaphneDSL/LanguageRef/#typed-and-untyped-functions-experimental","text":"DaphneDSL supports both typed and untyped functions. The definition of a typed function specifies the data and value types of all parameters and return values. Hence, a typed function can only be called with inputs of the specified types (if a provided input has an unexpected type, it is automatically casted to the expected type, if possible), and always returns outputs of the specified types. A typed function is compiled exactly once and specialized to the specified parameter and return types. In contrast to that, the definition of an untyped function leaves the data and value type, or just the value type, of one or more parameters and/or return values unspecified. At call sites, a value of any type, or any value type, can be passed to an untyped parameter. As a consequence, an untyped function is compiled and specialized on demand according to the types at a call site. Consistently, the types of untyped return values are infered from the parameter types and operations.","title":"Typed and Untyped Functions (experimental)"},{"location":"DaphneDSL/LanguageRef/#example-scripts","text":"A few example DaphneDSL scripts can be found in: scripts/algorithms/ scripts/examples/ test/api/cli/algorithms/","title":"Example Scripts"},{"location":"DaphneLib/APIRef/","text":"API Reference This document is a hand-crafted reference of the DaphneLib API. A general introduction to DaphneLib (DAPHNE's Python API) can be found in a separate document. DaphneLib will offer numerous methods for obtaining DAPHNE matrices and frames as well as for building complex computations based on them. Ultimately, DaphneLib will support all DaphneDSL built-in functions on matrices and frames. Futhermore, we also plan to create a library of higher-level primitives allowing users to productively implement integrated data analysis pipelines at a much higher level of abstraction. At the moment, the documentation is still rather incomplete. However, as the methods largely map to DaphneDSL built-in functions, you can find some more information in the List of DaphneDSL built-in functions , for the time being. Obtaining DAPHNE Matrices and Frames DaphneContext Importing data from other Python libraries: from_numpy (mat: np.array, shared_memory=True) -> Matrix from_pandas (df: pd.DataFrame) -> Frame Generating data in DAPHNE: fill (arg, rows:int, cols:int) -> Matrix seq (start, end, inc) -> Matrix rand (rows: int, cols: int, min: Union[float, int] = None, max: Union[float, int] = None, sparsity: Union[float, int] = 0, seed: Union[float, int] = 0) -> Matrix Reading files using DAPHNE's readers: readMatrix (file:str) -> Matrix readFrame (file:str) -> Frame Building Complex Computations Complex computations can be built using Python operators (see DaphneLib ) and using DAPHNE matrix/frame/scalar methods. In the following, we describe only the latter. Matrix API Reference Data Generation: diagMatrix () Matrix dimensions: ncol () nrow () Elementwise unary: sqrt () Elementwise binary: max (other: 'Matrix') min (other: 'Matrix') Aggregation: sum (axis: int = None) aggMin (axis: int = None) aggMax (axis: int = None) mean (axis: int = None) stddev (axis: int = None) Reorganization: t () Other matrix operations: solve (other: 'Matrix') Input/output: print () write (file: str) Frame API Reference Frame dimensions: nrow () ncol () Reorganization: cbind (other) rbind (other) Extended relational algebra: cartesian (other) Input/output: print () write (file: str) Scalar API Reference Unary operations: sqrt () Input/output: print ()","title":"APIRef"},{"location":"DaphneLib/APIRef/#api-reference","text":"This document is a hand-crafted reference of the DaphneLib API. A general introduction to DaphneLib (DAPHNE's Python API) can be found in a separate document. DaphneLib will offer numerous methods for obtaining DAPHNE matrices and frames as well as for building complex computations based on them. Ultimately, DaphneLib will support all DaphneDSL built-in functions on matrices and frames. Futhermore, we also plan to create a library of higher-level primitives allowing users to productively implement integrated data analysis pipelines at a much higher level of abstraction. At the moment, the documentation is still rather incomplete. However, as the methods largely map to DaphneDSL built-in functions, you can find some more information in the List of DaphneDSL built-in functions , for the time being.","title":"API Reference"},{"location":"DaphneLib/APIRef/#obtaining-daphne-matrices-and-frames","text":"","title":"Obtaining DAPHNE Matrices and Frames"},{"location":"DaphneLib/APIRef/#daphnecontext","text":"Importing data from other Python libraries: from_numpy (mat: np.array, shared_memory=True) -> Matrix from_pandas (df: pd.DataFrame) -> Frame Generating data in DAPHNE: fill (arg, rows:int, cols:int) -> Matrix seq (start, end, inc) -> Matrix rand (rows: int, cols: int, min: Union[float, int] = None, max: Union[float, int] = None, sparsity: Union[float, int] = 0, seed: Union[float, int] = 0) -> Matrix Reading files using DAPHNE's readers: readMatrix (file:str) -> Matrix readFrame (file:str) -> Frame","title":"DaphneContext"},{"location":"DaphneLib/APIRef/#building-complex-computations","text":"Complex computations can be built using Python operators (see DaphneLib ) and using DAPHNE matrix/frame/scalar methods. In the following, we describe only the latter.","title":"Building Complex Computations"},{"location":"DaphneLib/APIRef/#matrix-api-reference","text":"Data Generation: diagMatrix () Matrix dimensions: ncol () nrow () Elementwise unary: sqrt () Elementwise binary: max (other: 'Matrix') min (other: 'Matrix') Aggregation: sum (axis: int = None) aggMin (axis: int = None) aggMax (axis: int = None) mean (axis: int = None) stddev (axis: int = None) Reorganization: t () Other matrix operations: solve (other: 'Matrix') Input/output: print () write (file: str)","title":"Matrix API Reference"},{"location":"DaphneLib/APIRef/#frame-api-reference","text":"Frame dimensions: nrow () ncol () Reorganization: cbind (other) rbind (other) Extended relational algebra: cartesian (other) Input/output: print () write (file: str)","title":"Frame API Reference"},{"location":"DaphneLib/APIRef/#scalar-api-reference","text":"Unary operations: sqrt () Input/output: print ()","title":"Scalar API Reference"},{"location":"DaphneLib/Overview/","text":"Overview: DAPHNE's Python API DaphneLib is a simple user-facing Python API that allows calling individual basic and higher-level DAPHNE built-in functions. The overall design follows similar abstractions like PySpark and Dask by using lazy evaluation. When the evaluation is triggered, DaphneLib assembles and executes a DaphneDSL script that uses the entire DAPHNE compilation and runtime stack, including all optimizations. Users can easily mix and match DAPHNE computations with other Python libraries and plotting functionality. DaphneLib is still in an experimental stage, feedback and bug reports via GitHub issues are highly welcome. Introductory Example The following simple example script generates a 5x3 matrix of random values in [0, 1) using numpy, imports the data to DAPHNE, and shifts and scales the data such that each column has a mean of 0 and a standard deviation of 1 . # (1) Import DaphneLib. from api.python.context.daphne_context import DaphneContext import numpy as np # (2) Create DaphneContext. dc = DaphneContext () # (3) Obtain a DAPHNE matrix. X = dc . from_numpy ( np . random . rand ( 5 , 3 )) # (4) Define calculations in DAPHNE. Y = ( X - X . mean ( axis = 1 )) / X . stddev ( axis = 1 ) # (5) Compute result in DAPHNE. print ( Y . compute ()) First, DAPHNE's Python library must be imported (1) . We plan to make this as simple as import daphne in the future. Then, a DaphneContext must be created (2) . The DaphneContext offers means to obtain DAPHNE matrices and frames, which serve as the starting point for defining complex computations. Here, we generate some random data in numpy and import it to DAPHNE (3) . Based on DAPHNE matrices/frames/scalars (and Python scalars), complex expressions can be defined (4) using Python operators (such as - and / above) and methods on the DAPHNE matrices/frames/scalars (such as mean() and stddev() above). The results of these expressions again represent DAPHNE matrices/frames/scalars. Up until here, no acutal computations are performed. Instead, an internal DAG (directed acyclic graph) representation of the computation is built. When calling compute() on the result (5) , the DAG is automatically optimized and executed by DAPHNE. This principle is known as lazy evaluation . (Internally, a DaphneDSL script is created, which is sent through the entire DAPHNE compiler and runtime stack, thereby profiting from all optimizations in DAPHNE.) The result is returned as a numpy.ndarray (for DAPHNE matrices), as a pandas.DataFrame (for DAPHNE frames), or as a plain Python scalar (for DAPHNE scalars), and can then be further used in Python. The script above can be executed by: python3 scripts/examples/daphnelib/shift-and-scale.py Note that there are some temporary limitations (which will be fixed in the future): python3 must be executed from the DAPHNE base directory. Before executing DaphneLib Python scripts, the environment variable PYTHONPATH must be updated by executing the following command once per session: export PYTHONPATH = \" $PYTHONPATH : $PWD /src/\" The remainder of this document presents the core features of DaphneLib as they are right now , but note that DaphneLib is still under active development . Data and Value Types DAPHNE differentiates data types and value types . Currently, DAPHNE supports the following abstract data types : matrix : homogeneous value type for all cells frame : a table with columns of potentially different value types scalar : a single value Value types specify the representation of individual values. We currently support: floating-point numbers of various widths: f64 , f32 signed and unsigned integers of various widths: si64 , si32 , si8 , ui64 , ui32 , ui8 strings str (currently only for scalars, support for matrix elements is still experimental) booleans bool (currently only for scalars) Data types and value types can be combined, e.g.: matrix<f64> is a matrix of double-precision floating point values In DaphneLib, each node of the computation DAG has one of the types api.python.operator.nodes.matrix.Matrix , api.python.operator.nodes.frame.Frame , or api.python.operator.nodes.scalar.Scalar . The type of a node determines which methods can be invoked on it (see DaphneLib API reference ). Obtaining DAPHNE Matrices and Frames The DaphneContext offers means to obtain DAPHNE matrices and frames, which serve as the starting point for defining complex computations. More precisely, DAPHNE matrices and frames can be obtained in the following ways: importing data from other Python libraries (e.g., numpy and pandas) generating data in DAPHNE (e.g., random data, constants, or sequences) reading files using DAPHNE's readers (e.g., CSV, Matrix Market, Parquet, DAPHNE binary format) A comprehensive list can be found in the DaphneLib API reference . Building Complex Computations Based on DAPHNE matrices/frames/scalars and Python scalars, complex expressions can be defined by Python operators DAPHNE matrix/frame/scalar methods The results of these expressions again represent DAPHNE matrices/frames/scalars. Python Operators DaphneLib currently supports the following binary operators on DAPHNE matrices/frames/scalars: Operator Meaning @ matrix multiplication * , / multiplication, division + , - addition/string concatenation, subtraction == , != , < , <= , > , >= comparison We plan to add more operators, including unary operators. Matrix multiplication ( @ ): The inputs must be matrices of compatible shapes, and the output is always a matrix. All other operators: The following table shows which combinations of inputs are allowed and which result they yield: Left input Right input Result Details scalar scalar scalar binary operation of two scalars matrix (n x m) scalar matrix (n x m) element-wise operation of each value with scalar scalar matrix (n x m) matrix (n x m) element-wise operation of scalar with each value (*) matrix (n x m) matrix (n x m) matrix (n x m) element-wise operation on corresponding values matrix (n x m) matrix (1 x m) matrix (n x m) broadcasting of row-vector matrix (n x m) matrix (n x 1) matrix (n x m) broadcasting of column-vector (*) Scalar- op -matrix operations are so far only supported for + , - , * , / ; for / only if the matrix is of a floating-point value type. In the future, we will fully support scalar- op -matrix operations as well as row/column-matrices as the left-hand-side operands. Examples: 1.5 * X @ y + 0.001 Matrix/Frame/Scalar Methods DaphneLib's classes Matrix , Frame , and Scalar offer a range of methods to call DAPHNE built-in functions. A comprehensive list can be found in the DaphneLib API reference . Examples: X.t () X.sqrt () X.cbind ( Y ) Data Exchange with other Python Libraries DaphneLib will support efficient data exchange with other well-known Python libraries, in both directions. The data transfer from other Python libraries to DaphneLib can be triggered through the from_...() methods of the DaphneContext (e.g., from_numpy() ). A comprehensive list of these methods can be found in the DaphneLib API reference . The data transfer from DaphneLib back to Python happens during the call to compute() . If the result of the computation in DAPHNE is a matrix, compute() returns a numpy.ndarray ; if the result is a frame, it returns a pandas.DataFrame ; and if the result is a scalar, it returns a plain Python scalar. So far, DaphneLib can exchange data with numpy (via shared memory) and pandas (via CSV files). Enabling data exchange with TensorFlow and PyTorch is on our agenda. Furthermore, we are working on making the data exchange more efficient in general. Data Exchange with numpy Example: from api.python.context.daphne_context import DaphneContext import numpy as np dc = DaphneContext () # Create data in numpy. a = np . arange ( 8.0 ) . reshape (( 2 , 4 )) # Transfer data to DaphneLib (lazily evaluated). X = dc . from_numpy ( a ) print ( \"How DAPHNE sees the data from numpy:\" ) X . print () . compute () # Add 100 to each value in X. X = X + 100.0 # Compute in DAPHNE, transfer result back to Python. print ( \" \\n Result of adding 100 to each value, back in Python:\" ) print ( X . compute ()) Run by: python3 scripts/examples/daphnelib/data-exchange-numpy.py Output: How DAPHNE sees the data from numpy: DenseMatrix(2x4, double) 0 1 2 3 4 5 6 7 Result of adding 100 to each value, back in Python: [[100. 101. 102. 103.] [104. 105. 106. 107.]] Data Exchange with pandas Example: from api.python.context.daphne_context import DaphneContext import pandas as pd dc = DaphneContext () # Create data in pandas. df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 1.1 , - 2.2 , 3.3 ]}) # Transfer data to DaphneLib (lazily evaluated). F = dc . from_pandas ( df ) print ( \"How DAPHNE sees the data from pandas:\" ) F . print () . compute () # Append F to itself. F = F . rbind ( F ) # Compute in DAPHNE, transfer result back to Python. print ( \" \\n Result of appending the frame to itself, back in Python:\" ) print ( F . compute ()) Run by: python3 scripts/examples/daphnelib/data-exchange-pandas.py Output: How DAPHNE sees the data from pandas: Frame(3x2, [a:int64_t, b:double]) 1 1.1 2 -2.2 3 3.3 Result of appending the frame to itself, back in Python: a b 0 2 -2.2 1 3 3.3 2 1 1.1 3 2 -2.2 4 3 3.3 Known Limitations DaphneLib is still in an early development stage. Thus, there are a few limitations that users should be aware of. We plan to fix all of these limitations in the future. import ing DaphneLib is still unnecessarily verbose. Using DAPHNE's command-line arguments to influence its behavior is not supported yet. Many DaphneDSL built-in functions are not represented by DaphneLib methods yet. Complex control flow (if-then-else, loops, functions) are not supported yet. Python control flow statements are of limited applicability for DaphneLib. High-level primitives for integrated data analysis pipelines, which are implemented in DaphneDSL, cannot be called from DaphneLib yet.","title":"Overview"},{"location":"DaphneLib/Overview/#overview-daphnes-python-api","text":"DaphneLib is a simple user-facing Python API that allows calling individual basic and higher-level DAPHNE built-in functions. The overall design follows similar abstractions like PySpark and Dask by using lazy evaluation. When the evaluation is triggered, DaphneLib assembles and executes a DaphneDSL script that uses the entire DAPHNE compilation and runtime stack, including all optimizations. Users can easily mix and match DAPHNE computations with other Python libraries and plotting functionality. DaphneLib is still in an experimental stage, feedback and bug reports via GitHub issues are highly welcome.","title":"Overview: DAPHNE's Python API"},{"location":"DaphneLib/Overview/#introductory-example","text":"The following simple example script generates a 5x3 matrix of random values in [0, 1) using numpy, imports the data to DAPHNE, and shifts and scales the data such that each column has a mean of 0 and a standard deviation of 1 . # (1) Import DaphneLib. from api.python.context.daphne_context import DaphneContext import numpy as np # (2) Create DaphneContext. dc = DaphneContext () # (3) Obtain a DAPHNE matrix. X = dc . from_numpy ( np . random . rand ( 5 , 3 )) # (4) Define calculations in DAPHNE. Y = ( X - X . mean ( axis = 1 )) / X . stddev ( axis = 1 ) # (5) Compute result in DAPHNE. print ( Y . compute ()) First, DAPHNE's Python library must be imported (1) . We plan to make this as simple as import daphne in the future. Then, a DaphneContext must be created (2) . The DaphneContext offers means to obtain DAPHNE matrices and frames, which serve as the starting point for defining complex computations. Here, we generate some random data in numpy and import it to DAPHNE (3) . Based on DAPHNE matrices/frames/scalars (and Python scalars), complex expressions can be defined (4) using Python operators (such as - and / above) and methods on the DAPHNE matrices/frames/scalars (such as mean() and stddev() above). The results of these expressions again represent DAPHNE matrices/frames/scalars. Up until here, no acutal computations are performed. Instead, an internal DAG (directed acyclic graph) representation of the computation is built. When calling compute() on the result (5) , the DAG is automatically optimized and executed by DAPHNE. This principle is known as lazy evaluation . (Internally, a DaphneDSL script is created, which is sent through the entire DAPHNE compiler and runtime stack, thereby profiting from all optimizations in DAPHNE.) The result is returned as a numpy.ndarray (for DAPHNE matrices), as a pandas.DataFrame (for DAPHNE frames), or as a plain Python scalar (for DAPHNE scalars), and can then be further used in Python. The script above can be executed by: python3 scripts/examples/daphnelib/shift-and-scale.py Note that there are some temporary limitations (which will be fixed in the future): python3 must be executed from the DAPHNE base directory. Before executing DaphneLib Python scripts, the environment variable PYTHONPATH must be updated by executing the following command once per session: export PYTHONPATH = \" $PYTHONPATH : $PWD /src/\" The remainder of this document presents the core features of DaphneLib as they are right now , but note that DaphneLib is still under active development .","title":"Introductory Example"},{"location":"DaphneLib/Overview/#data-and-value-types","text":"DAPHNE differentiates data types and value types . Currently, DAPHNE supports the following abstract data types : matrix : homogeneous value type for all cells frame : a table with columns of potentially different value types scalar : a single value Value types specify the representation of individual values. We currently support: floating-point numbers of various widths: f64 , f32 signed and unsigned integers of various widths: si64 , si32 , si8 , ui64 , ui32 , ui8 strings str (currently only for scalars, support for matrix elements is still experimental) booleans bool (currently only for scalars) Data types and value types can be combined, e.g.: matrix<f64> is a matrix of double-precision floating point values In DaphneLib, each node of the computation DAG has one of the types api.python.operator.nodes.matrix.Matrix , api.python.operator.nodes.frame.Frame , or api.python.operator.nodes.scalar.Scalar . The type of a node determines which methods can be invoked on it (see DaphneLib API reference ).","title":"Data and Value Types"},{"location":"DaphneLib/Overview/#obtaining-daphne-matrices-and-frames","text":"The DaphneContext offers means to obtain DAPHNE matrices and frames, which serve as the starting point for defining complex computations. More precisely, DAPHNE matrices and frames can be obtained in the following ways: importing data from other Python libraries (e.g., numpy and pandas) generating data in DAPHNE (e.g., random data, constants, or sequences) reading files using DAPHNE's readers (e.g., CSV, Matrix Market, Parquet, DAPHNE binary format) A comprehensive list can be found in the DaphneLib API reference .","title":"Obtaining DAPHNE Matrices and Frames"},{"location":"DaphneLib/Overview/#building-complex-computations","text":"Based on DAPHNE matrices/frames/scalars and Python scalars, complex expressions can be defined by Python operators DAPHNE matrix/frame/scalar methods The results of these expressions again represent DAPHNE matrices/frames/scalars.","title":"Building Complex Computations"},{"location":"DaphneLib/Overview/#python-operators","text":"DaphneLib currently supports the following binary operators on DAPHNE matrices/frames/scalars: Operator Meaning @ matrix multiplication * , / multiplication, division + , - addition/string concatenation, subtraction == , != , < , <= , > , >= comparison We plan to add more operators, including unary operators. Matrix multiplication ( @ ): The inputs must be matrices of compatible shapes, and the output is always a matrix. All other operators: The following table shows which combinations of inputs are allowed and which result they yield: Left input Right input Result Details scalar scalar scalar binary operation of two scalars matrix (n x m) scalar matrix (n x m) element-wise operation of each value with scalar scalar matrix (n x m) matrix (n x m) element-wise operation of scalar with each value (*) matrix (n x m) matrix (n x m) matrix (n x m) element-wise operation on corresponding values matrix (n x m) matrix (1 x m) matrix (n x m) broadcasting of row-vector matrix (n x m) matrix (n x 1) matrix (n x m) broadcasting of column-vector (*) Scalar- op -matrix operations are so far only supported for + , - , * , / ; for / only if the matrix is of a floating-point value type. In the future, we will fully support scalar- op -matrix operations as well as row/column-matrices as the left-hand-side operands. Examples: 1.5 * X @ y + 0.001","title":"Python Operators"},{"location":"DaphneLib/Overview/#matrixframescalar-methods","text":"DaphneLib's classes Matrix , Frame , and Scalar offer a range of methods to call DAPHNE built-in functions. A comprehensive list can be found in the DaphneLib API reference . Examples: X.t () X.sqrt () X.cbind ( Y )","title":"Matrix/Frame/Scalar Methods"},{"location":"DaphneLib/Overview/#data-exchange-with-other-python-libraries","text":"DaphneLib will support efficient data exchange with other well-known Python libraries, in both directions. The data transfer from other Python libraries to DaphneLib can be triggered through the from_...() methods of the DaphneContext (e.g., from_numpy() ). A comprehensive list of these methods can be found in the DaphneLib API reference . The data transfer from DaphneLib back to Python happens during the call to compute() . If the result of the computation in DAPHNE is a matrix, compute() returns a numpy.ndarray ; if the result is a frame, it returns a pandas.DataFrame ; and if the result is a scalar, it returns a plain Python scalar. So far, DaphneLib can exchange data with numpy (via shared memory) and pandas (via CSV files). Enabling data exchange with TensorFlow and PyTorch is on our agenda. Furthermore, we are working on making the data exchange more efficient in general.","title":"Data Exchange with other Python Libraries"},{"location":"DaphneLib/Overview/#data-exchange-with-numpy","text":"Example: from api.python.context.daphne_context import DaphneContext import numpy as np dc = DaphneContext () # Create data in numpy. a = np . arange ( 8.0 ) . reshape (( 2 , 4 )) # Transfer data to DaphneLib (lazily evaluated). X = dc . from_numpy ( a ) print ( \"How DAPHNE sees the data from numpy:\" ) X . print () . compute () # Add 100 to each value in X. X = X + 100.0 # Compute in DAPHNE, transfer result back to Python. print ( \" \\n Result of adding 100 to each value, back in Python:\" ) print ( X . compute ()) Run by: python3 scripts/examples/daphnelib/data-exchange-numpy.py Output: How DAPHNE sees the data from numpy: DenseMatrix(2x4, double) 0 1 2 3 4 5 6 7 Result of adding 100 to each value, back in Python: [[100. 101. 102. 103.] [104. 105. 106. 107.]]","title":"Data Exchange with numpy"},{"location":"DaphneLib/Overview/#data-exchange-with-pandas","text":"Example: from api.python.context.daphne_context import DaphneContext import pandas as pd dc = DaphneContext () # Create data in pandas. df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 ], \"b\" : [ 1.1 , - 2.2 , 3.3 ]}) # Transfer data to DaphneLib (lazily evaluated). F = dc . from_pandas ( df ) print ( \"How DAPHNE sees the data from pandas:\" ) F . print () . compute () # Append F to itself. F = F . rbind ( F ) # Compute in DAPHNE, transfer result back to Python. print ( \" \\n Result of appending the frame to itself, back in Python:\" ) print ( F . compute ()) Run by: python3 scripts/examples/daphnelib/data-exchange-pandas.py Output: How DAPHNE sees the data from pandas: Frame(3x2, [a:int64_t, b:double]) 1 1.1 2 -2.2 3 3.3 Result of appending the frame to itself, back in Python: a b 0 2 -2.2 1 3 3.3 2 1 1.1 3 2 -2.2 4 3 3.3","title":"Data Exchange with pandas"},{"location":"DaphneLib/Overview/#known-limitations","text":"DaphneLib is still in an early development stage. Thus, there are a few limitations that users should be aware of. We plan to fix all of these limitations in the future. import ing DaphneLib is still unnecessarily verbose. Using DAPHNE's command-line arguments to influence its behavior is not supported yet. Many DaphneDSL built-in functions are not represented by DaphneLib methods yet. Complex control flow (if-then-else, loops, functions) are not supported yet. Python control flow statements are of limited applicability for DaphneLib. High-level primitives for integrated data analysis pipelines, which are implemented in DaphneDSL, cannot be called from DaphneLib yet.","title":"Known Limitations"},{"location":"development/BuildingDaphne/","text":"Building DAPHNE The DAPHNE project provides a full-fledged build script. After cloning, it does everything from dependency setup to generation of the executable. What does the build script do? (simplified) Download & build all code dependencies Build Daphne Clean Project How long does a build take? The first run will take a while, due to long compilation times of the dependencies (~1 hour on a 16 vcore desktop, ~10 minutes on a 128 vcore cluster node). But they only have to be compiled once (except updates). Following builds only take a few seconds/minutes. Contents: Usage of the build script Extension of the build script Usage of the build script This section shows the possibilities of the build script. Build The default command to build the default target daphne . ./build.sh Print the cli build help page. This also shows all the following options. ./build.sh --help Build a specific target . ./build.sh --target \"target\" For example the following builds the main test target. ./build.sh --target \"run_tests\" Clean Clean all build directories, i.e., the daphne build dir <project_root>/build and the build output in <project_root>/bin and <project_root>/lib ./build.sh --clean Clean all downloads and extracted archive directories, i.e., <thirdparty_dir> /download-cache, <thirdparty_dir> /sources and <thirdparty_dir> /*.download.success files: ./build.sh --cleanCache Clean third party build output, i.e., <thirdparty_dir>/installed , <thirdparty_dir>/build and <thirdparty_dir> /*.install.success files: ./build.sh --cleanDeps Clean everything (DAPHNE build output and third party directory) ./build.sh --cleanAll Minimize Compile Times of Dependencies The most time-consuming part of getting DAPHNE compiled is building the third party dependencies. To avoid this, one can either use a prebuilt container image (in combination with some parameters to the build script see below) or at least build the dependencies once and subsequently point to the directory where the third party dependencies get installed. The bulid script must be invoked with the following two parameters to achieve this: ./build.sh --no-deps --installPrefix path/to/installed/deps If you have built DAPHNE and change the installPrefix directory , it is required to clean up and build again: ./build.sh --clean Options All possible options for the build script: Option Effect -h, --help Print the help page --installPrefix <path> Install third party dependencies in <path> (default: <project_root>/thirdparty/installed ) --clean Clean DAPHNE build output ( <project_root>/{bin,build,lib} ) --cleanCache Clean downloaded and extracted third party artifacts --cleanDeps Clean third party dependency build output and installed files --cleanAll Clean DAPHNE build output and reset the third party directory to the state in the git repo --target <target> Build specific target -nf, --no-fancy Disable colorized output --no-deps Avoid building third party dependencies -y, --yes Accept prompt (e.g., when executing the clean command) --cuda Compile with support for GPU operations using the CUDA SDK --debug Compile the daphne binary with debug symbols --oneapi Compile with support for accelerated operations using the OneAPI SDK --fpgaopencl Compile with support for FPGA operations using the Intel FPGA SDK or OneAPI+FPGA Add-On Extension Overview over the build script The build script is divided into sections, visualized by #****************************************************************************** # #1 Section name #****************************************************************************** Each section should only contain functionality related to the section name. The following list contains a rough overview over the sections and the concrete functions or functionality done here. Help message printHelp() // prints help message Build message helper daphne_msg( <message> ) // prints a status message in DAPHNE style printableTimestamp( <timestamp> ) // converts a unix epoch timestamp into a human readable string (e.g., 5min 20s 100ms) printLogo() // prints a DAPHNE logo to the console Clean build directories clean( <array ref dirs> <array ref files> ) // removes all given directories (1. parameter) and all given files (2. parameter) from disk cleanBuildDirs() // cleans build dirs (daphne and dependency build dirs) cleanAll() // cleans daphne build dir and wipes all dependencies from disk (resetting the third party directory) cleanDeps() // removes third party build output cleanCache() // removes downloaded third party artifacts (but leaving git submodules (only LLVM/MLIR at the time of writing) Create / Check Indicator-files dependency_install_success( <dep> ) // used after successful build of a dependency; creates related indicator file dependency_download_success(<dep>) // used after successful download of a dependency; creates related indicator file is_dependency_installed( <dep> ) // checks if dependency is already installed/built successfully is_dependency_downloaded( <dep> ) // checks if dependency is already downloaded successfully Versions of third party dependencies Versions of the software dependencies are configured here Set some prefixes, paths and dirs Definition of project related paths Configuration of path prefixes. For example all build directories are prefixed with buildPrefix . If fast storage is available on the system, build directories could be redirected with this central configuration. Parse arguments Parsing Updating git submodules Download and install third-party dependencies if requested (default is yes, omit with --no-deps)) Antlr catch2 OpenBLAS nlohmannjson abseil-cpp - Required by gRPC. Compiled separately to apply a patch. MPI (Default is MPI library is OpenMPI but cut can be any) gRPC Arrow / Parquet MLIR Build DAPHNE target Compilation of the DAPHNE-target ('daphne' is default) Adding a dependency If the dependency is fixed to a specific version, add it to the dependency versions section (section 5). Create a new segment in section 8 for the new dependency. Define needed dependency variables: Directory Name (which is used by the script to locate the dependency in different stages) Create an internal version variable in form of an array with two entries. Those are used for internal versioning and updating of the dependency without rebuilding each time. First: Name and version of the dependency as a string of the form <dep_name>_v${dep_version} (This one is updated, if a new version of the dependency is choosen.) Second: Thirdparty Version of the dependency as a string of the form v1 (This one is incremented each time by hand, if something changes on the path system of the dependency or DAPHNE itself. This way already existing projects are updated automatically, if something changes.) Optionals: Dep-specific paths, Dep-specific files, etc. Download the dependency, encased by: # in segment 5 <dep>_version = \"<dep_version>\" # in segment 8 # ---- # 8.x Your dependency # ---- <dep>_dirname = \"<dep_name>\" # 3.1 <dep>_version_internal =( \"<dep_name>_v ${ <dep>_version } \" \"v1\" ) # 3.2 <dep>... # 3.3 if ! is_dependency_downloaded \" ${ <dep>_version_internal[@] } \" ; then # do your stuff here dependency_download_success \" ${ <dep>_version_internal[@] } \" fi Hint: It is recommended to use the paths defined in section 6 for dependency downloads and installations. There are predefined paths like 'cacheDir', 'sourcePrefix', 'buildPrefix' and 'installPrefix'. Take a look at other dependencies to see how to use them. 1. Install the dependency (if necessary), encased by: if ! is_dependency_installed \" ${ <dep>_version_internal[@] } \" ; then # do your stuff here dependency_install_success \" ${ <dep>_version_internal[@] } \" fi Define a flag for the build script if your dependency is optional or poses unnecessary overhead for users (e.g., CUDA is optional as the CUDA SDK is a considerably sized package that only owners of Nvidia hardware would want to install). See section 7 about argument parsing. Quick guide: define a variable and its default value and add an item to the argument handling loop.","title":"BuildingDaphne"},{"location":"development/BuildingDaphne/#building-daphne","text":"The DAPHNE project provides a full-fledged build script. After cloning, it does everything from dependency setup to generation of the executable.","title":"Building DAPHNE"},{"location":"development/BuildingDaphne/#what-does-the-build-script-do-simplified","text":"Download & build all code dependencies Build Daphne Clean Project","title":"What does the build script do? (simplified)"},{"location":"development/BuildingDaphne/#how-long-does-a-build-take","text":"The first run will take a while, due to long compilation times of the dependencies (~1 hour on a 16 vcore desktop, ~10 minutes on a 128 vcore cluster node). But they only have to be compiled once (except updates). Following builds only take a few seconds/minutes. Contents: Usage of the build script Extension of the build script","title":"How long does a build take?"},{"location":"development/BuildingDaphne/#usage-of-the-build-script","text":"This section shows the possibilities of the build script.","title":"Usage of the build script"},{"location":"development/BuildingDaphne/#build","text":"The default command to build the default target daphne . ./build.sh Print the cli build help page. This also shows all the following options. ./build.sh --help Build a specific target . ./build.sh --target \"target\" For example the following builds the main test target. ./build.sh --target \"run_tests\"","title":"Build"},{"location":"development/BuildingDaphne/#clean","text":"Clean all build directories, i.e., the daphne build dir <project_root>/build and the build output in <project_root>/bin and <project_root>/lib ./build.sh --clean Clean all downloads and extracted archive directories, i.e., <thirdparty_dir> /download-cache, <thirdparty_dir> /sources and <thirdparty_dir> /*.download.success files: ./build.sh --cleanCache Clean third party build output, i.e., <thirdparty_dir>/installed , <thirdparty_dir>/build and <thirdparty_dir> /*.install.success files: ./build.sh --cleanDeps Clean everything (DAPHNE build output and third party directory) ./build.sh --cleanAll","title":"Clean"},{"location":"development/BuildingDaphne/#minimize-compile-times-of-dependencies","text":"The most time-consuming part of getting DAPHNE compiled is building the third party dependencies. To avoid this, one can either use a prebuilt container image (in combination with some parameters to the build script see below) or at least build the dependencies once and subsequently point to the directory where the third party dependencies get installed. The bulid script must be invoked with the following two parameters to achieve this: ./build.sh --no-deps --installPrefix path/to/installed/deps If you have built DAPHNE and change the installPrefix directory , it is required to clean up and build again: ./build.sh --clean","title":"Minimize Compile Times of Dependencies"},{"location":"development/BuildingDaphne/#options","text":"All possible options for the build script: Option Effect -h, --help Print the help page --installPrefix <path> Install third party dependencies in <path> (default: <project_root>/thirdparty/installed ) --clean Clean DAPHNE build output ( <project_root>/{bin,build,lib} ) --cleanCache Clean downloaded and extracted third party artifacts --cleanDeps Clean third party dependency build output and installed files --cleanAll Clean DAPHNE build output and reset the third party directory to the state in the git repo --target <target> Build specific target -nf, --no-fancy Disable colorized output --no-deps Avoid building third party dependencies -y, --yes Accept prompt (e.g., when executing the clean command) --cuda Compile with support for GPU operations using the CUDA SDK --debug Compile the daphne binary with debug symbols --oneapi Compile with support for accelerated operations using the OneAPI SDK --fpgaopencl Compile with support for FPGA operations using the Intel FPGA SDK or OneAPI+FPGA Add-On","title":"Options"},{"location":"development/BuildingDaphne/#extension","text":"","title":"Extension"},{"location":"development/BuildingDaphne/#overview-over-the-build-script","text":"The build script is divided into sections, visualized by #****************************************************************************** # #1 Section name #****************************************************************************** Each section should only contain functionality related to the section name. The following list contains a rough overview over the sections and the concrete functions or functionality done here. Help message printHelp() // prints help message Build message helper daphne_msg( <message> ) // prints a status message in DAPHNE style printableTimestamp( <timestamp> ) // converts a unix epoch timestamp into a human readable string (e.g., 5min 20s 100ms) printLogo() // prints a DAPHNE logo to the console Clean build directories clean( <array ref dirs> <array ref files> ) // removes all given directories (1. parameter) and all given files (2. parameter) from disk cleanBuildDirs() // cleans build dirs (daphne and dependency build dirs) cleanAll() // cleans daphne build dir and wipes all dependencies from disk (resetting the third party directory) cleanDeps() // removes third party build output cleanCache() // removes downloaded third party artifacts (but leaving git submodules (only LLVM/MLIR at the time of writing) Create / Check Indicator-files dependency_install_success( <dep> ) // used after successful build of a dependency; creates related indicator file dependency_download_success(<dep>) // used after successful download of a dependency; creates related indicator file is_dependency_installed( <dep> ) // checks if dependency is already installed/built successfully is_dependency_downloaded( <dep> ) // checks if dependency is already downloaded successfully Versions of third party dependencies Versions of the software dependencies are configured here Set some prefixes, paths and dirs Definition of project related paths Configuration of path prefixes. For example all build directories are prefixed with buildPrefix . If fast storage is available on the system, build directories could be redirected with this central configuration. Parse arguments Parsing Updating git submodules Download and install third-party dependencies if requested (default is yes, omit with --no-deps)) Antlr catch2 OpenBLAS nlohmannjson abseil-cpp - Required by gRPC. Compiled separately to apply a patch. MPI (Default is MPI library is OpenMPI but cut can be any) gRPC Arrow / Parquet MLIR Build DAPHNE target Compilation of the DAPHNE-target ('daphne' is default)","title":"Overview over the build script"},{"location":"development/BuildingDaphne/#adding-a-dependency","text":"If the dependency is fixed to a specific version, add it to the dependency versions section (section 5). Create a new segment in section 8 for the new dependency. Define needed dependency variables: Directory Name (which is used by the script to locate the dependency in different stages) Create an internal version variable in form of an array with two entries. Those are used for internal versioning and updating of the dependency without rebuilding each time. First: Name and version of the dependency as a string of the form <dep_name>_v${dep_version} (This one is updated, if a new version of the dependency is choosen.) Second: Thirdparty Version of the dependency as a string of the form v1 (This one is incremented each time by hand, if something changes on the path system of the dependency or DAPHNE itself. This way already existing projects are updated automatically, if something changes.) Optionals: Dep-specific paths, Dep-specific files, etc. Download the dependency, encased by: # in segment 5 <dep>_version = \"<dep_version>\" # in segment 8 # ---- # 8.x Your dependency # ---- <dep>_dirname = \"<dep_name>\" # 3.1 <dep>_version_internal =( \"<dep_name>_v ${ <dep>_version } \" \"v1\" ) # 3.2 <dep>... # 3.3 if ! is_dependency_downloaded \" ${ <dep>_version_internal[@] } \" ; then # do your stuff here dependency_download_success \" ${ <dep>_version_internal[@] } \" fi Hint: It is recommended to use the paths defined in section 6 for dependency downloads and installations. There are predefined paths like 'cacheDir', 'sourcePrefix', 'buildPrefix' and 'installPrefix'. Take a look at other dependencies to see how to use them. 1. Install the dependency (if necessary), encased by: if ! is_dependency_installed \" ${ <dep>_version_internal[@] } \" ; then # do your stuff here dependency_install_success \" ${ <dep>_version_internal[@] } \" fi Define a flag for the build script if your dependency is optional or poses unnecessary overhead for users (e.g., CUDA is optional as the CUDA SDK is a considerably sized package that only owners of Nvidia hardware would want to install). See section 7 about argument parsing. Quick guide: define a variable and its default value and add an item to the argument handling loop.","title":"Adding a dependency"},{"location":"development/Contributing/","text":"Contributing to the DAPHNE System Thank you for your interest in contributing to the DAPHNE system. Our goal is to build an open and inclusive community of developers around the system. Thus, contributions are highly welcome , both from within the DAPHNE project consortium and from external researchers/developers . In the following, you find some rough guidelines on contributing , which will most likely be extended and further clarified in the future. Ways of Contributing There are various ways of contributing including (but not limited to): - actual implementation - writing test cases - writing documentation - reporting bugs or any other kind of issue - contributing to discussions We encourage open communication about the system through comments on issues and pull requests directly on GitHub. That way, discussions are made accessible and transparent to everyone interested. This is important to involve people and to avoid repetition in case multiple people have the same question/comment or encounter the same problem. So feel free to create an issue to start a discussion on a particular topic (including these contribution guidelines) or to report a bug or other problem. Issue tracking All open/ongoing/completed work is tracked as issues on GitHub. These could be anything from precisely defined small tasks to requests for complex components. In any case, we will try to keep the book-keeping effort at a low level; at the same time, we should make each other aware of what everyone is working on to avoid duplicate work. If you would like to contribute and are looking for a task to work on, browse the list of issues . If you are a new contributor , you might want to watch out for \"good first issues\" . Furthermore, everyone is invited to create issues , e.g., for tasks you want to work on or problems you encountered. This is also a good way to enable discussion on the topic. Note that there is a set of labels that can be attached to your issue to clarify what it is about and to make it more easy to find. Before you start working on an issue, please make sure to get assigned to the issue . New contributors need to leave a comment on the issue before they can get assigned. Collaborators can assign themselves . Contributing to the Source Code We appreciate that different contributors can have different levels of familiarity with the code base, and try to adapt to that accordingly. New DAPHNE Contributors Contributions from new people are always welcome , both from within the DAPHNE project consortium and external! We are aware that contributing to a new code base can be challenging in the beginning. Thus, we want to keep the barrier of entry low for new contributors. That is, please try your best to make a good-quality contribution and we will help you with constructive feedback. The procedure is roughly as follows: Get assigned to the issue to let others know you are going to work on it and to avoid duplicate work. Please leave a comment on the issue stating that you are going to work on it. After that, a collaborator will formally assign you. Fork the repository on GitHub and clone your fork (see GitHub docs ). We recommend cloning by git clone --recursive https://github.com/<USERNAME>/daphne.git (note the --recursive ), as specified in Getting Started . You may skip this step and reuse your existing fork if you have contributed before. Simply update your fork with the recent changes from the original DAPHNE repository (see GitHub docs ). 3. Create your own local branch : git checkout -b BRANCH_NAME . BRANCH_NAME should clearly indicate what the branch is about; the recommended pattern is 123-some-short-title (where 123 is the issue number). 4. Add as many commits as you like to your branch, and git push them to your fork. Use git push --set-upstream origin BRANCH_NAME when you push the first time. 5. If you work longer on your contribution, make sure to get the most recent changes from the upstream (original DAPHNE system repository) from time to time (see GitHub docs ). 6. Once you feel ready (for integration or for discussion/feedback), create a pull request on GitHub (see GitHub docs ). Normally, you'll want to ask for integration into base:main , the repo's default branch. Please choose an expressive title and provide a short description of your changes. Feel free to mark your pull request \"WIP: \" or \"Draft: \" in the title. Note that you can add more commits to your pull request after you created it. 7. You receive feedback on your proposed contribution. You may be asked to apply certain changes, or we might apply straightforward adjustments ourselves before the integration. 8. If it looks good (potentially after some help), your contribution becomes a part of DAPHNE . Experienced DAPHNE Contributors (Collaborators) We appreciate continued commitment to the DAPHNE system. Thus, frequent contributors can become collaborators on GitHub. Currently, this requires at least three non-trivial contributions to the system. Collaborators have direct write access to all branches of the repository, including the main branch. The goal is to make development easier for frequent contributors . Collaborators do not need to create a fork, and do not need to go through pull requests to integrate their changes. At the same time, this freedom comes with certain responsibilities, which are roughly sketched here: Please follow some simple guidelines when changing the code : Feel free to directly push to the main branch, but be mindful of what you commit , since it will affect everyone. As a guideline, commits fundamentally changing how certain things work should be announced and discussed first, whereas small changes or changes local to \"your\" component are not critical. But never force push to the main branch , since it can lead to severe inconsistencies in the Git history. Even collaborators may still use pull requests (just like new contributors) to suggest larger changes. This is also suitable whenever you feel unsure about a change or want to get feedback first. Please engage in the handling of pull requests ; especially those affecting the components you are working on. This includes: reading the code others suggest for integration trying if it works providing constructive and actionable feedback on improving the contribution prior to the integration actually merging a pull request in Balancing the handling of pull requests is important to keep the development process scalable .","title":"Contributing"},{"location":"development/Contributing/#contributing-to-the-daphne-system","text":"Thank you for your interest in contributing to the DAPHNE system. Our goal is to build an open and inclusive community of developers around the system. Thus, contributions are highly welcome , both from within the DAPHNE project consortium and from external researchers/developers . In the following, you find some rough guidelines on contributing , which will most likely be extended and further clarified in the future.","title":"Contributing to the DAPHNE System"},{"location":"development/Contributing/#ways-of-contributing","text":"There are various ways of contributing including (but not limited to): - actual implementation - writing test cases - writing documentation - reporting bugs or any other kind of issue - contributing to discussions We encourage open communication about the system through comments on issues and pull requests directly on GitHub. That way, discussions are made accessible and transparent to everyone interested. This is important to involve people and to avoid repetition in case multiple people have the same question/comment or encounter the same problem. So feel free to create an issue to start a discussion on a particular topic (including these contribution guidelines) or to report a bug or other problem.","title":"Ways of Contributing"},{"location":"development/Contributing/#issue-tracking","text":"All open/ongoing/completed work is tracked as issues on GitHub. These could be anything from precisely defined small tasks to requests for complex components. In any case, we will try to keep the book-keeping effort at a low level; at the same time, we should make each other aware of what everyone is working on to avoid duplicate work. If you would like to contribute and are looking for a task to work on, browse the list of issues . If you are a new contributor , you might want to watch out for \"good first issues\" . Furthermore, everyone is invited to create issues , e.g., for tasks you want to work on or problems you encountered. This is also a good way to enable discussion on the topic. Note that there is a set of labels that can be attached to your issue to clarify what it is about and to make it more easy to find. Before you start working on an issue, please make sure to get assigned to the issue . New contributors need to leave a comment on the issue before they can get assigned. Collaborators can assign themselves .","title":"Issue tracking"},{"location":"development/Contributing/#contributing-to-the-source-code","text":"We appreciate that different contributors can have different levels of familiarity with the code base, and try to adapt to that accordingly.","title":"Contributing to the Source Code"},{"location":"development/Contributing/#new-daphne-contributors","text":"Contributions from new people are always welcome , both from within the DAPHNE project consortium and external! We are aware that contributing to a new code base can be challenging in the beginning. Thus, we want to keep the barrier of entry low for new contributors. That is, please try your best to make a good-quality contribution and we will help you with constructive feedback. The procedure is roughly as follows: Get assigned to the issue to let others know you are going to work on it and to avoid duplicate work. Please leave a comment on the issue stating that you are going to work on it. After that, a collaborator will formally assign you. Fork the repository on GitHub and clone your fork (see GitHub docs ). We recommend cloning by git clone --recursive https://github.com/<USERNAME>/daphne.git (note the --recursive ), as specified in Getting Started . You may skip this step and reuse your existing fork if you have contributed before. Simply update your fork with the recent changes from the original DAPHNE repository (see GitHub docs ). 3. Create your own local branch : git checkout -b BRANCH_NAME . BRANCH_NAME should clearly indicate what the branch is about; the recommended pattern is 123-some-short-title (where 123 is the issue number). 4. Add as many commits as you like to your branch, and git push them to your fork. Use git push --set-upstream origin BRANCH_NAME when you push the first time. 5. If you work longer on your contribution, make sure to get the most recent changes from the upstream (original DAPHNE system repository) from time to time (see GitHub docs ). 6. Once you feel ready (for integration or for discussion/feedback), create a pull request on GitHub (see GitHub docs ). Normally, you'll want to ask for integration into base:main , the repo's default branch. Please choose an expressive title and provide a short description of your changes. Feel free to mark your pull request \"WIP: \" or \"Draft: \" in the title. Note that you can add more commits to your pull request after you created it. 7. You receive feedback on your proposed contribution. You may be asked to apply certain changes, or we might apply straightforward adjustments ourselves before the integration. 8. If it looks good (potentially after some help), your contribution becomes a part of DAPHNE .","title":"New DAPHNE Contributors"},{"location":"development/Contributing/#experienced-daphne-contributors-collaborators","text":"We appreciate continued commitment to the DAPHNE system. Thus, frequent contributors can become collaborators on GitHub. Currently, this requires at least three non-trivial contributions to the system. Collaborators have direct write access to all branches of the repository, including the main branch. The goal is to make development easier for frequent contributors . Collaborators do not need to create a fork, and do not need to go through pull requests to integrate their changes. At the same time, this freedom comes with certain responsibilities, which are roughly sketched here: Please follow some simple guidelines when changing the code : Feel free to directly push to the main branch, but be mindful of what you commit , since it will affect everyone. As a guideline, commits fundamentally changing how certain things work should be announced and discussed first, whereas small changes or changes local to \"your\" component are not critical. But never force push to the main branch , since it can lead to severe inconsistencies in the Git history. Even collaborators may still use pull requests (just like new contributors) to suggest larger changes. This is also suitable whenever you feel unsure about a change or want to get feedback first. Please engage in the handling of pull requests ; especially those affecting the components you are working on. This includes: reading the code others suggest for integration trying if it works providing constructive and actionable feedback on improving the contribution prior to the integration actually merging a pull request in Balancing the handling of pull requests is important to keep the development process scalable .","title":"Experienced DAPHNE Contributors (Collaborators)"},{"location":"development/ExtendingDistributedRuntime/","text":"Extending the DAPHNE Distributed Runtime This doc is about implementing and extending the distributed runtime. This focuses on helping the Daphne developer understand the building blocks of the distributed runtime, both of the Daphne coordinator as well as the Daphne distributed worker. If you want to learn how to execute Daphne scripts on the distributed runtime, please see here . Background Distributed runtime works similar to the vectorized engine. Compiler passes create pipelines by merging multiple operations. This allows Daphne to split data across multiple workers (nodes) and let them execute a pipeline in parallel. Daphne distributed runtime code can be found at src/runtime/distributed where it is split into two main parts. The coordinator and the worker . Daphne distributed runtime works on an hierarchical approach. Workers are not aware of the total execution. They handle a single task-pipeline and return outputs, one pipeline at a time. Workers are only aware of their chunk of data. Coordinator The coordinator is responsible for distributing data and broadcasting the IR code fragment, that is the task-pipeline. Each worker receives and compiles the IR optimizing it for it's local hardware. Coordinator code can be found at: src/runtime/distributed/coordinator/kernels Distributed Wrapper DistributedWrapper.h kernel contains the Distributed wrapper class which is the main entry point for a distributed pipeline on the coordinator: void DistributedWrapper :: execute ( const char * mlirCode , // The IR code fragment DT *** res , // An array of pipeline outputs const Structure ** inputs , // Array of pipeline inputs size_t numInputs , // number of inputs size_t numOutputs , // number of outputs int64_t * outRows , // number of Rows for each pipeline output int64_t * outCols , // number of Columns for each pipeline output VectorSplit * splits , // Compiler hints on how each input should be split VectorCombine * combines ) // Compiler hints on how each output should be combined Using the hints splits provided by the compiler we determine whether an input should be Distributed/Scattered (by rows or columns) or Broadcasted . Depending on which we call the corresponding kernel ( Distribute.h or Broadcast.h , more below) which is then responsible for transferring data to the workers. DistributedCompute.h kernel is then called in order to broadcast the IR code fragment and start the actual computation. Finally, DistributedCollect.h kernel collects the final results (pipeline outputs). Distribute.h , Broadcast.h , DistributedCompute.h and DistributedCollect.h similar to local runtime kernels use C++ template meta programming (more on how we utilize C++ templates on the local runtime here ). Since Daphne supports many different distributed backends (e.g. gRPC, MPI, etc.) we can not provide a fully generic code that would work for all implementations. Thus we specialize each template for each distributed backend we want to support. The Daphne developer can work on a new distributed backend by simply providing a new template specialization of these 4 kernels with a different distributed backend. Distributed backends Daphne supports multiple different devices (GPUs, distributed nodes, FPGAs etc.). Because of that all Daphne objects (e.g. matrices) require tracking where their data is stored. That is why each Daphne object contains meta data providing that information. Below you can see a list of all the different devices a Daphne object can be allocated to. Please note that not all of them are supported yet and we might add more in the future. enum class ALLOCATION_TYPE { DIST_GRPC , DIST_OPENMPI , DIST_SPARK , GPU_CUDA , GPU_HIP , HOST , HOST_PINNED_CUDA , FPGA_INT , // Intel FPGA_XLX , // Xilinx ONEAPI , // probably need separate ones for CPU/GPU/FPGA NUM_ALLOC_TYPES }; As we described above, each kernel is partially specialized for each distributed backend. We specialize the templated distributed kernels for each distributed allocation type in the enum class shown above (e.g. DIST_GRPC ). Templated distributed kernels Each kernel is responsible for two things. Handling the communication part. That is sending data to the workers. Updating the meta data. That is populating the meta data of an object which can then lead to reduced communication (if an object is already placed on nodes, we don't need to re-send it). You can find more on meta data implementation here . Below we see Broadcast.h templated kernel, along with it's gRPC specialization. DT is the type of the object being broadcasted. // **************************************************************************** // Struct for partial template specialization // **************************************************************************** // DT is object type // AT is allocation type (distributed backend) template < ALLOCATION_TYPE AT , class DT > struct Broadcast { static void apply ( DT * mat , bool isScalar , DCTX ( dctx )) = delete ; }; // **************************************************************************** // Convenience function // **************************************************************************** template < ALLOCATION_TYPE AT , class DT > void broadcast ( DT *& mat , bool isScalar , DCTX ( dctx )) { Broadcast < AT , DT >:: apply ( mat , isScalar , dctx ); } // **************************************************************************** // (Partial) template specializations for different distributed backends // **************************************************************************** // ---------------------------------------------------------------------------- // GRPC // ---------------------------------------------------------------------------- template < class DT > struct Broadcast < ALLOCATION_TYPE :: DIST_GRPC , DT > { // Specific gRPC implementation... ... So for example if we wanted to add broadcast support for MPI all we need to do is provide the partial template specialization of Broadcast kernel for MPI. // ---------------------------------------------------------------------------- // MPI // ---------------------------------------------------------------------------- template < class DT > struct Broadcast < ALLOCATION_TYPE :: DIST_MPI , DT > { // Specific MPI implementation... ... For now selection of a distributed backend is hardcoded here . Distributed Worker Worker code can be found here: src/runtime/distributed/worker WorkerImpl.h contains the WorkerImpl class which provides all the logic for the distributed worker. There are 3 important methods in this class: The Store method, which stores an object in memory and returns an identifier. The Compute method, which receives the IR code fragment along with identifier of inputs, computes the pipeline and returns identifiers of pipeline outputs. And the Transfer method, which is used to return an object using an identifier. /** * @brief Stores a matrix at worker's memory * * @param mat Structure * obj to store * @return StoredInfo Information regarding stored object (identifier, numRows, numCols) */ StoredInfo Store ( DT * mat ) ; /** * @brief Computes a pipeline * * @param outputs vector to populate with results of the pipeline (identifier, numRows/cols, etc.) * @param inputs vector with inputs of pipeline (identifiers to use, etc.) * @param mlirCode mlir code fragment * @return WorkerImpl::Status tells us if everything went fine, with an optional error message */ Status Compute ( vector < StoredInfo > * outputs , vector < StoredInfo > inputs , string mlirCode ) ; /** * @brief Returns a matrix stored in worker's memory * * @param storedInfo Information regarding stored object (identifier, numRows, numCols) * @return Structure* Returns object */ Structure * Transfer ( StoredInfo storedInfo ); The developer can provide an implementation for a distributed worker by deriving WorkerImpl class. The derived class handles all the communication using the preferred distributed backend and invokes the parent methods for the logic. You can find the gRPC implementation of the distributed worker here: src/runtime/distributed/worker/WorkerImplGrpc.h/.cpp main.cpp is the entry point of the distributed worker. A distributed implementation is created using a pointer to the parent class WorkerImpl . The distributed node then blocks and waits for the coordinator to send a request by invoking the virtual method: virtual void Wait () { }; Each distributed worker implementation needs to override this method and implement it in order to wait for requests.","title":"ExtendingDistributedRuntime"},{"location":"development/ExtendingDistributedRuntime/#extending-the-daphne-distributed-runtime","text":"This doc is about implementing and extending the distributed runtime. This focuses on helping the Daphne developer understand the building blocks of the distributed runtime, both of the Daphne coordinator as well as the Daphne distributed worker. If you want to learn how to execute Daphne scripts on the distributed runtime, please see here .","title":"Extending the DAPHNE Distributed Runtime"},{"location":"development/ExtendingDistributedRuntime/#background","text":"Distributed runtime works similar to the vectorized engine. Compiler passes create pipelines by merging multiple operations. This allows Daphne to split data across multiple workers (nodes) and let them execute a pipeline in parallel. Daphne distributed runtime code can be found at src/runtime/distributed where it is split into two main parts. The coordinator and the worker . Daphne distributed runtime works on an hierarchical approach. Workers are not aware of the total execution. They handle a single task-pipeline and return outputs, one pipeline at a time. Workers are only aware of their chunk of data.","title":"Background"},{"location":"development/ExtendingDistributedRuntime/#coordinator","text":"The coordinator is responsible for distributing data and broadcasting the IR code fragment, that is the task-pipeline. Each worker receives and compiles the IR optimizing it for it's local hardware. Coordinator code can be found at: src/runtime/distributed/coordinator/kernels","title":"Coordinator"},{"location":"development/ExtendingDistributedRuntime/#distributed-wrapper","text":"DistributedWrapper.h kernel contains the Distributed wrapper class which is the main entry point for a distributed pipeline on the coordinator: void DistributedWrapper :: execute ( const char * mlirCode , // The IR code fragment DT *** res , // An array of pipeline outputs const Structure ** inputs , // Array of pipeline inputs size_t numInputs , // number of inputs size_t numOutputs , // number of outputs int64_t * outRows , // number of Rows for each pipeline output int64_t * outCols , // number of Columns for each pipeline output VectorSplit * splits , // Compiler hints on how each input should be split VectorCombine * combines ) // Compiler hints on how each output should be combined Using the hints splits provided by the compiler we determine whether an input should be Distributed/Scattered (by rows or columns) or Broadcasted . Depending on which we call the corresponding kernel ( Distribute.h or Broadcast.h , more below) which is then responsible for transferring data to the workers. DistributedCompute.h kernel is then called in order to broadcast the IR code fragment and start the actual computation. Finally, DistributedCollect.h kernel collects the final results (pipeline outputs). Distribute.h , Broadcast.h , DistributedCompute.h and DistributedCollect.h similar to local runtime kernels use C++ template meta programming (more on how we utilize C++ templates on the local runtime here ). Since Daphne supports many different distributed backends (e.g. gRPC, MPI, etc.) we can not provide a fully generic code that would work for all implementations. Thus we specialize each template for each distributed backend we want to support. The Daphne developer can work on a new distributed backend by simply providing a new template specialization of these 4 kernels with a different distributed backend.","title":"Distributed Wrapper"},{"location":"development/ExtendingDistributedRuntime/#distributed-backends","text":"Daphne supports multiple different devices (GPUs, distributed nodes, FPGAs etc.). Because of that all Daphne objects (e.g. matrices) require tracking where their data is stored. That is why each Daphne object contains meta data providing that information. Below you can see a list of all the different devices a Daphne object can be allocated to. Please note that not all of them are supported yet and we might add more in the future. enum class ALLOCATION_TYPE { DIST_GRPC , DIST_OPENMPI , DIST_SPARK , GPU_CUDA , GPU_HIP , HOST , HOST_PINNED_CUDA , FPGA_INT , // Intel FPGA_XLX , // Xilinx ONEAPI , // probably need separate ones for CPU/GPU/FPGA NUM_ALLOC_TYPES }; As we described above, each kernel is partially specialized for each distributed backend. We specialize the templated distributed kernels for each distributed allocation type in the enum class shown above (e.g. DIST_GRPC ).","title":"Distributed backends"},{"location":"development/ExtendingDistributedRuntime/#templated-distributed-kernels","text":"Each kernel is responsible for two things. Handling the communication part. That is sending data to the workers. Updating the meta data. That is populating the meta data of an object which can then lead to reduced communication (if an object is already placed on nodes, we don't need to re-send it). You can find more on meta data implementation here . Below we see Broadcast.h templated kernel, along with it's gRPC specialization. DT is the type of the object being broadcasted. // **************************************************************************** // Struct for partial template specialization // **************************************************************************** // DT is object type // AT is allocation type (distributed backend) template < ALLOCATION_TYPE AT , class DT > struct Broadcast { static void apply ( DT * mat , bool isScalar , DCTX ( dctx )) = delete ; }; // **************************************************************************** // Convenience function // **************************************************************************** template < ALLOCATION_TYPE AT , class DT > void broadcast ( DT *& mat , bool isScalar , DCTX ( dctx )) { Broadcast < AT , DT >:: apply ( mat , isScalar , dctx ); } // **************************************************************************** // (Partial) template specializations for different distributed backends // **************************************************************************** // ---------------------------------------------------------------------------- // GRPC // ---------------------------------------------------------------------------- template < class DT > struct Broadcast < ALLOCATION_TYPE :: DIST_GRPC , DT > { // Specific gRPC implementation... ... So for example if we wanted to add broadcast support for MPI all we need to do is provide the partial template specialization of Broadcast kernel for MPI. // ---------------------------------------------------------------------------- // MPI // ---------------------------------------------------------------------------- template < class DT > struct Broadcast < ALLOCATION_TYPE :: DIST_MPI , DT > { // Specific MPI implementation... ... For now selection of a distributed backend is hardcoded here .","title":"Templated distributed kernels"},{"location":"development/ExtendingDistributedRuntime/#distributed-worker","text":"Worker code can be found here: src/runtime/distributed/worker WorkerImpl.h contains the WorkerImpl class which provides all the logic for the distributed worker. There are 3 important methods in this class: The Store method, which stores an object in memory and returns an identifier. The Compute method, which receives the IR code fragment along with identifier of inputs, computes the pipeline and returns identifiers of pipeline outputs. And the Transfer method, which is used to return an object using an identifier. /** * @brief Stores a matrix at worker's memory * * @param mat Structure * obj to store * @return StoredInfo Information regarding stored object (identifier, numRows, numCols) */ StoredInfo Store ( DT * mat ) ; /** * @brief Computes a pipeline * * @param outputs vector to populate with results of the pipeline (identifier, numRows/cols, etc.) * @param inputs vector with inputs of pipeline (identifiers to use, etc.) * @param mlirCode mlir code fragment * @return WorkerImpl::Status tells us if everything went fine, with an optional error message */ Status Compute ( vector < StoredInfo > * outputs , vector < StoredInfo > inputs , string mlirCode ) ; /** * @brief Returns a matrix stored in worker's memory * * @param storedInfo Information regarding stored object (identifier, numRows, numCols) * @return Structure* Returns object */ Structure * Transfer ( StoredInfo storedInfo ); The developer can provide an implementation for a distributed worker by deriving WorkerImpl class. The derived class handles all the communication using the preferred distributed backend and invokes the parent methods for the logic. You can find the gRPC implementation of the distributed worker here: src/runtime/distributed/worker/WorkerImplGrpc.h/.cpp main.cpp is the entry point of the distributed worker. A distributed implementation is created using a pointer to the parent class WorkerImpl . The distributed node then blocks and waits for the coordinator to send a request by invoking the virtual method: virtual void Wait () { }; Each distributed worker implementation needs to override this method and implement it in order to wait for requests.","title":"Distributed Worker"},{"location":"development/ExtendingSchedulingKnobs/","text":"Extending DAPHNE with More Scheduling Knobs This document focuses on how a daphne developer may extend the DAPHNE system by adding new scheduling techniques Guidelines The daphne developer should consider the following files for adding a new scheduling technique src/runtime/local/vectorized/LoadPartitioning.h src/api/cli/daphne.cpp Adding the actual code of the technique: The first file LoadPartitioning.h contains the implementation of the currently supported scheduling techniques, i.e., the current version of DAPHNE uses self-scheduling techniques to partition the tasks. Also, it uses the self-scheduling principle for executing the tasks. For more details, please visit Scheduler design for tasks and pipelines . In this file, the developer should change two things: The enumeration that is called SelfSchedulingScheme . The developer will have to add a name for the new technique, e.g., MYTECH enum SelfSchedulingScheme { STATIC = 0 , SS , GSS , TSS , FAC2 , TFSS , FISS , VISS , PLS , MSTATIC , MFSC , PSS , MYTECH }; The function that is called getNextChunk() . This function has a switch case that selects the mathematical formula that corresponds to the chosen scheduling method. The developer has to add a new case to handle the new technique. uint64_t getNextChunk (){ //... switch ( schedulingMethod ){ //... //Only the following part is what the developer has to add. The rest remains the same case MYTECH :{ // the new technique chunkSize = FORMULA ; //Some Formula to calculate the chunksize (partition size) break ; } //... } //... return chunkSize ; } Enabling the selection of the newly added technique: The second file daphne.cpp contains the code that parses the command line arguments and passes them to the DAPHNE compiler and runtime. The developer has to add the new technique as a vaild option. Otherwise, the developer will not be able to use the newly added technique. There is a variable called taskPartitioningScheme and it is of type opt<SelfSchedulingScheme> . The developer should extend the declaration of opt<SelfSchedulingScheme> as follows: opt < SelfSchedulingScheme > taskPartitioningScheme ( cat ( daphneOptions ), desc ( \"Choose task partitioning scheme:\" ), values ( clEnumVal ( STATIC , \"Static (default)\" ), clEnumVal ( SS , \"Self-scheduling\" ), clEnumVal ( GSS , \"Guided self-scheduling\" ), clEnumVal ( TSS , \"Trapezoid self-scheduling\" ), clEnumVal ( FAC2 , \"Factoring self-scheduling\" ), clEnumVal ( TFSS , \"Trapezoid Factoring self-scheduling\" ), clEnumVal ( FISS , \"Fixed-increase self-scheduling\" ), clEnumVal ( VISS , \"Variable-increase self-scheduling\" ), clEnumVal ( PLS , \"Performance loop-based self-scheduling\" ), clEnumVal ( MSTATIC , \"Modified version of Static, i.e., instead of n/p, it uses n/(4*p) where n is number of tasks and p is number of threads\" ), clEnumVal ( MFSC , \"Modified version of fixed size chunk self-scheduling, i.e., MFSC does not require profiling information as FSC\" ), clEnumVal ( PSS , \"Probabilistic self-scheduling\" ), clEnumVal ( MYTECH , \"some meaningful description to the abbreviation of the new technique\" ) ) ); Usage of the new technique: Daphne developers may now pass the new technique as an option when they execute a DaphneDSL script. daphne --vec --MYTECH --grain-size 10 --num-threads 4 --PERCPU --SEQPRI --hyperthreading --debug-mt my_script.daphne In this example, the daphne system will execute my_script.daphne with the following configuration: the vectorized engine is enabled due to --vec the DAPHNE runtime will use MYTECH for task partitioning due to --MYTECH the minimum partition size will be 10 due to --grain-size 10 the vectorized engine will use 4 threads due to --num-threads 4 work stealing will be used with a separate queue for each CPU due to --PERCPU the work stealing victim selection will be sequential prioritized due to --SEQPRI the rows will be evenly distributed before the scheduling technique is applied due to --pre-partition the CPU workers will be pinned to CPU cores due to --pin-workers if the number of threads were not specified the number of logical CPU cores would be used (instead of physical CPU cores) due to --hyperthreading Debugging information related to the multithreading of vectorizable operations will be printed due to --debug-mt","title":"ExtendingSchedulingKnobs"},{"location":"development/ExtendingSchedulingKnobs/#extending-daphne-with-more-scheduling-knobs","text":"This document focuses on how a daphne developer may extend the DAPHNE system by adding new scheduling techniques","title":"Extending DAPHNE with More Scheduling Knobs"},{"location":"development/ExtendingSchedulingKnobs/#guidelines","text":"The daphne developer should consider the following files for adding a new scheduling technique src/runtime/local/vectorized/LoadPartitioning.h src/api/cli/daphne.cpp Adding the actual code of the technique: The first file LoadPartitioning.h contains the implementation of the currently supported scheduling techniques, i.e., the current version of DAPHNE uses self-scheduling techniques to partition the tasks. Also, it uses the self-scheduling principle for executing the tasks. For more details, please visit Scheduler design for tasks and pipelines . In this file, the developer should change two things: The enumeration that is called SelfSchedulingScheme . The developer will have to add a name for the new technique, e.g., MYTECH enum SelfSchedulingScheme { STATIC = 0 , SS , GSS , TSS , FAC2 , TFSS , FISS , VISS , PLS , MSTATIC , MFSC , PSS , MYTECH }; The function that is called getNextChunk() . This function has a switch case that selects the mathematical formula that corresponds to the chosen scheduling method. The developer has to add a new case to handle the new technique. uint64_t getNextChunk (){ //... switch ( schedulingMethod ){ //... //Only the following part is what the developer has to add. The rest remains the same case MYTECH :{ // the new technique chunkSize = FORMULA ; //Some Formula to calculate the chunksize (partition size) break ; } //... } //... return chunkSize ; } Enabling the selection of the newly added technique: The second file daphne.cpp contains the code that parses the command line arguments and passes them to the DAPHNE compiler and runtime. The developer has to add the new technique as a vaild option. Otherwise, the developer will not be able to use the newly added technique. There is a variable called taskPartitioningScheme and it is of type opt<SelfSchedulingScheme> . The developer should extend the declaration of opt<SelfSchedulingScheme> as follows: opt < SelfSchedulingScheme > taskPartitioningScheme ( cat ( daphneOptions ), desc ( \"Choose task partitioning scheme:\" ), values ( clEnumVal ( STATIC , \"Static (default)\" ), clEnumVal ( SS , \"Self-scheduling\" ), clEnumVal ( GSS , \"Guided self-scheduling\" ), clEnumVal ( TSS , \"Trapezoid self-scheduling\" ), clEnumVal ( FAC2 , \"Factoring self-scheduling\" ), clEnumVal ( TFSS , \"Trapezoid Factoring self-scheduling\" ), clEnumVal ( FISS , \"Fixed-increase self-scheduling\" ), clEnumVal ( VISS , \"Variable-increase self-scheduling\" ), clEnumVal ( PLS , \"Performance loop-based self-scheduling\" ), clEnumVal ( MSTATIC , \"Modified version of Static, i.e., instead of n/p, it uses n/(4*p) where n is number of tasks and p is number of threads\" ), clEnumVal ( MFSC , \"Modified version of fixed size chunk self-scheduling, i.e., MFSC does not require profiling information as FSC\" ), clEnumVal ( PSS , \"Probabilistic self-scheduling\" ), clEnumVal ( MYTECH , \"some meaningful description to the abbreviation of the new technique\" ) ) ); Usage of the new technique: Daphne developers may now pass the new technique as an option when they execute a DaphneDSL script. daphne --vec --MYTECH --grain-size 10 --num-threads 4 --PERCPU --SEQPRI --hyperthreading --debug-mt my_script.daphne In this example, the daphne system will execute my_script.daphne with the following configuration: the vectorized engine is enabled due to --vec the DAPHNE runtime will use MYTECH for task partitioning due to --MYTECH the minimum partition size will be 10 due to --grain-size 10 the vectorized engine will use 4 threads due to --num-threads 4 work stealing will be used with a separate queue for each CPU due to --PERCPU the work stealing victim selection will be sequential prioritized due to --SEQPRI the rows will be evenly distributed before the scheduling technique is applied due to --pre-partition the CPU workers will be pinned to CPU cores due to --pin-workers if the number of threads were not specified the number of logical CPU cores would be used (instead of physical CPU cores) due to --hyperthreading Debugging information related to the multithreading of vectorizable operations will be printed due to --debug-mt","title":"Guidelines"},{"location":"development/HandlingPRs/","text":"Pull Request (PR) Guideline Terminology Contributor : the person who wants to contribute code by opening a PR Collaborator : a person who has the right to merge a branch into main (besides other rights) (official collaborator status on GitHub) Reviewer : a person who provides feedback on the contribution Disclaimer These guidelines are mainly for DAPHNE collaborators . However, they could also be interesting for contributors to (a) understand how we handle PRs, and (b) learn which things we check, so they can try to prepare their contribution to speed up the review/merge procedure. Feel free to suggest changes to these guidelines by opening an issue or a pull request if you feel something is missing, could be improved, needs further clarification, etc. Goals of these Guidelines Merge useful contributions (not necessarily perfect ones) into main without unnecessary delay Guarantee a certain quality level , especially since many people are working with the main branch Balance the load for handling PRs among collaborators PR Review/Merging Procedure PR creation The contributor creates the PR. if the PR is marked as a draft, it is handled by an informal discussion depending on concrete questions by the contributor; if there are none, the PR is left alone for now if the PR is not marked as a draft, the review/merge procedure continues Initial response and reviewer assignment The DAPHNE collaborators provide an initial response and assign one (or multiple) reviewers (usually from among themselves, but can also be non-collaborators). Initial response ideally within a few working days after the PR was opened thank contributor for the contribution have a quick glance to decide if the contribution is relevant (default: yes) Reviewer selection any collaborator (or non-collaborator) may volunteer collaborators may select reviewer in a discussion (use @mentions) who qualifies as a reviewer collaborator experienced with the respective part of the code collaborator mentoring the contributor (e.g., in case of undergrad students) collaborator who wants to learn more about the respective part of the code base any other collaborator to balance the reviewing load among collaborators there may be multiple reviewers Assignment of the reviewer(s) on GitHub ideally, reviewer(s) should communicate when review can be expected (based on their availability and urgency of the PR) Rounds of feedback and response If necessary, the reviewer(s) and the contributor prepare the contribution for a merge by multiple (but ideally not more than one) rounds of feedback and response. Reviewer examines the contribution: read the code, look at the diff level of detail focus on integration into overall code base if really special topic, which reviewer is not familiar with, then no deep review possible/required especially for \"good first issues\": read code in detail be the stricter the more central the code is (the more people are affected by it) clarify relation of PR to issue if PR states to address an issue, check if it really does so it can be okay if a PR addresses just a part of a complex issue (if contribution still makes sense) briefly check if PR addresses further issues (if so, also mention that in feedback and commit message later) PR does not need to address an issue, but if it doesn't, check if contribution really belongs to the DAPHNE system itself (there might be useful contributions which should better reside in a separate repo, e.g., for the usage of DAPHNE, tools around DAPHNE, experiments/reproducibility, ...) contribution DOs readable code necessary API changes should be reflected in the documentation (e.g., DaphneDSL/DaphneLib, command line arguments, environment variables, ...) appropriate test cases (should be present and make sense, test expected cases, corner cases, and exceptional cases) comments/explanations in central pieces of the code meaningful placement of the contributed code in the directory hierarchy correct integration of additional third-party code (reasonable placement in directory hierarchy, license compatibility, ...) DAPHNE license header ( should be checked automatically ) ... contributions DON'Ts obvious bugs (also think of corner cases) changes unrelated to the PR (should be addressed in separate PRs) significant performance degradation (in terms of building as well as executing DAPHNE) ( such checks should be automated ) files that should not be committed, because they are not useful to others, too large, or can be generated from other files (e.g., IDE project files, output logs, executables, container images, empty files, unrelated files, experimental results, diagrams, unused files, auto-generated files, ...) unnecessary API changes (e.g., DaphneDSL/DaphneLib, command line arguments, possibly environment variables, ...) reimplementation of things we already have or that should better be imported from some third-party library breaking existing code, formatting, tests, documentation, etc. confidential information (usernames, passwords, ...) paths on local system of contributor misleading comments copy-paste errors extreme code duplication useless prints (might even fail test cases) whitespace changes that unnecessarily blow up the diff (especially in files that otherwise have no changes) ... code style don't be strict as long as we don't have a clearly defined code style which can be enforced automatically but watch out for things that make code hard to read, e.g. wrong indentation lots of commented out lines (especially artifacts from development/debugging) try out the code check out the branch If the contribution originates from a github fork, these steps will help to clone the PR's state into a branch of your working copy (example taken from PR #415): Make sure your local copy of the main branch is up to date git checkout main git pull Create a branch for the PR changes and pull them on top of that local branch git checkout -b akroviakov-415-densemat-strings-kernels main git pull git@github.com:akroviakov/daphne.git 415 -densemat-strings-kernels Once you have resolved all potential merge conflicts, you will have to do a merge commit. To get rid of this and ensure a linear history, start an interactive rebase from the last commit in main. In that process all non-relevant commits can be squashed and meaningful commit messages created if necessary. git rebase -i <commit hash of last commit in main> Once everything is cleaned up in the local PR branch, switch back to main and merge from the PR branch. This should yield clean commits on top of main because of the prior rebasing. git checkout main git merge akroviakov-415-densemat-strings-kernels git push origin main check if the code builds at all (should be checked automatically) check if there are compiler warnings (should be fixed) (should be checked automatically) check if the test cases pass (should be checked automatically) whether these checks succeed or fail may be platform-specific TODO: think about that aspect in more detail Reviewer fixes minor problems: things that are quicker to fix, than to communicate back and forth typos and grammar mistakes (in variable names, status/error messages, comments, ...) obvious minor bugs wording/terminology (especially in comments) add separate commit(s) on PR's branch to clearly separate these amendments from original contribution changes should be briefly mentioned/summarized in feedback to document that something was changed to notify contributor (ideally, they look at the changes in detail, learn from them, and do it better the next time) may be done at any point in time (before or after requested changes have been addressed by contributor) Reviewer provides feedback on the contribution: identify requests for concrete changes from contributor things that the reviewer cannot fix within a few minutes more general corrections, refactoring, ... more difficult bugs suitable for requesting mandatory changes in general, things that must to be done before the contribution can be merged, because there will be problems of some kind otherwise bugs (functional, non-functional/performance) things that could hinder others (e.g., unsolicited refactoring) simplifications that make the code dramatically shorter and/or easier to read/maintain, and are straightforward to achieve potentially also things that are in conflict with upcoming other PRs not suitable for requesting mandatory changes nice-to-have extensions of the feature: anything that could be done in a separate PR without leaving the code base in a bad state should not be a requirement for merging in at least a meaningful part of a feature the contribution of a PR does not need to be perfect, but it should bring us forward requests based on personal opinions which cannot be convincingly justified (e.g., implementing a feature in a different way as a matter of taste) (but might be okay for consistency) top efficiency such points can become follow-up issues and/or todos in the code (feel free to include issue number in todo) reviewer gives feedback by commenting on the PR use the form on GitHub (\"Files changed\"-tab -> \"Review changes\": select \"Approve\" or \"Request changes\") things to change should be enumerated clearly in the feedback on the PR (ideally numbered list or bullet points) briefly explain why these requested changes are necessary ideally provide some rough hints on how they could be addressed (but contributor is responsible for figuring out the details) optional extensions can be added as suggestions (some contributors are very eager), but clearly say that they are not required before merging feedback should be polite, actionable, concrete, and constructive Contributor addresses reviewer comments: ideally, the contributor is willing to do this otherwise (and especially for new contributors, for whom we want to lower the barrier of entry), the reviewer or someone else should take charge of this, if possible Once the contribution is ready, a collaborator merges the PR can be done by the reviewer or any collaborator we want to keep a clean history on the main branch (and remember never to force-push to main) makes it easier for others to keep track of the changes that happen PR's branch might have untidy history with lots of commits for implementing the contribution and addressing reviewer comments; that should not end up on main typically, we want to rebase the PR branch on main, which may require resolving conflicts an example of how to use git on the command line is given in try out the code in section 3.1 above case A) if PR is conceptually one contribution on GitHub: \"Conversation\"-tab: use \"Squash and merge\"-button (select this mode if necessary) on the command line: rebase and squash as required in the locally checked out PR branch force-push to the PR branch (but never force-push to main) locally switch to main and merge the PR branch push to main note: this procedure also ensures that the PR is shown as merged (not as closed ) in GitHub later this will place a single new commit onto main because you rebased/squashed in the PR branch first case B) if PR consists of individual meaningful commits of a larger feature (with meaningful commit messages) on GitHub: \"Conversation\"-tab: use \"Rebase and merge\"-button (select this mode if necessary) on the command line: rebase as required in the locally checked out PR branch force-push to the PR branch (but never force-push to main) locally switch to main and merge the PR branch push to main note: this procedure also ensures that the PR is shown as merged (not as closed ) in GitHub later this will place the new commits onto main because you rebased in the PR branch first in any case enter a meaningful commit message (what and why, closed issues, ...) ideally reuse the initial description of the PR in case of squashing (case A above): please remove the unnecessarily long generated commit message) TODO: commit messages should be a separate item in the developer documentation authorship if multiple authors edited the branch: choose one of them as the main author (after squashing in GitHub it should be the person who opened the PR); more authors can be added by adding Co-authored-by: NAME NAME@EXAMPLE.COM after two blank lines at the end of the commit message (one for each co-author). very often, reviewers may have made minor fixes, but should refrain from adding themselves as co-authors (prefer to give full credit for the contribution to the initial contributor, unless the reviewer's contribution was significant) Creation of follow-up issues (optional) things that were left out nice-to-haves functional, non-functional, documentation, tests Inviting the contributor as a collaborator (conditional) If this contributor has made enough non-trivial contributions of good quality (currently, we require three), he/she should be invited as a collaborator on GitHub. More Hints reviewer's time is precious, don't hesitate to request changes from the contributor (but keep in mind that we want to lower the barrier of entry for new contributors) avoid making a PR too large makes it difficult to context-switch into it again and again makes overall changes hard to understand and diffs hard to read whenever in doubt: use discussion features on GitHub to get others' opinions Communication We want to facilitate an open and inclusive atmosphere, which should be reflected in the way we communicate. TODO: we should set up concrete guidelines for that, but that's actually a separate topic always be polite and respectful to others keep the conversation constructive keep in mind the background of other persons experienced DAPHNE collaborator or new contributor level of technical experience English language skills ...","title":"HandlingPRs"},{"location":"development/HandlingPRs/#pull-request-pr-guideline","text":"","title":"Pull Request (PR) Guideline"},{"location":"development/HandlingPRs/#terminology","text":"Contributor : the person who wants to contribute code by opening a PR Collaborator : a person who has the right to merge a branch into main (besides other rights) (official collaborator status on GitHub) Reviewer : a person who provides feedback on the contribution","title":"Terminology"},{"location":"development/HandlingPRs/#disclaimer","text":"These guidelines are mainly for DAPHNE collaborators . However, they could also be interesting for contributors to (a) understand how we handle PRs, and (b) learn which things we check, so they can try to prepare their contribution to speed up the review/merge procedure. Feel free to suggest changes to these guidelines by opening an issue or a pull request if you feel something is missing, could be improved, needs further clarification, etc.","title":"Disclaimer"},{"location":"development/HandlingPRs/#goals-of-these-guidelines","text":"Merge useful contributions (not necessarily perfect ones) into main without unnecessary delay Guarantee a certain quality level , especially since many people are working with the main branch Balance the load for handling PRs among collaborators","title":"Goals of these Guidelines"},{"location":"development/HandlingPRs/#pr-reviewmerging-procedure","text":"","title":"PR Review/Merging Procedure"},{"location":"development/HandlingPRs/#pr-creation","text":"The contributor creates the PR. if the PR is marked as a draft, it is handled by an informal discussion depending on concrete questions by the contributor; if there are none, the PR is left alone for now if the PR is not marked as a draft, the review/merge procedure continues","title":"PR creation"},{"location":"development/HandlingPRs/#initial-response-and-reviewer-assignment","text":"The DAPHNE collaborators provide an initial response and assign one (or multiple) reviewers (usually from among themselves, but can also be non-collaborators). Initial response ideally within a few working days after the PR was opened thank contributor for the contribution have a quick glance to decide if the contribution is relevant (default: yes) Reviewer selection any collaborator (or non-collaborator) may volunteer collaborators may select reviewer in a discussion (use @mentions) who qualifies as a reviewer collaborator experienced with the respective part of the code collaborator mentoring the contributor (e.g., in case of undergrad students) collaborator who wants to learn more about the respective part of the code base any other collaborator to balance the reviewing load among collaborators there may be multiple reviewers Assignment of the reviewer(s) on GitHub ideally, reviewer(s) should communicate when review can be expected (based on their availability and urgency of the PR)","title":"Initial response and reviewer assignment"},{"location":"development/HandlingPRs/#rounds-of-feedback-and-response","text":"If necessary, the reviewer(s) and the contributor prepare the contribution for a merge by multiple (but ideally not more than one) rounds of feedback and response. Reviewer examines the contribution: read the code, look at the diff level of detail focus on integration into overall code base if really special topic, which reviewer is not familiar with, then no deep review possible/required especially for \"good first issues\": read code in detail be the stricter the more central the code is (the more people are affected by it) clarify relation of PR to issue if PR states to address an issue, check if it really does so it can be okay if a PR addresses just a part of a complex issue (if contribution still makes sense) briefly check if PR addresses further issues (if so, also mention that in feedback and commit message later) PR does not need to address an issue, but if it doesn't, check if contribution really belongs to the DAPHNE system itself (there might be useful contributions which should better reside in a separate repo, e.g., for the usage of DAPHNE, tools around DAPHNE, experiments/reproducibility, ...) contribution DOs readable code necessary API changes should be reflected in the documentation (e.g., DaphneDSL/DaphneLib, command line arguments, environment variables, ...) appropriate test cases (should be present and make sense, test expected cases, corner cases, and exceptional cases) comments/explanations in central pieces of the code meaningful placement of the contributed code in the directory hierarchy correct integration of additional third-party code (reasonable placement in directory hierarchy, license compatibility, ...) DAPHNE license header ( should be checked automatically ) ... contributions DON'Ts obvious bugs (also think of corner cases) changes unrelated to the PR (should be addressed in separate PRs) significant performance degradation (in terms of building as well as executing DAPHNE) ( such checks should be automated ) files that should not be committed, because they are not useful to others, too large, or can be generated from other files (e.g., IDE project files, output logs, executables, container images, empty files, unrelated files, experimental results, diagrams, unused files, auto-generated files, ...) unnecessary API changes (e.g., DaphneDSL/DaphneLib, command line arguments, possibly environment variables, ...) reimplementation of things we already have or that should better be imported from some third-party library breaking existing code, formatting, tests, documentation, etc. confidential information (usernames, passwords, ...) paths on local system of contributor misleading comments copy-paste errors extreme code duplication useless prints (might even fail test cases) whitespace changes that unnecessarily blow up the diff (especially in files that otherwise have no changes) ... code style don't be strict as long as we don't have a clearly defined code style which can be enforced automatically but watch out for things that make code hard to read, e.g. wrong indentation lots of commented out lines (especially artifacts from development/debugging) try out the code check out the branch If the contribution originates from a github fork, these steps will help to clone the PR's state into a branch of your working copy (example taken from PR #415): Make sure your local copy of the main branch is up to date git checkout main git pull Create a branch for the PR changes and pull them on top of that local branch git checkout -b akroviakov-415-densemat-strings-kernels main git pull git@github.com:akroviakov/daphne.git 415 -densemat-strings-kernels Once you have resolved all potential merge conflicts, you will have to do a merge commit. To get rid of this and ensure a linear history, start an interactive rebase from the last commit in main. In that process all non-relevant commits can be squashed and meaningful commit messages created if necessary. git rebase -i <commit hash of last commit in main> Once everything is cleaned up in the local PR branch, switch back to main and merge from the PR branch. This should yield clean commits on top of main because of the prior rebasing. git checkout main git merge akroviakov-415-densemat-strings-kernels git push origin main check if the code builds at all (should be checked automatically) check if there are compiler warnings (should be fixed) (should be checked automatically) check if the test cases pass (should be checked automatically) whether these checks succeed or fail may be platform-specific TODO: think about that aspect in more detail Reviewer fixes minor problems: things that are quicker to fix, than to communicate back and forth typos and grammar mistakes (in variable names, status/error messages, comments, ...) obvious minor bugs wording/terminology (especially in comments) add separate commit(s) on PR's branch to clearly separate these amendments from original contribution changes should be briefly mentioned/summarized in feedback to document that something was changed to notify contributor (ideally, they look at the changes in detail, learn from them, and do it better the next time) may be done at any point in time (before or after requested changes have been addressed by contributor) Reviewer provides feedback on the contribution: identify requests for concrete changes from contributor things that the reviewer cannot fix within a few minutes more general corrections, refactoring, ... more difficult bugs suitable for requesting mandatory changes in general, things that must to be done before the contribution can be merged, because there will be problems of some kind otherwise bugs (functional, non-functional/performance) things that could hinder others (e.g., unsolicited refactoring) simplifications that make the code dramatically shorter and/or easier to read/maintain, and are straightforward to achieve potentially also things that are in conflict with upcoming other PRs not suitable for requesting mandatory changes nice-to-have extensions of the feature: anything that could be done in a separate PR without leaving the code base in a bad state should not be a requirement for merging in at least a meaningful part of a feature the contribution of a PR does not need to be perfect, but it should bring us forward requests based on personal opinions which cannot be convincingly justified (e.g., implementing a feature in a different way as a matter of taste) (but might be okay for consistency) top efficiency such points can become follow-up issues and/or todos in the code (feel free to include issue number in todo) reviewer gives feedback by commenting on the PR use the form on GitHub (\"Files changed\"-tab -> \"Review changes\": select \"Approve\" or \"Request changes\") things to change should be enumerated clearly in the feedback on the PR (ideally numbered list or bullet points) briefly explain why these requested changes are necessary ideally provide some rough hints on how they could be addressed (but contributor is responsible for figuring out the details) optional extensions can be added as suggestions (some contributors are very eager), but clearly say that they are not required before merging feedback should be polite, actionable, concrete, and constructive Contributor addresses reviewer comments: ideally, the contributor is willing to do this otherwise (and especially for new contributors, for whom we want to lower the barrier of entry), the reviewer or someone else should take charge of this, if possible","title":"Rounds of feedback and response"},{"location":"development/HandlingPRs/#once-the-contribution-is-ready-a-collaborator-merges-the-pr","text":"can be done by the reviewer or any collaborator we want to keep a clean history on the main branch (and remember never to force-push to main) makes it easier for others to keep track of the changes that happen PR's branch might have untidy history with lots of commits for implementing the contribution and addressing reviewer comments; that should not end up on main typically, we want to rebase the PR branch on main, which may require resolving conflicts an example of how to use git on the command line is given in try out the code in section 3.1 above case A) if PR is conceptually one contribution on GitHub: \"Conversation\"-tab: use \"Squash and merge\"-button (select this mode if necessary) on the command line: rebase and squash as required in the locally checked out PR branch force-push to the PR branch (but never force-push to main) locally switch to main and merge the PR branch push to main note: this procedure also ensures that the PR is shown as merged (not as closed ) in GitHub later this will place a single new commit onto main because you rebased/squashed in the PR branch first case B) if PR consists of individual meaningful commits of a larger feature (with meaningful commit messages) on GitHub: \"Conversation\"-tab: use \"Rebase and merge\"-button (select this mode if necessary) on the command line: rebase as required in the locally checked out PR branch force-push to the PR branch (but never force-push to main) locally switch to main and merge the PR branch push to main note: this procedure also ensures that the PR is shown as merged (not as closed ) in GitHub later this will place the new commits onto main because you rebased in the PR branch first in any case enter a meaningful commit message (what and why, closed issues, ...) ideally reuse the initial description of the PR in case of squashing (case A above): please remove the unnecessarily long generated commit message) TODO: commit messages should be a separate item in the developer documentation authorship if multiple authors edited the branch: choose one of them as the main author (after squashing in GitHub it should be the person who opened the PR); more authors can be added by adding Co-authored-by: NAME NAME@EXAMPLE.COM after two blank lines at the end of the commit message (one for each co-author). very often, reviewers may have made minor fixes, but should refrain from adding themselves as co-authors (prefer to give full credit for the contribution to the initial contributor, unless the reviewer's contribution was significant)","title":"Once the contribution is ready, a collaborator merges the PR"},{"location":"development/HandlingPRs/#creation-of-follow-up-issues-optional","text":"things that were left out nice-to-haves functional, non-functional, documentation, tests","title":"Creation of follow-up issues (optional)"},{"location":"development/HandlingPRs/#inviting-the-contributor-as-a-collaborator-conditional","text":"If this contributor has made enough non-trivial contributions of good quality (currently, we require three), he/she should be invited as a collaborator on GitHub.","title":"Inviting the contributor as a collaborator (conditional)"},{"location":"development/HandlingPRs/#more-hints","text":"reviewer's time is precious, don't hesitate to request changes from the contributor (but keep in mind that we want to lower the barrier of entry for new contributors) avoid making a PR too large makes it difficult to context-switch into it again and again makes overall changes hard to understand and diffs hard to read whenever in doubt: use discussion features on GitHub to get others' opinions","title":"More Hints"},{"location":"development/HandlingPRs/#communication","text":"We want to facilitate an open and inclusive atmosphere, which should be reflected in the way we communicate. TODO: we should set up concrete guidelines for that, but that's actually a separate topic always be polite and respectful to others keep the conversation constructive keep in mind the background of other persons experienced DAPHNE collaborator or new contributor level of technical experience English language skills ...","title":"Communication"},{"location":"development/ImplementBuiltinKernel/","text":"Implementing a Built-in Kernel for a DaphneIR Operation Background (Almost) every DaphneIR operation will be backed by a kernel (= physical operator) at run-time. Extensibility w.r.t. kernels is one on the core goals of the DAPHNE system. It shall be easy for a user to add a custom kernel. However, the system will offer a full set of built-in kernels so that all DaphneIR operations can be used out-of-the-box. Scope This document focuses on: default built-in kernels (not custom/external kernels) implementations for CPU (not HW accelerators) local execution (not distributed) pre-compiled kernels (not on-the-fly code generation and operator fusion) We will extend the system to those more advanced aspects step by step. Guidelines The following are some guidelines and best practices (rather than strict rules) for implementing built-in kernels. As we proceed, we might adapt these guidelines step by step. The goal is to clarify how to implement a built-in kernel and what the thoughts behind it are. This is meant as a proposal, comments/suggestions are always welcome . Integration into the directory tree: The implementations of built-in kernels shall reside in src/runtime/local/kernels . By default, one C++ header-file should be used for all specializations of a kernel. Depending on the amount of code, the separation into multiple header files is also possible. At least, we should rather not mix kernels of different DaphneIR operations in one header file. Interfaces: Technically, a kernel is a C++ function taking one or more data objects (matrices, frames) and/or scalars as input and returning one or more data objects and/or scalars as output. As a central idea, (almost) all DaphneIR operations should be able to process (almost) all kinds of Daphne data structures, whereby these could have any Daphne value type. For example, elementwise binary operations ( + , * , ...) should be applicable to DenseMatrix of double , uint32_t , etc. as well as to CSRMatrix of double , uint32_t , etc. and so on. This type flexibility motivates the use of C++ template metaprogramming. Thus, a kernel is a template function with: template parameters one for the type of each input/output data object (and scalar, if necessary) inputs of type const DT * for data objects (whereby DT is a particular C++ type such as DenseMatrix<double> ) of type VT for scalars of some value type VT outputs as a return value, in case of a single scalar as a parameter of type DT *& in case of data objects (all output parameters before all input parameters) The reason for passing output data objects as parameters is that this mechanism could be used for efficient update-in-place operations. The declaration of a kernel function could look as follows: template < class DTRes , class DTArg , typename VT > void someOp ( DTRes *& res , const DTArg * arg , VT otherArg ); For most DaphneIR operations, it will not be possible to define the template as a fully generic algorithm. For example, the algorithms for processing different data type implementations (like DenseMatrix and CSRMatrix ) will usually differ significantly. Thus, we will usually want to specialize the template. At the same time, it will be possible to implement algorithms generically w.r.t. the value type . Thus, we will usually not need to specialize for that. Hence, we will often use partial template specialization. Unfortunately, this is not possible for functions in C++. Therefore, we use a very typical workaround: In addition to the kernel template function, we declare a template struct/class with the same template parameters as the kernel function. By convention, let us call this struct like the kernel function, but starting with upper case letter. This struct/class has a single static member function with the same results and parameters as the kernel function. By convention, let us always call this function apply . Then, the kernel function always forwards the processing to the apply -function of the correct template instantiation of the kernel struct. Instead of partially specializing the kernel function, we partially specialize the kernel struct and implement the respective algorithm in that specialization's apply -function. Finally, callers will call an instantiation of the kernel template function. C++ templates offer many ways to express such (partial) specializations, some examples are given below: // Kernel struct to enable partial template specialization. template < class DTRes , class DTArg , typename VT > struct SomeOp { static void apply ( DTRes *& res , const DTArg * arg , VT otherArg ) = delete ; }; // Kernel function to be called from outside. template < class DTRes , class DTArg , typename VT > void someOp ( DTRes *& res , const DTArg * arg , VT otherArg ) { SomeOp < DTRes , DTArg , VT >:: apply ( res , arg , otherArg ); } // (Partial) specializations of the kernel struct (some examples). // E.g. for DenseMatrix of any value type. template < typename VT > struct SomeOp < DenseMatrix < VT > , DenseMatrix < VT > , VT > { static void apply ( DenseMatrix < VT > *& res , const DenseMatrix < VT > * arg , VT otherArg ) { // do something } }; // E.g. for DenseMatrix and CSRMatrix of the same value type. template < typename VT > struct SomeOp < DenseMatrix < VT > , CSRMatrix < VT > , VT > { static void apply ( DenseMatrix < VT > *& res , const CSRMatrix < VT > * arg , VT otherArg ) { // do something } }; // E.g. for DenseMatrix of independent value types. template < typename VTRes , typename VTArg , typename VTOtherArg > struct SomeOp < DenseMatrix < VTRes > , DenseMatrix < VTArg > , VTOtherArg > { static void apply ( DenseMatrix < VTRes > *& res , const DenseMatrix < VTArg > * arg , VTOtherArg otherArg ) { // do something } }; // E.g. for super-class Matrix of the same value type. template < typename VT > struct SomeOp < Matrix < VT > , Matrix < VT > , VT > { static void apply ( Matrix < VT > *& res , const Matrix < VT > * arg , VT otherArg ) { // do something } }; // E.g. for DenseMatrix<double>, CSRMatrix<float>, and double (full specialization). template <> struct SomeOp < DenseMatrix < double > , CSRMatrix < float > , double > { static void apply ( DenseMatrix < double > *& res , const CSRMatrix < float > * arg , double otherArg ) { // do something } }; Implementation of the apply -functions: As stated above, the apply -function contain the actual implementation of the kernel. Of course, that depends on what the kernel is supposed to do, but there some recurring actions. Obtaining an output data object Data objects like matrices and frames cannot be obtained using the new -operator, but must be obtained from the DataObjectFactory , e.g., as follows: auto res = DataObjectFactory :: create < CSRMatrix < double >> ( 3 , 4 , 6 , false ); Internally, this create -function calls a private constructor of the specified data type implementation; so please have a look at these. - Accessing the input and output data objects For efficiency reasons, accessing the data in way specific to the data type implementation is preferred to generic access method of the super-classes. That is, if possible, rather use getValues() (for DenseMatrix ) or getValues() / colColIdxs() / getRowOffsets() (for CSRMatrix ) rather than get() / set() / append() from the super-class Matrix . The reason is that these generic access methods can incur a lot of unnecessary effort, depending on the data type implementation. However, in the end it is always a trade-off between performance and code complexity. For kernels that are rarely used or typically used on small data objects, a simple but inefficient implementation might be okay. Nevertheless, since the DAPHNE system should be able to handle unexpected scripts efficiently, we should not get too much used to sacrificing efficiency. Concrete Examples For concrete examples, please have a look at existing kernel implementations in src/runtime/local/kernels . For instance, the following kernels represent some interesting cases: ewBinarySca works only on scalars. ewBinaryMat works only on matrices. ewBinaryObjSca combines matrix/frame and scalar inputs. matMul delegates to an external library (OpenBLAS). Test Cases Implementing test cases for each kernel is important to reduce the likelihood of bugs, now and after changes to the code base. Please have a look at test cases for existing kernel implementations in test/runtime/local/kernels (surely, these could still be improved).","title":"ImplementBuiltinKernel"},{"location":"development/ImplementBuiltinKernel/#implementing-a-built-in-kernel-for-a-daphneir-operation","text":"","title":"Implementing a Built-in Kernel for a DaphneIR Operation"},{"location":"development/ImplementBuiltinKernel/#background","text":"(Almost) every DaphneIR operation will be backed by a kernel (= physical operator) at run-time. Extensibility w.r.t. kernels is one on the core goals of the DAPHNE system. It shall be easy for a user to add a custom kernel. However, the system will offer a full set of built-in kernels so that all DaphneIR operations can be used out-of-the-box.","title":"Background"},{"location":"development/ImplementBuiltinKernel/#scope","text":"This document focuses on: default built-in kernels (not custom/external kernels) implementations for CPU (not HW accelerators) local execution (not distributed) pre-compiled kernels (not on-the-fly code generation and operator fusion) We will extend the system to those more advanced aspects step by step.","title":"Scope"},{"location":"development/ImplementBuiltinKernel/#guidelines","text":"The following are some guidelines and best practices (rather than strict rules) for implementing built-in kernels. As we proceed, we might adapt these guidelines step by step. The goal is to clarify how to implement a built-in kernel and what the thoughts behind it are. This is meant as a proposal, comments/suggestions are always welcome . Integration into the directory tree: The implementations of built-in kernels shall reside in src/runtime/local/kernels . By default, one C++ header-file should be used for all specializations of a kernel. Depending on the amount of code, the separation into multiple header files is also possible. At least, we should rather not mix kernels of different DaphneIR operations in one header file. Interfaces: Technically, a kernel is a C++ function taking one or more data objects (matrices, frames) and/or scalars as input and returning one or more data objects and/or scalars as output. As a central idea, (almost) all DaphneIR operations should be able to process (almost) all kinds of Daphne data structures, whereby these could have any Daphne value type. For example, elementwise binary operations ( + , * , ...) should be applicable to DenseMatrix of double , uint32_t , etc. as well as to CSRMatrix of double , uint32_t , etc. and so on. This type flexibility motivates the use of C++ template metaprogramming. Thus, a kernel is a template function with: template parameters one for the type of each input/output data object (and scalar, if necessary) inputs of type const DT * for data objects (whereby DT is a particular C++ type such as DenseMatrix<double> ) of type VT for scalars of some value type VT outputs as a return value, in case of a single scalar as a parameter of type DT *& in case of data objects (all output parameters before all input parameters) The reason for passing output data objects as parameters is that this mechanism could be used for efficient update-in-place operations. The declaration of a kernel function could look as follows: template < class DTRes , class DTArg , typename VT > void someOp ( DTRes *& res , const DTArg * arg , VT otherArg ); For most DaphneIR operations, it will not be possible to define the template as a fully generic algorithm. For example, the algorithms for processing different data type implementations (like DenseMatrix and CSRMatrix ) will usually differ significantly. Thus, we will usually want to specialize the template. At the same time, it will be possible to implement algorithms generically w.r.t. the value type . Thus, we will usually not need to specialize for that. Hence, we will often use partial template specialization. Unfortunately, this is not possible for functions in C++. Therefore, we use a very typical workaround: In addition to the kernel template function, we declare a template struct/class with the same template parameters as the kernel function. By convention, let us call this struct like the kernel function, but starting with upper case letter. This struct/class has a single static member function with the same results and parameters as the kernel function. By convention, let us always call this function apply . Then, the kernel function always forwards the processing to the apply -function of the correct template instantiation of the kernel struct. Instead of partially specializing the kernel function, we partially specialize the kernel struct and implement the respective algorithm in that specialization's apply -function. Finally, callers will call an instantiation of the kernel template function. C++ templates offer many ways to express such (partial) specializations, some examples are given below: // Kernel struct to enable partial template specialization. template < class DTRes , class DTArg , typename VT > struct SomeOp { static void apply ( DTRes *& res , const DTArg * arg , VT otherArg ) = delete ; }; // Kernel function to be called from outside. template < class DTRes , class DTArg , typename VT > void someOp ( DTRes *& res , const DTArg * arg , VT otherArg ) { SomeOp < DTRes , DTArg , VT >:: apply ( res , arg , otherArg ); } // (Partial) specializations of the kernel struct (some examples). // E.g. for DenseMatrix of any value type. template < typename VT > struct SomeOp < DenseMatrix < VT > , DenseMatrix < VT > , VT > { static void apply ( DenseMatrix < VT > *& res , const DenseMatrix < VT > * arg , VT otherArg ) { // do something } }; // E.g. for DenseMatrix and CSRMatrix of the same value type. template < typename VT > struct SomeOp < DenseMatrix < VT > , CSRMatrix < VT > , VT > { static void apply ( DenseMatrix < VT > *& res , const CSRMatrix < VT > * arg , VT otherArg ) { // do something } }; // E.g. for DenseMatrix of independent value types. template < typename VTRes , typename VTArg , typename VTOtherArg > struct SomeOp < DenseMatrix < VTRes > , DenseMatrix < VTArg > , VTOtherArg > { static void apply ( DenseMatrix < VTRes > *& res , const DenseMatrix < VTArg > * arg , VTOtherArg otherArg ) { // do something } }; // E.g. for super-class Matrix of the same value type. template < typename VT > struct SomeOp < Matrix < VT > , Matrix < VT > , VT > { static void apply ( Matrix < VT > *& res , const Matrix < VT > * arg , VT otherArg ) { // do something } }; // E.g. for DenseMatrix<double>, CSRMatrix<float>, and double (full specialization). template <> struct SomeOp < DenseMatrix < double > , CSRMatrix < float > , double > { static void apply ( DenseMatrix < double > *& res , const CSRMatrix < float > * arg , double otherArg ) { // do something } }; Implementation of the apply -functions: As stated above, the apply -function contain the actual implementation of the kernel. Of course, that depends on what the kernel is supposed to do, but there some recurring actions. Obtaining an output data object Data objects like matrices and frames cannot be obtained using the new -operator, but must be obtained from the DataObjectFactory , e.g., as follows: auto res = DataObjectFactory :: create < CSRMatrix < double >> ( 3 , 4 , 6 , false ); Internally, this create -function calls a private constructor of the specified data type implementation; so please have a look at these. - Accessing the input and output data objects For efficiency reasons, accessing the data in way specific to the data type implementation is preferred to generic access method of the super-classes. That is, if possible, rather use getValues() (for DenseMatrix ) or getValues() / colColIdxs() / getRowOffsets() (for CSRMatrix ) rather than get() / set() / append() from the super-class Matrix . The reason is that these generic access methods can incur a lot of unnecessary effort, depending on the data type implementation. However, in the end it is always a trade-off between performance and code complexity. For kernels that are rarely used or typically used on small data objects, a simple but inefficient implementation might be okay. Nevertheless, since the DAPHNE system should be able to handle unexpected scripts efficiently, we should not get too much used to sacrificing efficiency.","title":"Guidelines"},{"location":"development/ImplementBuiltinKernel/#concrete-examples","text":"For concrete examples, please have a look at existing kernel implementations in src/runtime/local/kernels . For instance, the following kernels represent some interesting cases: ewBinarySca works only on scalars. ewBinaryMat works only on matrices. ewBinaryObjSca combines matrix/frame and scalar inputs. matMul delegates to an external library (OpenBLAS).","title":"Concrete Examples"},{"location":"development/ImplementBuiltinKernel/#test-cases","text":"Implementing test cases for each kernel is important to reduce the likelihood of bugs, now and after changes to the code base. Please have a look at test cases for existing kernel implementations in test/runtime/local/kernels (surely, these could still be improved).","title":"Test Cases"},{"location":"development/Logging/","text":"Logging General To write out messages of any kind from DAPHNE internals we use the spdlog library. E.g., not from a user's print() statement but when std::cout << \"my value: \" << value << std::endl; is needed. With spdlog, the previous std::cout example would read like this: spdlog::info(\"my value: {}\", value); . The only difference being that we now need to choose a log level (which is arbitrarily chosen to be info in this case). Usage Before using the logging functionality, the loggers need to be created and registered. Due to the nature of how singletons work in C++, this has to be done once per binary (e.g., daphne, run_tests, libAllKernels.so, libCUDAKernels.so, etc). For the mentioned binaries this has already been taken care of (either somewhere near the main program entrypoint or via context creation in case of the libs). All setup is handled by the class DaphneLogger (with some extras in ConfigParser). Log messages can be submitted in two forms: spdlog::warn(\"my warning\"); spdlog::get(\"default\")->warn(\"my warning\"); The two statements have the same effect. But while the former is a short form for using the default logger, the latter explicitly chooses the logger via the static get() method. This get() method is to be used with caution as it involves acquiring a lock, which is to be avoided in performance critical sections of the code. In the initial implementation there is a logger for runtime kernels provided by the context object to work around this limitation. See the matrix multiplication kernel in src/runtime/local/kernels/MatMult.cpp for example usage. We can have several loggers, which can be configured differently. For example, to control how messages are logged in the CUDA compiler pass MarkCUDAOpsPass , a logger named \"compiler::cuda\" is used. Additionally, avoiding the use of spdlog::get() is demonstrated there. For each used logger, an entry in fallback_loggers (see DaphneLogger.cpp) must exist to prevent crashing when using an unconfigured logger. To configure log levels, formatting and output options, the DaphneUserConfig and ConfigParser have been extended. See an example of this in the UserConfig.json in the root directory of the DAPHNE code base. At the moment, the output options of our logging infrastructure are a bit limited (initial version). A logger currently always emits messages to the console's std-out and optionally to a file if a file name is given in the config. The format of log messages can be customized. See the examples in UserConfig.json and the spdlog documentation . If a logger is called while running unit tests (run_tests executable), make sure to #include <run_tests.h> and call auto dctx = setupContextAndLogger(); somewhere before calling the kernel to be tested. Logging can be set to only work from a certain log level and above. This mechanism also serves as a global toggle. To set the log level limit, set { \"log-level-limit\": \"OFF\" }, . In this example, taken from UserConfig.json , all logging is switched off, regardless of configuration. Log Levels These are the available log levels (taken from <spdlog/common.h> ). Since it's an enum, their numeric value start from 0 for TRACE to 6 for OFF. namespace level { enum level_enum : int { trace = SPDLOG_LEVEL_TRACE , debug = SPDLOG_LEVEL_DEBUG , info = SPDLOG_LEVEL_INFO , warn = SPDLOG_LEVEL_WARN , err = SPDLOG_LEVEL_ERROR , critical = SPDLOG_LEVEL_CRITICAL , off = SPDLOG_LEVEL_OFF , n_levels }; ToDo Guideline when which log level is recommended Toggle console output Other log sinks","title":"Logging"},{"location":"development/Logging/#logging","text":"","title":"Logging"},{"location":"development/Logging/#general","text":"To write out messages of any kind from DAPHNE internals we use the spdlog library. E.g., not from a user's print() statement but when std::cout << \"my value: \" << value << std::endl; is needed. With spdlog, the previous std::cout example would read like this: spdlog::info(\"my value: {}\", value); . The only difference being that we now need to choose a log level (which is arbitrarily chosen to be info in this case).","title":"General"},{"location":"development/Logging/#usage","text":"Before using the logging functionality, the loggers need to be created and registered. Due to the nature of how singletons work in C++, this has to be done once per binary (e.g., daphne, run_tests, libAllKernels.so, libCUDAKernels.so, etc). For the mentioned binaries this has already been taken care of (either somewhere near the main program entrypoint or via context creation in case of the libs). All setup is handled by the class DaphneLogger (with some extras in ConfigParser). Log messages can be submitted in two forms: spdlog::warn(\"my warning\"); spdlog::get(\"default\")->warn(\"my warning\"); The two statements have the same effect. But while the former is a short form for using the default logger, the latter explicitly chooses the logger via the static get() method. This get() method is to be used with caution as it involves acquiring a lock, which is to be avoided in performance critical sections of the code. In the initial implementation there is a logger for runtime kernels provided by the context object to work around this limitation. See the matrix multiplication kernel in src/runtime/local/kernels/MatMult.cpp for example usage. We can have several loggers, which can be configured differently. For example, to control how messages are logged in the CUDA compiler pass MarkCUDAOpsPass , a logger named \"compiler::cuda\" is used. Additionally, avoiding the use of spdlog::get() is demonstrated there. For each used logger, an entry in fallback_loggers (see DaphneLogger.cpp) must exist to prevent crashing when using an unconfigured logger. To configure log levels, formatting and output options, the DaphneUserConfig and ConfigParser have been extended. See an example of this in the UserConfig.json in the root directory of the DAPHNE code base. At the moment, the output options of our logging infrastructure are a bit limited (initial version). A logger currently always emits messages to the console's std-out and optionally to a file if a file name is given in the config. The format of log messages can be customized. See the examples in UserConfig.json and the spdlog documentation . If a logger is called while running unit tests (run_tests executable), make sure to #include <run_tests.h> and call auto dctx = setupContextAndLogger(); somewhere before calling the kernel to be tested. Logging can be set to only work from a certain log level and above. This mechanism also serves as a global toggle. To set the log level limit, set { \"log-level-limit\": \"OFF\" }, . In this example, taken from UserConfig.json , all logging is switched off, regardless of configuration.","title":"Usage"},{"location":"development/Logging/#log-levels","text":"These are the available log levels (taken from <spdlog/common.h> ). Since it's an enum, their numeric value start from 0 for TRACE to 6 for OFF. namespace level { enum level_enum : int { trace = SPDLOG_LEVEL_TRACE , debug = SPDLOG_LEVEL_DEBUG , info = SPDLOG_LEVEL_INFO , warn = SPDLOG_LEVEL_WARN , err = SPDLOG_LEVEL_ERROR , critical = SPDLOG_LEVEL_CRITICAL , off = SPDLOG_LEVEL_OFF , n_levels };","title":"Log Levels"},{"location":"development/Logging/#todo","text":"Guideline when which log level is recommended Toggle console output Other log sinks","title":"ToDo"},{"location":"development/Profiling/","text":"Profiling in DAPHNE For a general overview of the profiling support in DAPHNE see the user profiling documentation . Profiling is implemented via an instrumentation pass that injects calls to the StartProfiling and StopProfiling kernels at the start and end of each block. In turn, the kernels call the PAPI-HL API start and stop functions. Known Issues / TODO For scripts with multiple blocks (e.g. UDFs), the compiler will generate profiling passes for each block separately instead of a single script-wide profiling pass. The profiling kernels should be exposed at the DSL / IR level, so that users can instrument / profile specific parts of their script. This will also need compiler cooperation, to make sure that the profiled bock is not rearranged / fused with other operations. To aid with the development and regression tracking of the runtime, the profiling kernels could also be extended to support profiling specific kernels or parts of kernels.","title":"Profiling"},{"location":"development/Profiling/#profiling-in-daphne","text":"For a general overview of the profiling support in DAPHNE see the user profiling documentation . Profiling is implemented via an instrumentation pass that injects calls to the StartProfiling and StopProfiling kernels at the start and end of each block. In turn, the kernels call the PAPI-HL API start and stop functions.","title":"Profiling in DAPHNE"},{"location":"development/Profiling/#known-issues-todo","text":"For scripts with multiple blocks (e.g. UDFs), the compiler will generate profiling passes for each block separately instead of a single script-wide profiling pass. The profiling kernels should be exposed at the DSL / IR level, so that users can instrument / profile specific parts of their script. This will also need compiler cooperation, to make sure that the profiled bock is not rearranged / fused with other operations. To aid with the development and regression tracking of the runtime, the profiling kernels could also be extended to support profiling specific kernels or parts of kernels.","title":"Known Issues / TODO"},{"location":"development/WriteDocs/","text":"Writing Documentation At the moment the collection of markdown files in the doc directory is rendered to HTML and deployed via GitHub Pages. If you insert a new markdown file, you have to add it into the html docs tree in mkdocs.yml at a suitable position under the nav section. Markdown Guideline Please write clean markdown code to ensure a proper parsing by the tools used to render HTML. It is very recommended to use an IDE like VS Code . Code offers the feature to directly render markdown pages while you work on them. The extension markdownlint directly highlights syntax violations / problems. Links With [<link-name>](<link-url/path>) you can link to other files in the repo Write links to other markdown files or source code files/diretories so that they work locally / in the github repository Do not use relative links like ../BuildingDaphne.md Always use absolute paths relative to the repo root like /doc/development/BuildingDaphne.md The Links/URLs will be altered in order to work on the rendered HTML page as well Reference to issues with [<description>](https://github.com/daphne-eu/daphne/tree/main/issues/123) . This won't work on github itself but will be rendered in the html page then Additional Syntax While some markdown renderers are much more relaxed and render as wished, some points have to be considered so that mkdocs renders correctly as well. 4 spaces indentation for nested lists (ordered/unordered) and code blocks within lists to ensure proper rendering for HTML Using <>: To use angle brackets use <\\> notation outside an codeblock Example: <nicer dicer\\> renders to <nicer dicer> Toolstack MkDocs to build html from markdown files Material for MkDocs as HTML theme","title":"WriteDocs"},{"location":"development/WriteDocs/#writing-documentation","text":"At the moment the collection of markdown files in the doc directory is rendered to HTML and deployed via GitHub Pages. If you insert a new markdown file, you have to add it into the html docs tree in mkdocs.yml at a suitable position under the nav section.","title":"Writing Documentation"},{"location":"development/WriteDocs/#markdown-guideline","text":"Please write clean markdown code to ensure a proper parsing by the tools used to render HTML. It is very recommended to use an IDE like VS Code . Code offers the feature to directly render markdown pages while you work on them. The extension markdownlint directly highlights syntax violations / problems.","title":"Markdown Guideline"},{"location":"development/WriteDocs/#links","text":"With [<link-name>](<link-url/path>) you can link to other files in the repo Write links to other markdown files or source code files/diretories so that they work locally / in the github repository Do not use relative links like ../BuildingDaphne.md Always use absolute paths relative to the repo root like /doc/development/BuildingDaphne.md The Links/URLs will be altered in order to work on the rendered HTML page as well Reference to issues with [<description>](https://github.com/daphne-eu/daphne/tree/main/issues/123) . This won't work on github itself but will be rendered in the html page then","title":"Links"},{"location":"development/WriteDocs/#additional-syntax","text":"While some markdown renderers are much more relaxed and render as wished, some points have to be considered so that mkdocs renders correctly as well. 4 spaces indentation for nested lists (ordered/unordered) and code blocks within lists to ensure proper rendering for HTML Using <>: To use angle brackets use <\\> notation outside an codeblock Example: <nicer dicer\\> renders to <nicer dicer>","title":"Additional Syntax"},{"location":"development/WriteDocs/#toolstack","text":"MkDocs to build html from markdown files Material for MkDocs as HTML theme","title":"Toolstack"},{"location":"tutorial/sqlTutorial/","text":"Using SQL in DaphneDSL DAPHNE supports a rudimentary version of SQL. At any point in a DaphneDSL script, we can execute a SQL query on frames. We need two operations to achieve this: registerView(...) and sql(...) For the following examples we assume we already have a DaphneDSL script which includes calculations on a frame \"x\" that has the columns \"a\", \"b\" and \"c\". General Procedure registerView(...) RegisterView registers a frame for the sql operation. If we want to execute a SQL query on a frame, we need to register it before that. The operation has two inputs: the name of the table, as a string, and the frame which shall be associated with the given name. For example, we can register the frame \"x\", from previous calculations, under the name \"Table1\". The DaphneDSL script for this would look like this: registerView ( \"Table1\" , x ); sql(...) Now that we have registered the tables, that we need for our SQL query, we can go ahead and execute our query. The SQL operation takes one input: the SQL query, as a string. In it, we will reference the table names we previously have registered via registerView(...). As a result of this operation, we get back a frame. The columns of the frame are named after the projection arguments inside the SQL query. For example, we want to return all the rows of the frame x, which we have previously registered under the name \"Table1\", where the column \"a\" is greater than 5 and save it in a new frame named \"y\". The DaphneDSL script for this would look like this: y = sql ( \"SELECT t.a as a, t.b as b, t.c as c FROM Table1 as t WHERE t.a > 5;\" ); This results in a frame \"y\" that has three columns \"a\", \"b\" and \"c\". On the frame y we can continue to build our DaphneDSL script. Features We don't support the complete SQL standard at the moment. For instance, we need to fully specify on which columns we want to operate. In the example above, we see \"t.a\" instead of simply \"a\". Also, not supported are DDL and DCL Queries. Our goal for DML queries is to only support SELECT-statements. Other features we do and don't support right now can be found below. Supported Features Cross Product Complex Where Clauses Inner Join with single and multiple join conditions separated by an \"AND\" Operator Group By Clauses Having Clauses Order By Clauses As Not Yet Supported Features The Star Operator * Nested SQL Queries like: SELECT a FROM x WHERE a IN SELECT a FROM y All Set Operations (Union, Except, Intersect) Recursive SQL Queries Limit Distinct Examples In the following, we show two simple examples of SQL in DaphneDSL. The DaphneDSL scripts can be found in doc/tutorial/sqlExample1.daph and doc/tutorial/sqlExample2.daph . Example 1 //Creation of different matrices for a Frame //seq(a, b, c) generates a sequences of the form [a, b] and step size c employee_id = seq ( 1 , 20 , 1 ); //rand(a, b, c, d, e, f) generates a matrix with a rows and b columns in a value range of [c, d] salary = rand ( 20 , 1 , 250.0 , 500.0 , 1.0 , -1 ); //with [a, b, ..] we can create a matrix with the given values. age = [ 20 , 30 , 23 , 65 , 70 , 42 , 34 , 55 , 76 , 32 , 53 , 40 , 42 , 69 , 63 , 26 , 70 , 36 , 21 , 23 ]; //createFrame() creates a Frame with the given matrices. The column names (strings) are optional. employee_frame = createFrame ( employee_id , salary , age , \"employee_id\" , \"salary\" , \"age\" ); //We register the employee_frame we created previously. note the name for the registration and the //name of the frame don't have to be the same. registerView ( \"employee\" , employee_frame ); //We run a SQL Query on the registered Frame. Note here we have to reference the name we choose //during registration. res = sql ( \"SELECT e.employee_id as employee_id, e.salary as salary, e.age as age FROM employee as e WHERE e . salary > 450.0 ; \"); //We can Print both employee and the query result to the console with print(). print ( employee_frame ); print ( res ); Example 2 employee_id = seq ( 1 , 20 , 1 ); salary = rand ( 20 , 1 , 250.0 , 500.0 , 1.0 , -1 ); age = [ 20 , 30 , 23 , 65 , 70 , 42 , 34 , 55 , 76 , 32 , 53 , 40 , 42 , 69 , 63 , 26 , 70 , 36 , 21 , 23 ]; employee_frame = createFrame ( employee_id , salary , age , \"employee_id\" , \"salary\" , \"age\" ); registerView ( \"employee\" , employee_frame ); res = sql ( \"SELECT e.age as age, avg(e.salary) as salary FROM employee as e GROUP BY e . age ORDER BY e . age \"); print ( employee_frame ); print ( res );","title":"sqlTutorial"},{"location":"tutorial/sqlTutorial/#using-sql-in-daphnedsl","text":"DAPHNE supports a rudimentary version of SQL. At any point in a DaphneDSL script, we can execute a SQL query on frames. We need two operations to achieve this: registerView(...) and sql(...) For the following examples we assume we already have a DaphneDSL script which includes calculations on a frame \"x\" that has the columns \"a\", \"b\" and \"c\".","title":"Using SQL in DaphneDSL"},{"location":"tutorial/sqlTutorial/#general-procedure","text":"","title":"General Procedure"},{"location":"tutorial/sqlTutorial/#registerview","text":"RegisterView registers a frame for the sql operation. If we want to execute a SQL query on a frame, we need to register it before that. The operation has two inputs: the name of the table, as a string, and the frame which shall be associated with the given name. For example, we can register the frame \"x\", from previous calculations, under the name \"Table1\". The DaphneDSL script for this would look like this: registerView ( \"Table1\" , x );","title":"registerView(...)"},{"location":"tutorial/sqlTutorial/#sql","text":"Now that we have registered the tables, that we need for our SQL query, we can go ahead and execute our query. The SQL operation takes one input: the SQL query, as a string. In it, we will reference the table names we previously have registered via registerView(...). As a result of this operation, we get back a frame. The columns of the frame are named after the projection arguments inside the SQL query. For example, we want to return all the rows of the frame x, which we have previously registered under the name \"Table1\", where the column \"a\" is greater than 5 and save it in a new frame named \"y\". The DaphneDSL script for this would look like this: y = sql ( \"SELECT t.a as a, t.b as b, t.c as c FROM Table1 as t WHERE t.a > 5;\" ); This results in a frame \"y\" that has three columns \"a\", \"b\" and \"c\". On the frame y we can continue to build our DaphneDSL script.","title":"sql(...)"},{"location":"tutorial/sqlTutorial/#features","text":"We don't support the complete SQL standard at the moment. For instance, we need to fully specify on which columns we want to operate. In the example above, we see \"t.a\" instead of simply \"a\". Also, not supported are DDL and DCL Queries. Our goal for DML queries is to only support SELECT-statements. Other features we do and don't support right now can be found below.","title":"Features"},{"location":"tutorial/sqlTutorial/#supported-features","text":"Cross Product Complex Where Clauses Inner Join with single and multiple join conditions separated by an \"AND\" Operator Group By Clauses Having Clauses Order By Clauses As","title":"Supported Features"},{"location":"tutorial/sqlTutorial/#not-yet-supported-features","text":"The Star Operator * Nested SQL Queries like: SELECT a FROM x WHERE a IN SELECT a FROM y All Set Operations (Union, Except, Intersect) Recursive SQL Queries Limit Distinct","title":"Not Yet Supported Features"},{"location":"tutorial/sqlTutorial/#examples","text":"In the following, we show two simple examples of SQL in DaphneDSL. The DaphneDSL scripts can be found in doc/tutorial/sqlExample1.daph and doc/tutorial/sqlExample2.daph .","title":"Examples"},{"location":"tutorial/sqlTutorial/#example-1","text":"//Creation of different matrices for a Frame //seq(a, b, c) generates a sequences of the form [a, b] and step size c employee_id = seq ( 1 , 20 , 1 ); //rand(a, b, c, d, e, f) generates a matrix with a rows and b columns in a value range of [c, d] salary = rand ( 20 , 1 , 250.0 , 500.0 , 1.0 , -1 ); //with [a, b, ..] we can create a matrix with the given values. age = [ 20 , 30 , 23 , 65 , 70 , 42 , 34 , 55 , 76 , 32 , 53 , 40 , 42 , 69 , 63 , 26 , 70 , 36 , 21 , 23 ]; //createFrame() creates a Frame with the given matrices. The column names (strings) are optional. employee_frame = createFrame ( employee_id , salary , age , \"employee_id\" , \"salary\" , \"age\" ); //We register the employee_frame we created previously. note the name for the registration and the //name of the frame don't have to be the same. registerView ( \"employee\" , employee_frame ); //We run a SQL Query on the registered Frame. Note here we have to reference the name we choose //during registration. res = sql ( \"SELECT e.employee_id as employee_id, e.salary as salary, e.age as age FROM employee as e WHERE e . salary > 450.0 ; \"); //We can Print both employee and the query result to the console with print(). print ( employee_frame ); print ( res );","title":"Example 1"},{"location":"tutorial/sqlTutorial/#example-2","text":"employee_id = seq ( 1 , 20 , 1 ); salary = rand ( 20 , 1 , 250.0 , 500.0 , 1.0 , -1 ); age = [ 20 , 30 , 23 , 65 , 70 , 42 , 34 , 55 , 76 , 32 , 53 , 40 , 42 , 69 , 63 , 26 , 70 , 36 , 21 , 23 ]; employee_frame = createFrame ( employee_id , salary , age , \"employee_id\" , \"salary\" , \"age\" ); registerView ( \"employee\" , employee_frame ); res = sql ( \"SELECT e.age as age, avg(e.salary) as salary FROM employee as e GROUP BY e . age ORDER BY e . age \"); print ( employee_frame ); print ( res );","title":"Example 2"}]}